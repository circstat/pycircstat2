{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#pycircstat2-circular-statistics-with-python","title":"PyCircStat2: Circular statistics with Python","text":"<p>A rework of pycircstat.</p> <p>Key Features | Installlation |  API Reference | Examples ( Books | Topics )</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>One-Stop Circular Data Analysis Pipeline with <code>Circular</code> Class </p> <p>The <code>Circular</code> class simplifies circular data analysis by providing automatic data transformation, descriptive statistics, hypothesis testing, and visualization tools\u2014all in one place.  </p> </li> <li> <p>Compatibility with Legacy APIs </p> <p>APIs for descriptive statistics and hypothesis testing follow the conventions established by the original circstat-matlab and pycircstat, ensuring ease of use for existing users.</p> </li> <li> <p>Wide-Ranging Circular Distributions </p> <p>The package supports a variety of circular distributions, including:  </p> <ul> <li>Symmetric distributions: Circular Uniform, Cardioid, Cartwright, Wrapped Normal, Wrapped Cauchy, von Mises (and its flat-top extension), and Jones-Pewsey.</li> <li>Asymmetric distributions: Sine-skewed Jones-Pewsey, Asymmetric Extended Jones-Pewsey, Inverse Batschelet.</li> </ul> </li> </ul> <p>Also see the full feature checklist here.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install the latest tagged version:</p> <pre><code>pip install pycircstat2\n</code></pre> <p>Or to install the development version, clone the repository and install it with <code>pip install -e</code>:</p> <pre><code>git clone https://github.com/circstat/pycircstat2\npip install -e pycircstat2\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<p>The API reference is available here.</p>"},{"location":"#example-notebooks","title":"Example notebooks","text":"<p>In the notebooks below, we reproduce examples and figures from a few textbooks on circular statistics.</p>"},{"location":"#books","title":"Books","text":"<ul> <li>Statistical Analysis of Circular Data (Fisher, 1993)</li> <li>Chapter 26 and 27 from Biostatistical Analysis (Zar, 2010).</li> <li>Circular Statistics in R (Pewsey, et al., 2014)</li> </ul> <p>And a few more examples on selective topics:</p>"},{"location":"#topics","title":"Topics","text":"<ol> <li>Utils</li> <li>Descriptive Statistics</li> <li>Hypothesis Testing</li> <li>Circular Models</li> <li>Regression</li> <li>Clustering</li> </ol>"},{"location":"feature-checklist/","title":"Feature Checklist","text":""},{"location":"feature-checklist/#1-descriptive-statistics","title":"1. Descriptive Statistics","text":"Feature PyCircStat2 PyCircStat CircStat (MATLAB) CircStats (R) circular (R) Measures of Central Tendency Circular Mean <code>circ_mean</code> <code>mean(alpha)</code> <code>circ_mean(alpha)</code> <code>circ.mean</code> <code>mean.circular</code> Circular Mean CI <code>circ_mean_ci</code> <code>mean(alpha, ci=95)</code> <code>circ_confmean</code> - <code>mle.vonmises.bootstrap.ci</code> Circular Median <code>circ_median</code> <code>median</code> <code>circ_median</code> - <code>median.circular</code>/<code>medianHL.circular</code> Circular Median CI <code>circ_median_ci</code> - - - - Circular Quantile <code>circ_quantile</code> - - - <code>quantile.circular</code> Measures of Spread &amp; Dispersion Resultant Vector Length <code>circ_r</code> <code>resultant_vector_length</code> <code>circ_r</code> <code>est.rho</code> <code>rho.circular</code> Angular Variance <code>angular_var</code> <code>avar</code> <code>circ_var</code> - <code>angular.variance</code> Angular Standard Deviation <code>angular_std</code> <code>astd</code> <code>circ_std</code> - <code>angular.deviation</code> Circular Variance <code>circ_var</code> <code>var</code> <code>circ_var</code> <code>circ.disp</code> <code>var.circular</code> Circular Standard Deviation <code>circ_std</code> <code>std</code> <code>circ_std</code> - <code>sd.circular</code> Circular Dispersion <code>circ_dispersion</code> - - - - Higher-Order Statistics Circular Moment <code>circ_moment</code> <code>moment</code> <code>circ_moment</code> <code>tri.moment</code> <code>trigonometric.moment</code> Circular Skewness <code>circ_skewness</code> <code>skewness</code> <code>circ_skewness</code> - - Circular Kurtosis <code>circ_kurtosis</code> <code>kurtoisis</code> <code>circ_kurtosis</code> - - Distance &amp; Pairwise Comparisons Mean deviation <code>circ_mean_deviation</code> - - - <code>meandeviation</code> Circular Distance <code>circ_dist</code> <code>cdist</code> <code>circ_dist</code> - - Pairwise Circular Distance <code>circ_pairdist</code> <code>pairwise_cdiff</code> <code>circ_dist2</code> - <code>dist.circular</code>"},{"location":"feature-checklist/#2-hypothesis-testing","title":"2. Hypothesis Testing","text":""},{"location":"feature-checklist/#one-sample-tests-for-significance","title":"One-Sample Tests for Significance","text":"Feature H0 PyCircStat2 PyCircStat CircStat (MATLAB) CircStats (R) circular (R) Mean Direction Rayleigh Test \\(\\rho=0\\) <sup>1</sup> <code>rayleigh_test</code> <code>rayleigh</code> <code>circ_rtest</code> <code>r.test</code> <code>rayleigh.test</code> V-Test \\(\\rho=0\\) <code>V_test</code> <code>vtest</code> <code>circ_vtest</code> <code>v0.test</code> - One-sample Test \\(\\tilde\\mu=\u03bc_0\\) <code>one_sample_test</code> <code>mtest</code> <code>circ_mtest</code> - - Change Point Test no change point <code>change_point_test</code> - - <code>change.pt</code> <code>change.point</code> Median Direction Hodges-Ajne (omnibus) Test \\(\\rho=0\\) <code>omnibus_test</code> <code>omnibus</code> <code>circ_otest</code> - - Batschelet Test \\(\\rho=0\\) <code>batschelet_test</code> - - - - Binomial Test \\(\\tilde\\theta = \\theta_0\\) <sup>2</sup> <code>binomial_test</code> <code>medtest</code> <code>circ_medtest</code> - - Symmetry Test around median \\(\\text{symmetry}\\) <code>symmetry_test</code> <code>symtest</code> <code>circ_symtest</code> - -"},{"location":"feature-checklist/#multi-sample-tests-for-significance","title":"Multi-Sample Tests for Significance","text":"Feature H0 PyCircStat2 PyCircStat CircStat (MATLAB) CircStats (R) circular (R) Mean Direction Circular Analysis of Variance \\(\\mu_1 = \\dots = \\mu_n\\) <code>circ_anova</code> - - - <code>aov.circular</code> Watson-Williams Test <sup>4</sup> \\(\\mu_1 = \\dots = \\mu_n\\) <code>watson_williams_test</code> <code>watson_williams</code> <code>circ_wwtest</code> - <code>watson.williams.test</code> Harrison-Kanji Test<sup>5</sup> \\(\\mu_1 = \\dots = \\mu_n\\) <code>harrison_kanji_test</code> <code>hktest</code> <code>circ_hktest</code> - - Median Direction Common Median Test \\(\\tilde{\\theta}_1 = \\dots = \\tilde{\\theta}_n\\) <code>common_median_test</code> <code>cmtest</code> <code>circ_cmtest</code> - - Concentration Concentration Test (F-test) \\(\\kappa_1 = \\dots = \\kappa_n\\) <code>concentration_test</code> - <code>circ_ktest</code> - - Equal Kappa Test \\(\\kappa_1 = \\dots = \\kappa_n\\) <code>equal_kappa_test</code> - - - <code>equal.kappa.test</code> Distribution Homogeneity Watson's U2 Test \\(F_1 = F_2\\) <sup>3</sup> <code>watson_u2_test</code> - - <code>watson.two</code> <code>watson.two.test</code> Wallraff Test \\(F_1 = F_2\\) <code>wallraff_test</code> - - - <code>wallraff.test</code> Wheeler-Watson Test \\(F_1 = F_2\\) <code>wheeler_watson_test</code> - - - <code>watson.wheeler.test</code> Angular Randomization Test \\(F_1 = F_2\\) <code>angular_randomisation_test</code> - - - - Rao's Tests for Homogeneity \\(F_1 = F_2\\) <code>rao_homogeneity_test</code> - - <code>rao.homogeneity</code> <code>rao.test</code>"},{"location":"feature-checklist/#goodness-of-fit-tests","title":"Goodness-of-fit Tests","text":"Feature H0 PyCircStat2 PyCircStat CircStat (MATLAB) CircStats (R) circular (R) Kuiper\u2019s Test \\(\\rho = 0\\) <code>circ_kuiper_test</code> <code>kupier</code> <code>circ_kuipertest</code> <code>kuiper</code> <code>kuiper.test</code> Rao\u2019s Spacing Test \\(\\rho = 0\\) <code>rao_spacing_test</code> <code>raospacing</code> <code>circ_raotest</code> <code>rao.spacing</code> <code>rao.spacing.test</code> Watson's Test \\(\\rho = 0\\) <code>watson_test</code> - - <code>watson</code> <code>watson.test</code> Circular Range Test \\(\\rho = 0\\) <code>circ_range_test</code> - - <code>circ_range</code> <code>range.circular</code>"},{"location":"feature-checklist/#3-correlation-regression","title":"3. Correlation &amp; Regression","text":"Feature PyCircStat2 PyCircStat CircStat (MATLAB) CircStats (R) circular (R) Circular-Circular Correlation <code>circ_corrcc</code> <code>corrcc</code> <code>circ_corrcc</code> <code>circ.cor</code> <code>cor.circular</code> Circular-Linear Correlation <code>circ_corrcl</code> <code>corrcl</code> <code>circ_corrcl</code> - - Circular-Circular Regression <code>CCRegression</code> - - <code>circ.reg</code> <code>lm.circular(type=\"c-c\")</code> Circular-Linear Regression <code>CLRegression</code> - - - <code>lm.circular(type=\"c-l\")</code>"},{"location":"feature-checklist/#4-circular-distributions","title":"4. Circular Distributions","text":"<p>All circular distributions assume angles are on <code>[0, 2\u03c0)</code>. Inputs are automatically wrapped to that support as a convenience. We remove SciPy's <code>loc</code>/<code>scale</code> convention\u2014parameters like <code>mu</code>, <code>rho</code>, etc. are the only inputs. </p>"},{"location":"feature-checklist/#symmetric-circular-distributions","title":"Symmetric Circular Distributions","text":"Feature Method PyCircStat2 PyCircStat CircStat (MATLAB) CircStats (R) circular (R) Circular Uniform PDF <code>circularuniform.pdf</code> - - - <code>dcircularuniform</code> CDF <code>circularuniform.cdf</code> - - - - PPF <code>circularuniform.ppf</code> - - - - RVS <code>circularuniform.rvs</code> - - - <code>rcircularuniform</code> Fit <code>circularuniform.fit</code> - - - - Triangular PDF <code>triangular.pdf</code> <code>triangular.pdf</code> - <code>dtri</code> - CDF <code>triangular.cdf</code> <code>triangular.cdf</code> - - - PPF <code>triangular.ppf</code> <code>triangular.ppf</code> - - - RVS <code>triangular.rvs</code> <code>triangular.rvs</code> - <code>rtri</code> - Fit <code>triangular.fit</code> <code>triangular.fit</code> - - - Cardioid PDF <code>cardioid.pdf</code> <code>cardioid.pdf</code> - <code>dcard</code> <code>dcardioid</code> CDF <code>cardioid.cdf</code> <code>cardioid.cdf</code> - - - PPF <code>cardioid.ppf</code> <code>cardioid.ppf</code> - - RVS <code>cardioid.rvs</code> <code>cardioid.rvs</code> - <code>rcard</code> <code>rcardioid</code> Fit <code>cardioid.fit</code> <code>cardioid.fit</code> - - Cartwright PDF <code>cartwright.pdf</code> - - - <code>dcarthwrite</code> CDF <code>cartwright.cdf</code> - - - - PPF <code>cartwright.ppf</code> - - - - RVS <code>cartwright.rvs</code> - - - - Fit <code>cartwright.fit</code> - - - - Wrapped Normal PDF <code>wrapnorm.pdf</code> - - <code>dwrpnorm</code> <code>dwrappednormal</code> CDF <code>wrapnorm.cdf</code> - - - <code>pwrappednormal</code> PPF <code>wrapnorm.ppf</code> - - - <code>qwrappednormal</code> RVS <code>wrapnorm.rvs</code> - - <code>rwrpnorm</code> <code>rwrappednormal</code> Fit <code>wrapnorm.fit</code> - - - <code>mle.wrappednormal</code> Wrapped Cauchy PDF <code>wrapcauchy.pdf</code> - - <code>dwrpcauchy</code> <code>dwrappedcauchy</code> CDF <code>wrapcauchy.cdf</code> - - - - PPF <code>wrapcauchy.ppf</code> - - - - RVS <code>wrapcauchy.rvs</code> - - <code>rwrpcauchy</code> <code>rwrappedcauchy</code> Fit <code>wrapcauchy.fit</code> - - - <code>mle.wrappedcauchy</code> Von Mises PDF <code>vonmises.pdf</code> - <code>circ_vmpdf</code> <code>dvm</code> <code>dvonmises</code> CDF <code>vonmises.cdf</code> - - <code>pvm</code> <code>pvonmises</code> PPF <code>vonmises.ppf</code> - - - <code>qvonmises</code> RVS <code>vonmises.rvs</code> - <code>circ_vmrnd</code> <code>rvm</code> <code>rvonmises</code> Fit <code>vonmises.fit</code> - <code>circ_vmpar</code> <code>vm.ml</code> <code>mle.vonmises</code> Flattopped Von Mises PDF <code>vonmises_flattopped.pdf</code> - - - - CDF <code>vonmises_flattopped.cdf</code> - - - - PPF <code>vonmises_flattopped.ppf</code> - - - - RVS <code>vonmises_flattopped.rvs</code> - - - - Fit <code>vonmises_flattopped.fit</code> - - - - Jones-Pewsey PDF <code>jonespewsey.pdf</code> - - - <code>djonespewsey</code> CDF <code>jonespewsey.cdf</code> - - - - PPF <code>jonespewsey.ppf</code> - - - - RVS <code>jonespewsey.rvs</code> - - - - Fit <code>jonespewsey.fit</code> - - - -"},{"location":"feature-checklist/#asymmetric-circular-distributions","title":"Asymmetric Circular Distributions","text":"Feature Method PyCircStat2 PyCircStat CircStat (MATLAB) CircStats (R) circular (R) Jones-Pewsey Sine-Skewed PDF <code>jonespewsey_sineskewed.pdf</code> - - - - CDF <code>jonespewsey_sineskewed.cdf</code> - - - - PPF <code>jonespewsey_sineskewed.ppf</code> - - - - RVS <code>jonespewsey_sineskewed.rvs</code> - - - - Fit <code>jonespewsey_sineskewed.fit</code> - - - - Jones-Pewsey Asymmetric PDF <code>jonespewsey_asym.pdf</code> - - - - CDF <code>jonespewsey_asym.cdf</code> - - - - PPF <code>jonespewsey_asym.ppf</code> - - - - RVS <code>jonespewsey_asym.rvs</code> - - - - Fit <code>jonespewsey_asym.fit</code> - - - - Inverse Batschelet PDF <code>inverse_batschelet.pdf</code> - - - - CDF <code>inverse_batschelet.cdf</code> - - - - PPF <code>inverse_batschelet.ppf</code> - - - - RVS <code>inverse_batschelet.rvs</code> - - - - Fit <code>inverse_batschelet.fit</code> - - - - Kato-Jones PDF <code>katojones.pdf</code> - - - <code>dkatojones</code> CDF <code>katojones.cdf</code> - - - - PPF <code>katojones.ppf</code> - - - - RVS <code>katojones.rvs</code> - - - <code>rkatojones</code> Fit <code>katojones.fit</code> - - - - Wrapped Stable PDF <code>wrapstable.pdf</code> - - - - CDF <code>wrapstable.cdf</code> - - - - PPF <code>wrapstable.ppf</code> - - - - RVS <code>wrapstable.rvs</code> - - <code>rwrpstab</code> - Fit <code>wrapstable.fit</code> - - - - Asymmetric Trangular PDF - - - - <code>dasytriangular</code> Projected Normal PDF - - - - <code>dpnorm</code> RVS - - - - <code>rpnorm</code> <ol> <li> <p>\\(\\rho=0\\) stands for uniform distributed.\u00a0\u21a9</p> </li> <li> <p>\\(\\theta\\) stands for median.\u00a0\u21a9</p> </li> <li> <p>\\(F\\) stands for distributions.\u00a0\u21a9</p> </li> <li> <p>Yet anothr one-way ANOVA.\u00a0\u21a9</p> </li> <li> <p>Two-way ANOVA.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/base/","title":"Circular Data Base","text":""},{"location":"reference/base/#pycircstat2.base.Circular","title":"<code>Circular</code>","text":"<p>Circular Data Analysis Object.</p> <p>This class encapsulates circular data and provides tools for descriptive statistics, hypothesis testing, and visualization. It automatically computes key circular statistics and tests when the data are loaded.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like(n)</code> <p>The raw circular data, typically in degrees, radians, or other angular units.</p> required <code>w</code> <code>array - like(n) or None</code> <p>Frequencies or weights for the data points. If None, all data points are treated equally. Default is None.</p> <code>None</code> <code>bins</code> <code>(int, array - like(n + 1) or None)</code> <p>Number of bins or bin edges to group the data. If None, the data is not binned. Default is None.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit of the input data. Must be one of {\"degree\", \"radian\", \"hour\"}. Default is \"degree\".</p> <code>'degree'</code> <code>full_cycle</code> <code>int, float, or None</code> <p>The total range of values that complete one full cycle in the chosen unit. If None, the value is inferred based on the unit:</p> <ul> <li>360 for degrees,</li> <li>\\(2\\pi\\) for radians,</li> <li>24 for hours.</li> </ul> <p>Custom values can be set explicitly for other units. Default is None.</p> <code>None</code> <code>n_clusters_max</code> <code>int</code> <p>Maximum number of clusters to test for a mixture of von Mises distributions. Default is 1.</p> <code>1</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to customize the computation of statistics such as the median.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>Total sample size, including weights.</p> <code>mean</code> <code>float</code> <p>Angular mean in radians.</p> <code>mean_ci</code> <code>tuple of float</code> <p>Confidence interval for the angular mean, if applicable.</p> <code>median</code> <code>float</code> <p>Angular median in radians.</p> <code>median_ci</code> <code>tuple of float</code> <p>Confidence interval for the angular median, if computed.</p> <code>r</code> <code>float</code> <p>Resultant vector length, measuring data concentration (0 to 1).</p> <code>kappa</code> <code>float</code> <p>Concentration parameter, measuring data sharpness.</p> <code>s</code> <code>float</code> <p>Angular deviation, measuring data dispersion.</p> <code>skewness</code> <code>float</code> <p>Circular skewness of the data.</p> <code>kurtosis</code> <code>float</code> <p>Circular kurtosis of the data.</p> <code>R</code> <code>float</code> <p>Rayleigh's R statistic, derived from the resultant vector length.</p> <code>mixtures</code> <code>list</code> <p>Mixture models of von Mises distributions fitted to the data (if <code>n_clusters_max &gt; 1</code>).</p> <p>Methods:</p> Name Description <code>summary</code> <p>Returns a detailed summary of the computed statistics.</p> <code>plot</code> <p>Visualizes the circular data, including histograms and other representations.</p> Notes <ul> <li>Angular data is automatically converted to radians for internal computations.</li> <li>Data can be grouped or ungrouped. Ungrouped data is handled by assigning equal weights.</li> <li>The Rayleigh test for angular mean is computed, with p-values indicating significance.</li> <li>Confidence intervals for the angular mean are approximated using either bootstrap   or dispersion methods, depending on the sample size and significance.</li> </ul> References <ul> <li>Zar, J. H. (2010). Biostatistical Analysis (5th Edition). Pearson.</li> <li>Fisher, N. I. (1995). Statistical Analysis of Circular Data. Cambridge University Press.</li> </ul> <p>Examples:</p>"},{"location":"reference/base/#pycircstat2.base.Circular--basic-usage","title":"Basic Usage","text":"<pre><code>data = [30, 60, 90, 120, 150]\ncirc = Circular(data, unit=\"degree\")\nprint(circ.summary())\n</code></pre>"},{"location":"reference/base/#pycircstat2.base.Circular--grouped-data","title":"Grouped Data","text":"<pre><code>data = [0, 30, 60, 90]\nweights = [1, 2, 3, 4]\ncirc = Circular(data, w=weights, unit=\"degree\")\nprint(circ.summary())\n</code></pre> Source code in <code>pycircstat2/base.py</code> <pre><code>class Circular:\n    r\"\"\"\n    Circular Data Analysis Object.\n\n    This class encapsulates circular data and provides tools for descriptive statistics,\n    hypothesis testing, and visualization. It automatically computes key circular\n    statistics and tests when the data are loaded.\n\n    Parameters\n    ----------\n    data : array-like (n,)\n        The raw circular data, typically in degrees, radians, or other angular units.\n\n    w : array-like (n,) or None, optional\n        Frequencies or weights for the data points. If None, all data points are treated equally.\n        Default is None.\n\n    bins : int, array-like (n+1,) or None, optional\n        Number of bins or bin edges to group the data. If None, the data is not binned.\n        Default is None.\n\n    unit : str, optional\n        Unit of the input data. Must be one of {\"degree\", \"radian\", \"hour\"}.\n        Default is \"degree\".\n\n    full_cycle : int, float, or None, optional\n        The total range of values that complete one full cycle in the chosen unit.\n        If None, the value is inferred based on the unit:\n\n        - 360 for degrees,\n        - $2\\pi$ for radians,\n        - 24 for hours.\n\n        Custom values can be set explicitly for other units.\n        Default is None.\n\n    n_clusters_max : int, optional\n        Maximum number of clusters to test for a mixture of von Mises distributions.\n        Default is 1.\n\n    kwargs : dict, optional\n        Additional keyword arguments to customize the computation of statistics such as the median.\n\n    Attributes\n    ----------\n    n : int\n        Total sample size, including weights.\n\n    mean : float\n        Angular mean in radians.\n\n    mean_ci : tuple of float\n        Confidence interval for the angular mean, if applicable.\n\n    median : float\n        Angular median in radians.\n\n    median_ci : tuple of float\n        Confidence interval for the angular median, if computed.\n\n    r : float\n        Resultant vector length, measuring data concentration (0 to 1).\n\n    kappa : float\n        Concentration parameter, measuring data sharpness.\n\n    s : float\n        Angular deviation, measuring data dispersion.\n\n    skewness : float\n        Circular skewness of the data.\n\n    kurtosis : float\n        Circular kurtosis of the data.\n\n    R : float\n        Rayleigh's R statistic, derived from the resultant vector length.\n\n    mixtures : list\n        Mixture models of von Mises distributions fitted to the data (if `n_clusters_max &gt; 1`).\n\n    Methods\n    -------\n    summary()\n        Returns a detailed summary of the computed statistics.\n\n    plot(ax=None, kind=None, **kwargs)\n        Visualizes the circular data, including histograms and other representations.\n\n    Notes\n    -----\n    - Angular data is automatically converted to radians for internal computations.\n    - Data can be grouped or ungrouped. Ungrouped data is handled by assigning equal weights.\n    - The Rayleigh test for angular mean is computed, with p-values indicating significance.\n    - Confidence intervals for the angular mean are approximated using either bootstrap\n      or dispersion methods, depending on the sample size and significance.\n\n    References\n    ----------\n    - Zar, J. H. (2010). Biostatistical Analysis (5th Edition). Pearson.\n    - Fisher, N. I. (1995). Statistical Analysis of Circular Data. Cambridge University Press.\n\n    Examples\n    --------\n\n    #### Basic Usage\n\n    ```python\n    data = [30, 60, 90, 120, 150]\n    circ = Circular(data, unit=\"degree\")\n    print(circ.summary())\n    ```\n\n    #### Grouped Data\n\n    ```python\n    data = [0, 30, 60, 90]\n    weights = [1, 2, 3, 4]\n    circ = Circular(data, w=weights, unit=\"degree\")\n    print(circ.summary())\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Union[np.ndarray, list],  # angle\n        w: Optional[Union[np.ndarray, list]] = None,  # frequency\n        bins: Optional[Union[int, np.ndarray]] = None,\n        unit: str = \"degree\",\n        full_cycle: Optional[Union[\n            int, float\n        ]] = None,  # number of intervals in the full cycle\n        n_clusters_max: int = 1,  # number of clusters to be tested for mixture of von Mises\n        rotate: Optional[float] = None, # in rad\n        **kwargs,\n    ):\n        # meta\n        self.unit = unit\n        if full_cycle is None:\n            if unit == \"degree\":\n                self.full_cycle = full_cycle = 360\n            elif unit == \"radian\":\n                self.full_cycle = full_cycle = 2 * np.pi\n            elif unit == \"hour\":\n                self.full_cycle = full_cycle = 24\n            else:\n                raise ValueError(\n                    \"You need to provide a value for `full_cycle` if it is not `degree` or `hour`.\"\n                )\n        else:\n            self.full_cycle = full_cycle\n\n        self.n_clusters_max = n_clusters_max\n        self.kwargs_median = kwargs_median = {\n            **{\n                \"method\": \"deviation\",\n                \"return_average\": True,\n                \"average_method\": \"all\",\n            },\n            **kwargs.pop(\"kwargs_median\", {}),\n        }\n        self.kwargs_mean_ci = kwargs_mean_ci = kwargs.pop(\"kwargs_mean_ci\", None)\n\n        # data\n        self.data = data = np.array(data) if isinstance(data, list) else data\n        self.alpha = alpha = np.array(data2rad(data, full_cycle)) if rotate is None else rotate_data(np.array(data2rad(data, full_cycle)), rotate, unit=\"radian\")\n\n        # data preprocessing\n        if bins is None:\n            if w is None:  # ungrouped data, because no `w` is provided.\n                self.w = w = np.ones_like(alpha).astype(int)\n                self.grouped = grouped = False\n                self.bin_size = bin_size = 0.0\n            else:  # grouped data\n                assert len(w) == len(alpha), \"`w` and `data` must be the same length.\"\n                assert len(w) == len(\n                    np.arange(0, 2 * np.pi, 2 * np.pi / len(w))\n                ), \"Grouped data should included empty bins.\"\n                self.w = w = np.asarray(w)\n                self.grouped = grouped = True\n                self.bin_size = bin_size = np.diff(alpha).min()\n                self.alpha_lb = alpha - bin_size / 2\n                self.alpha_ub = alpha + bin_size / 2\n\n        # bin data usingse np.histogram\n        else:\n            if isinstance(bins, int) or isinstance(bins, np.ndarray):\n                w, alpha = np.histogram(\n                    alpha, bins=bins, range=(0, 2 * np.pi)\n                )  # np.histogram return bin edges\n            self.w = w = np.asarray(w)\n            self.alpha_lb = alpha[:-1]  # bin lower bound\n            self.alpha_ub = alpha[1:]  # bin upper bound\n            self.alpha = alpha = 0.5 * (alpha[:-1] + alpha[1:])  # get bin centers\n            self.grouped = grouped = True\n            self.bin_size = bin_size = np.diff(alpha).min()\n\n        # sample size\n        self.n = n = np.sum(w).astype(int)\n\n        # angular mean and resultant vector length\n        self.mean, self.r = (_, r) = circ_mean_and_r(alpha=alpha, w=w)\n\n        # z-score and p-value from rayleigh test for angular mean\n        self.mean_test_result = rayleigh_test_result = rayleigh_test(n=n, r=r)\n        mean_pval = rayleigh_test_result.pval\n\n        # Rayleigh's R\n        self.R = n * r\n\n        # kappa\n        self.kappa = circ_kappa(r=r, n=n)\n\n        # confidence interval for angular mean\n        # in practice, the equations for approximating mean ci for 8 &lt;= n &lt;= 12 in zar 2010\n        # can still yield nan\n        if self.kwargs_mean_ci is None:\n            if mean_pval &lt; 0.05 and (8 &lt;= self.n &lt; 25):\n                self.method_mean_ci = method_mean_ci = \"bootstrap\"\n                self.mean_ci_level = mean_ci_level = 0.95\n            elif mean_pval &lt; 0.05 and self.n &gt;= 25:\n                # Eq 4.22 (Fisher, 1995)\n                self.method_mean_ci = method_mean_ci = \"dispersion\"\n                self.mean_ci_level = mean_ci_level = 0.95\n            else:  # mean_pval &gt; 0.05\n                self.method_mean_ci = method_mean_ci = None\n                self.mean_ci_level = mean_ci_level = np.nan\n        else:\n            self.method_mean_ci = method_mean_ci = kwargs_mean_ci.pop(\n                \"method\", \"bootstrap\"\n            )\n            self.mean_ci_level = mean_ci_level = 0.95\n\n        if method_mean_ci is not None and mean_pval &lt; 0.05:\n            self.mean_lb, self.mean_ub = mean_lb, mean_ub = circ_mean_ci(\n                alpha=self.alpha,\n                w=self.w,\n                mean=self.mean,\n                r=self.r,\n                n=self.n,\n                ci=mean_ci_level,\n                method=method_mean_ci,\n            )\n        else:\n            self.mean_lb, self.mean_ub = np.nan, np.nan\n\n        # angular deviation, circular standard deviation, adjusted resultant vector length (if needed)\n        self.s = angular_std(r=r, bin_size=bin_size)\n        self.s0 = circ_std(r=r, bin_size=bin_size)\n\n        # angular median\n        if n &gt; 10000 and kwargs_median[\"method\"] is not None:\n            print(\n                \"Sample size is large (n&gt;10000), it will take a while to find the median.\\nOr set `kwargs_median={'method': None}` to skip.\"\n            )\n\n        self.median = median = circ_median(\n            alpha=alpha,\n            w=w,\n            method=kwargs_median[\"method\"],\n            return_average=kwargs_median[\"return_average\"],\n            average_method=kwargs_median[\"average_method\"],\n        )\n\n        # confidence inerval for angular median (only for ungrouped data)\n        # it's unclear how to do it for grouped data.\n        if not grouped and not np.isnan(median):\n            self.median_lb, self.median_ub, self.median_ci_level = circ_median_ci(\n                median=float(median), alpha=alpha\n            )\n\n        self.skewness = circ_skewness(alpha=alpha, w=w)\n        self.kurtosis = circ_kurtosis(alpha=alpha, w=w)\n\n        # check multimodality\n        self.mixtures = []\n        if n_clusters_max &gt; 1:\n            for k in range(1, n_clusters_max + 1):\n                m = MovM(\n                    n_clusters=k,\n                    full_cycle=full_cycle,\n                    unit=\"radian\",\n                    random_seed=0,\n                )\n                m.fit(np.repeat(alpha, w))\n                self.mixtures.append(m)\n            self.mixtures_BIC = [m.compute_BIC() for m in self.mixtures]\n            if not np.isnan(self.mixtures_BIC).all():\n                self.mixture_opt = self.mixtures[np.nanargmin(self.mixtures_BIC)]\n            else:\n                self.mixture_opt = None\n\n    def __repr__(self):\n        unit = self.unit\n        k = self.full_cycle\n\n        docs = \"Circular Data\\n\"\n        docs += \"=============\\n\\n\"\n\n        docs += \"Summary\\n\"\n        docs += \"-------\\n\"\n        docs += \"  Grouped?: Yes\\n\" if self.grouped else \"  Grouped?: No\\n\"\n        if self.n_clusters_max &gt; 1 and self.mixture_opt is not None:\n            docs += (\n                \"  Unimodal?: Yes \\n\"\n                if len(self.mixture_opt.m_) == 1\n                else f\"  Unimodal?: No (n_clusters={len(self.mixture_opt.m_)}) \\n\"\n            )\n\n        docs += f\"  Unit: {unit}\\n\"\n        docs += f\"  Sample size: {self.n}\\n\"\n\n        docs += f\"  Angular mean: {rad2data(self.mean, k=k):.02f} ( p={self.mean_test_result.pval:.04f} {significance_code(self.mean_test_result.pval)} ) \\n\"\n\n        if hasattr(self, \"mean_lb\") and not np.isnan(self.mean_lb):\n            docs += f\"  Angular mean CI ({self.mean_ci_level:.2f}): {rad2data(self.mean_lb, k=k):.02f} - {rad2data(self.mean_ub, k=k):.02f}\\n\"\n\n        docs += f\"  Angular median: {rad2data(self.median, k=k):.02f} \\n\"\n        if hasattr(self, \"median_lb\") and not np.isnan(self.median_lb):\n            docs += f\"  Angular median CI ({self.median_ci_level:.2f}): {rad2data(self.median_lb, k=k):.02f} - {rad2data(self.median_ub, k=k):.02f}\\n\"\n\n        docs += f\"  Angular deviation (s): {rad2data(self.s, k=k):.02f} \\n\"\n        docs += f\"  Circular standard deviation (s0): {rad2data(self.s0, k=k):.02f} \\n\"\n        docs += f\"  Concentration (r): {self.r:0.2f}\\n\"\n        docs += f\"  Concentration (kappa): {self.kappa:0.2f}\\n\"\n        docs += f\"  Skewness: {self.skewness:0.3f}\\n\"\n        docs += f\"  Kurtosis: {self.kurtosis:0.3f}\\n\"\n\n        docs += \"\\n\"\n\n        docs += \"Signif. codes:\\n\"\n        docs += \"--------------\\n\"\n        docs += \" 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\\n\\n\"\n\n        docs += \"Method\\n\"\n        docs += \"------\\n\"\n        docs += f\"  Angular median: {self.kwargs_median['method']}\\n\"\n        docs += f\"  Angular mean CI: {self.method_mean_ci}\\n\"\n\n        return docs\n\n    def __str__(self):\n        return self.__repr__()\n\n    def summary(self):\n        r\"\"\"\n        Summary of basic statistics for circular data.\n\n        This method generates a textual summary of the key descriptive and inferential\n        statistics computed for the circular data. It provides information about\n        the data type, concentration, dispersion, and more.\n\n        The summary includes the following components:\n\n        1. **Grouping**:\n\n            Indicates whether the data is grouped (binned) or ungrouped.\n\n        2. **Unimodality**:\n\n            For models with mixtures of von Mises distributions, it specifies whether\n        the data is unimodal or multimodal, along with the number of clusters if applicable.\n\n        3. **Data Characteristics**:\n\n            - The unit of measurement (e.g., degrees, radians, hours).\n            - Total sample size, including weights if provided.\n\n        4. **Angular Mean**:\n\n            - The angular mean, with its corresponding p-value from the Rayleigh test.\n            - The confidence interval (CI) for the angular mean, if available.\n\n        5. **Angular Median**:\n\n            - The angular median, representing the central tendency.\n            - The confidence interval (CI) for the angular median, if applicable.\n\n        6. **Measures of Dispersion**:\n\n            - Angular deviation ($s$): A measure of spread in circular data.\n            - Circular standard deviation ($s_0$): An alternative dispersion measure.\n\n        7. **Measures of Concentration**:\n\n            - Resultant vector length ($r$): A measure of data concentration, ranging from 0 (uniform) to 1 (highly concentrated).\n            - Concentration parameter ($\\kappa$): Indicates sharpness or clustering of the data.\n\n        8. **Higher-Order Statistics**:\n\n            - Circular skewness: A measure of asymmetry.\n            - Circular kurtosis: A measure of peakedness or flatness relative to a uniform distribution.\n\n        9. **Significance Codes**:\n\n            - A guide to interpret the p-values of statistical tests.\n\n        10. **Methods**:\n\n            - The method used for calculating the angular median.\n            - The method used for estimating confidence intervals for the angular mean.\n        \"\"\"\n\n        return self.__repr__()\n\n    def plot(self, ax=None, config=None):\n        \"\"\"\n        Visualize circular data.\n\n        This method provides various visualization options for circular data, including scatter\n        plots, density plots, and rose diagrams. It is a wrapper around the `circ_plot` function.\n\n        Parameters\n        ----------\n        ax : matplotlib.axes._axes.Axes, optional\n            The matplotlib Axes object where the plot will be drawn. If None, a new Axes object\n            is created. Default is None.\n        config: dict, optional\n            Configuration dictionary that overrides defaults.\n\n        Returns\n        -------\n        ax : matplotlib.axes._axes.Axes\n            The matplotlib Axes object containing the plot.\n\n        Notes\n        -----\n        - This method supports both grouped and ungrouped data.\n        - Density estimation can be performed using either nonparametric methods or mixtures\n        of von Mises distributions.\n        - The rose diagram represents grouped data as a histogram over angular bins.\n        - Confidence intervals for the mean and median are plotted as arcs on the circle.\n\n        Examples\n        --------\n        ```\n        from pycircstat2 import load_data, Circular\n\n        data = load_data(\"B3\", source=\"fisher\")[\"\u03b8\"].values\n        c = Circular(data, unit=\"degree\")\n        c.plot(config={\"scatter\": {\"color\" : \"blue\", \"size\": 15}})\n        ```\n\n        See docstring of `circ_plot` for more examples and customization options.\n        \"\"\"\n        ax = circ_plot(self, ax=ax, config=config)\n</code></pre>"},{"location":"reference/base/#pycircstat2.base.Circular.summary","title":"<code>summary()</code>","text":"<p>Summary of basic statistics for circular data.</p> <p>This method generates a textual summary of the key descriptive and inferential statistics computed for the circular data. It provides information about the data type, concentration, dispersion, and more.</p> <p>The summary includes the following components:</p> <ol> <li> <p>Grouping:</p> <p>Indicates whether the data is grouped (binned) or ungrouped.</p> </li> <li> <p>Unimodality:</p> <p>For models with mixtures of von Mises distributions, it specifies whether the data is unimodal or multimodal, along with the number of clusters if applicable.</p> </li> <li> <p>Data Characteristics:</p> <ul> <li>The unit of measurement (e.g., degrees, radians, hours).</li> <li>Total sample size, including weights if provided.</li> </ul> </li> <li> <p>Angular Mean:</p> <ul> <li>The angular mean, with its corresponding p-value from the Rayleigh test.</li> <li>The confidence interval (CI) for the angular mean, if available.</li> </ul> </li> <li> <p>Angular Median:</p> <ul> <li>The angular median, representing the central tendency.</li> <li>The confidence interval (CI) for the angular median, if applicable.</li> </ul> </li> <li> <p>Measures of Dispersion:</p> <ul> <li>Angular deviation (\\(s\\)): A measure of spread in circular data.</li> <li>Circular standard deviation (\\(s_0\\)): An alternative dispersion measure.</li> </ul> </li> <li> <p>Measures of Concentration:</p> <ul> <li>Resultant vector length (\\(r\\)): A measure of data concentration, ranging from 0 (uniform) to 1 (highly concentrated).</li> <li>Concentration parameter (\\(\\kappa\\)): Indicates sharpness or clustering of the data.</li> </ul> </li> <li> <p>Higher-Order Statistics:</p> <ul> <li>Circular skewness: A measure of asymmetry.</li> <li>Circular kurtosis: A measure of peakedness or flatness relative to a uniform distribution.</li> </ul> </li> <li> <p>Significance Codes:</p> <ul> <li>A guide to interpret the p-values of statistical tests.</li> </ul> </li> <li> <p>Methods:</p> <ul> <li>The method used for calculating the angular median.</li> <li>The method used for estimating confidence intervals for the angular mean.</li> </ul> </li> </ol> Source code in <code>pycircstat2/base.py</code> <pre><code>def summary(self):\n    r\"\"\"\n    Summary of basic statistics for circular data.\n\n    This method generates a textual summary of the key descriptive and inferential\n    statistics computed for the circular data. It provides information about\n    the data type, concentration, dispersion, and more.\n\n    The summary includes the following components:\n\n    1. **Grouping**:\n\n        Indicates whether the data is grouped (binned) or ungrouped.\n\n    2. **Unimodality**:\n\n        For models with mixtures of von Mises distributions, it specifies whether\n    the data is unimodal or multimodal, along with the number of clusters if applicable.\n\n    3. **Data Characteristics**:\n\n        - The unit of measurement (e.g., degrees, radians, hours).\n        - Total sample size, including weights if provided.\n\n    4. **Angular Mean**:\n\n        - The angular mean, with its corresponding p-value from the Rayleigh test.\n        - The confidence interval (CI) for the angular mean, if available.\n\n    5. **Angular Median**:\n\n        - The angular median, representing the central tendency.\n        - The confidence interval (CI) for the angular median, if applicable.\n\n    6. **Measures of Dispersion**:\n\n        - Angular deviation ($s$): A measure of spread in circular data.\n        - Circular standard deviation ($s_0$): An alternative dispersion measure.\n\n    7. **Measures of Concentration**:\n\n        - Resultant vector length ($r$): A measure of data concentration, ranging from 0 (uniform) to 1 (highly concentrated).\n        - Concentration parameter ($\\kappa$): Indicates sharpness or clustering of the data.\n\n    8. **Higher-Order Statistics**:\n\n        - Circular skewness: A measure of asymmetry.\n        - Circular kurtosis: A measure of peakedness or flatness relative to a uniform distribution.\n\n    9. **Significance Codes**:\n\n        - A guide to interpret the p-values of statistical tests.\n\n    10. **Methods**:\n\n        - The method used for calculating the angular median.\n        - The method used for estimating confidence intervals for the angular mean.\n    \"\"\"\n\n    return self.__repr__()\n</code></pre>"},{"location":"reference/base/#pycircstat2.base.Circular.plot","title":"<code>plot(ax=None, config=None)</code>","text":"<p>Visualize circular data.</p> <p>This method provides various visualization options for circular data, including scatter plots, density plots, and rose diagrams. It is a wrapper around the <code>circ_plot</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The matplotlib Axes object where the plot will be drawn. If None, a new Axes object is created. Default is None.</p> <code>None</code> <code>config</code> <p>Configuration dictionary that overrides defaults.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The matplotlib Axes object containing the plot.</p> Notes <ul> <li>This method supports both grouped and ungrouped data.</li> <li>Density estimation can be performed using either nonparametric methods or mixtures of von Mises distributions.</li> <li>The rose diagram represents grouped data as a histogram over angular bins.</li> <li>Confidence intervals for the mean and median are plotted as arcs on the circle.</li> </ul> <p>Examples:</p> <pre><code>from pycircstat2 import load_data, Circular\n\ndata = load_data(\"B3\", source=\"fisher\")[\"\u03b8\"].values\nc = Circular(data, unit=\"degree\")\nc.plot(config={\"scatter\": {\"color\" : \"blue\", \"size\": 15}})\n</code></pre> <p>See docstring of <code>circ_plot</code> for more examples and customization options.</p> Source code in <code>pycircstat2/base.py</code> <pre><code>def plot(self, ax=None, config=None):\n    \"\"\"\n    Visualize circular data.\n\n    This method provides various visualization options for circular data, including scatter\n    plots, density plots, and rose diagrams. It is a wrapper around the `circ_plot` function.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes._axes.Axes, optional\n        The matplotlib Axes object where the plot will be drawn. If None, a new Axes object\n        is created. Default is None.\n    config: dict, optional\n        Configuration dictionary that overrides defaults.\n\n    Returns\n    -------\n    ax : matplotlib.axes._axes.Axes\n        The matplotlib Axes object containing the plot.\n\n    Notes\n    -----\n    - This method supports both grouped and ungrouped data.\n    - Density estimation can be performed using either nonparametric methods or mixtures\n    of von Mises distributions.\n    - The rose diagram represents grouped data as a histogram over angular bins.\n    - Confidence intervals for the mean and median are plotted as arcs on the circle.\n\n    Examples\n    --------\n    ```\n    from pycircstat2 import load_data, Circular\n\n    data = load_data(\"B3\", source=\"fisher\")[\"\u03b8\"].values\n    c = Circular(data, unit=\"degree\")\n    c.plot(config={\"scatter\": {\"color\" : \"blue\", \"size\": 15}})\n    ```\n\n    See docstring of `circ_plot` for more examples and customization options.\n    \"\"\"\n    ax = circ_plot(self, ax=ax, config=config)\n</code></pre>"},{"location":"reference/clustering/","title":"Clustering","text":""},{"location":"reference/clustering/#pycircstat2.clustering.MovM","title":"<code>MovM</code>","text":"<p>Mixture of von Mises (MovM) Clustering.</p> <p>This class implements the Expectation-Maximization (EM) algorithm for clustering  circular data using a mixture of von Mises distributions. It is analogous to  Gaussian Mixture Models (GMM) but adapted for directional statistics.</p> <p>Parameters:</p> Name Type Description Default <code>burnin</code> <code>int</code> <p>Number of initial iterations before checking for convergence.</p> <code>30</code> <code>n_clusters</code> <code>int</code> <p>The number of von Mises distributions (clusters) to fit.</p> <code>5</code> <code>n_iters</code> <code>int</code> <p>Maximum number of iterations for the EM algorithm.</p> <code>100</code> <code>full_cycle</code> <code>int</code> <p>Used for converting degree-based data into radians.</p> <code>360</code> <code>unit</code> <code>('degree', 'radian')</code> <p>Specifies whether input data is in degrees or radians.</p> <code>\"degree\"</code> <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>2046</code> <code>threshold</code> <code>float</code> <p>Convergence threshold based on the negative log-likelihood difference.</p> <code>1e-16</code> <p>Attributes:</p> Name Type Description <code>converged</code> <code>bool</code> <p>Whether the algorithm has converged.</p> <code>nLL</code> <code>ndarray</code> <p>Array of negative log-likelihood values over iterations.</p> <code>m</code> <code>ndarray</code> <p>Cluster means (circular means).</p> <code>r</code> <code>ndarray</code> <p>Cluster mean resultant vectors.</p> <code>p</code> <code>ndarray</code> <p>Cluster probabilities.</p> <code>kappa</code> <code>ndarray</code> <p>Concentration parameters for each von Mises component.</p> <code>gamma</code> <code>ndarray</code> <p>Responsibility matrix (posterior probabilities of clusters for each data point).</p> <code>labels</code> <code>ndarray</code> <p>The most probable cluster assignment for each data point.</p> <code>params_</code> <code>list of dict or None</code> <p>Per-component parameter dictionaries ({\"mu\", \"kappa\"}) populated after :meth:<code>fit</code>.</p> <p>Examples:</p> <pre><code>import numpy as np\nfrom pycircstat2.clustering import MovM\nnp.random.seed(42)\nx1 = np.random.vonmises(mu=0, kappa=5, size=100)\nx2 = np.random.vonmises(mu=np.pi, kappa=10, size=100)\nx = np.concatenate([x1, x2])\nnp.random.shuffle(x)\nmovm = MovM(n_clusters=2, n_iters=200, unit=\"radian\", random_seed=42)\nmovm.fit(x, verbose=False)\n</code></pre> Source code in <code>pycircstat2/clustering.py</code> <pre><code>class MovM:\n    \"\"\"\n    Mixture of von Mises (MovM) Clustering.\n\n    This class implements the Expectation-Maximization (EM) algorithm for clustering \n    circular data using a mixture of von Mises distributions. It is analogous to \n    Gaussian Mixture Models (GMM) but adapted for directional statistics.\n\n    Parameters\n    ----------\n    burnin : int, default=30\n        Number of initial iterations before checking for convergence.\n    n_clusters : int, default=5\n        The number of von Mises distributions (clusters) to fit.\n    n_iters : int, default=100\n        Maximum number of iterations for the EM algorithm.\n    full_cycle : int, default=360\n        Used for converting degree-based data into radians.\n    unit : {\"degree\", \"radian\"}, default=\"degree\"\n        Specifies whether input data is in degrees or radians.\n    random_seed : int, default=2046\n        Random seed for reproducibility.\n    threshold : float, default=1e-16\n        Convergence threshold based on the negative log-likelihood difference.\n\n    Attributes\n    ----------\n    converged : bool\n        Whether the algorithm has converged.\n    nLL : np.ndarray\n        Array of negative log-likelihood values over iterations.\n    m : np.ndarray\n        Cluster means (circular means).\n    r : np.ndarray\n        Cluster mean resultant vectors.\n    p : np.ndarray\n        Cluster probabilities.\n    kappa : np.ndarray\n        Concentration parameters for each von Mises component.\n    gamma : np.ndarray\n        Responsibility matrix (posterior probabilities of clusters for each data point).\n    labels : np.ndarray\n        The most probable cluster assignment for each data point.\n    params_ : list of dict or None\n        Per-component parameter dictionaries ({\"mu\", \"kappa\"}) populated after :meth:`fit`.\n\n    Examples\n    --------\n        import numpy as np\n        from pycircstat2.clustering import MovM\n        np.random.seed(42)\n        x1 = np.random.vonmises(mu=0, kappa=5, size=100)\n        x2 = np.random.vonmises(mu=np.pi, kappa=10, size=100)\n        x = np.concatenate([x1, x2])\n        np.random.shuffle(x)\n        movm = MovM(n_clusters=2, n_iters=200, unit=\"radian\", random_seed=42)\n        movm.fit(x, verbose=False)\n    \"\"\"\n\n    def __init__(\n        self,\n        burnin: int = 30,\n        n_clusters: int = 5,\n        n_iters: int = 100,\n        full_cycle: Union[int, float] = 360,\n        unit: str = \"degree\",\n        random_seed: Optional[int] = 2046,\n        threshold: float = 1e-16,\n    ):\n        if burnin &lt; 0:\n            raise ValueError(\"`burnin` must be non-negative.\")\n        if n_clusters &lt;= 0:\n            raise ValueError(\"`n_clusters` must be a positive integer.\")\n        if n_iters &lt;= 0:\n            raise ValueError(\"`n_iters` must be a positive integer.\")\n        if threshold &lt;= 0:\n            raise ValueError(\"`threshold` must be positive.\")\n        if unit not in {\"degree\", \"radian\"}:\n            raise ValueError(\"`unit` must be either 'degree' or 'radian'.\")\n\n        self.burnin = burnin\n        self.threshold = threshold\n        self.n_clusters = n_clusters\n        self.n_iters = n_iters\n        self.full_cycle = full_cycle\n        self.unit = unit\n        self._rng = np.random.default_rng(random_seed)\n\n        self.converged = False\n        self.converged_iters: Optional[int] = None\n\n        # Attributes populated after fitting (scikit-learn style trailing underscore)\n        self.m_: Optional[np.ndarray] = None\n        self.r_: Optional[np.ndarray] = None\n        self.p_: Optional[np.ndarray] = None\n        self.kappa_: Optional[np.ndarray] = None\n        self.gamma_: Optional[np.ndarray] = None\n        self.labels_: Optional[np.ndarray] = None\n        self.nLL: Optional[np.ndarray] = None\n        self.data: Optional[np.ndarray] = None\n        self.alpha: Optional[np.ndarray] = None\n        self.n: Optional[int] = None\n        self.params_: Optional[List[Dict[str, float]]] = None\n\n    def _initialize(\n        self,\n        x: np.ndarray,\n        n_clusters_init: int,\n    ) -&gt; tuple:\n        \"\"\"\n        Initializes cluster parameters before running the EM algorithm.\n\n        Parameters\n        ----------\n        x : np.ndarray\n            Input circular data in radians.\n        n_clusters_init : int\n            Number of initial clusters.\n\n        Returns\n        -------\n        tuple\n            - m (np.ndarray): Initial cluster means.\n            - kappa (np.ndarray): Initial concentration parameters.\n            - p (np.ndarray): Initial cluster probabilities.\n        \"\"\"\n        n = len(x)\n        if n_clusters_init &gt; n:\n            raise ValueError(\n                \"Number of clusters cannot exceed number of observations during initialisation.\"\n            )\n\n        # Randomly assign each observation to a cluster ensuring no cluster is empty\n        for _ in range(100):\n            labels = self._rng.integers(n_clusters_init, size=n)\n            if all(np.any(labels == c) for c in range(n_clusters_init)):\n                break\n        else:\n            raise RuntimeError(\"Failed to initialise clusters without empty components.\")\n\n        means = np.zeros(n_clusters_init, dtype=float)\n        resultants = np.zeros(n_clusters_init, dtype=float)\n        kappas = np.zeros(n_clusters_init, dtype=float)\n\n        for c in range(n_clusters_init):\n            subset = x[labels == c]\n            m_c, r_c = circ_mean_and_r(subset)\n            means[c] = m_c\n            resultants[c] = r_c\n            kappa_c = circ_kappa(r=r_c)\n            if not np.isfinite(kappa_c):\n                kappa_c = 1e-3\n            kappas[c] = max(kappa_c, 1e-3)\n\n        p = np.full(n_clusters_init, 1.0 / n_clusters_init, dtype=float)\n        return means, kappas, p\n\n    def fit(self, X: np.ndarray, verbose: Union[bool, int] = 0):\n        \"\"\"\n        Fits the mixture of von Mises model to the given data using the EM algorithm.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            Input data points in degrees or radians.\n        verbose : bool or int, default=0\n            If True, prints progress every iteration. If an integer, prints every `verbose` iterations.\n\n        Updates\n        -------\n        - self.m : Fitted cluster means.\n        - self.kappa : Fitted concentration parameters.\n        - self.p : Fitted cluster probabilities.\n        - self.labels : Final cluster assignments.\n        \"\"\"\n        X = np.asarray(X, dtype=float).reshape(-1)\n        if X.size == 0:\n            raise ValueError(\"Input data must contain at least one observation.\")\n\n        alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n        self.data = X\n        self.alpha = alpha\n        self.n = n = alpha.size\n\n        means, kappa, p = self._initialize(alpha, self.n_clusters)\n\n        if verbose:\n            header = \"Iter\".ljust(10) + \"nLL\"\n            print(header)\n\n        nLL_history = np.full(self.n_iters, np.nan)\n\n        for iteration in range(self.n_iters):\n            log_responsibilities = self._log_gamma(alpha, p, means, kappa)\n            log_norm = np.logaddexp.reduce(log_responsibilities, axis=0)\n            gamma_normed = np.exp(log_responsibilities - log_norm)\n\n            # M-step updates\n            p = gamma_normed.sum(axis=1)\n            p /= p.sum()\n\n            means_updated = np.zeros_like(means)\n            resultants = np.zeros_like(means)\n            for c in range(self.n_clusters):\n                weights = gamma_normed[c]\n                if np.allclose(weights.sum(), 0.0):\n                    means_updated[c] = means[c]\n                    resultants[c] = 0.0\n                else:\n                    mc, rc = circ_mean_and_r(alpha, w=weights)\n                    means_updated[c] = mc\n                    resultants[c] = rc\n\n            kappas = np.array([max(circ_kappa(r=rc), 1e-3) for rc in resultants])\n\n            means, kappa = means_updated, kappas\n\n            nLL = -np.sum(log_norm)\n            nLL_history[iteration] = nLL\n\n            if verbose and (iteration % int(verbose or 1) == 0):\n                print(f\"{iteration}\".ljust(10) + f\"{nLL:.3f}\")\n\n            if (\n                iteration &gt; self.burnin\n                and np.abs(nLL_history[iteration] - nLL_history[iteration - 1])\n                &lt; self.threshold\n            ):\n                self.converged = True\n                self.converged_iters = iteration + 1\n                if verbose:\n                    print(f\"Converged at iter {iteration}. Final nLL = {nLL:.3f}\\n\")\n                break\n        else:\n            if verbose:\n                print(\n                    f\"Reached max iter {self.n_iters}. Final nLL = {nLL:.3f}\\n\"\n                )\n\n        self.nLL = nLL_history[~np.isnan(nLL_history)]\n\n        self.m_ = means\n        self.r_ = resultants\n        self.p_ = p\n        self.kappa_ = kappa\n        self.params_ = [\n            {\"mu\": float(self.m_[i]), \"kappa\": float(self.kappa_[i])} for i in range(self.n_clusters)\n        ]\n        log_gamma_final = self._log_gamma(alpha, p, means, kappa)\n        log_norm_final = np.logaddexp.reduce(log_gamma_final, axis=0, keepdims=True)\n        gamma_final = np.exp(log_gamma_final - log_norm_final)\n        self.gamma_ = gamma_final\n        self.labels_ = gamma_final.argmax(axis=0)\n        return self\n\n\n    def compute_gamma(\n        self,\n        alpha: np.ndarray,\n        p: np.ndarray,\n        m: np.ndarray,\n        kappa: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Computes posterior probabilities (responsibilities) for each cluster.\n\n        Returns\n        -------\n        np.ndarray\n            Cluster assignment probabilities for each data point.\n        \"\"\"\n        log_gamma = self._log_gamma(alpha, p, m, kappa)\n        gamma = np.exp(log_gamma)\n        gamma /= gamma.sum(axis=0, keepdims=True)\n        return gamma\n\n    def _log_gamma(\n        self,\n        alpha: np.ndarray,\n        p: np.ndarray,\n        m: np.ndarray,\n        kappa: np.ndarray,\n    ) -&gt; np.ndarray:\n        log_prob = np.vstack(\n            [\n                np.log(p[i] + 1e-32) + vonmises.logpdf(alpha, m[i], kappa[i])\n                for i in range(self.n_clusters)\n            ]\n        )\n        return log_prob\n\n    def compute_nLL(\n        self,\n        alpha: np.ndarray,\n        p: np.ndarray,\n        m: np.ndarray,\n        kappa: np.ndarray,\n    ) -&gt; float:\n        \"\"\"\n        Computes the negative log-likelihood.\n\n        Parameters\n        ----------\n        alpha : np.ndarray\n            Input data in radians.\n        p : np.ndarray\n            Component probabilities.\n        m : np.ndarray\n            Component means.\n        kappa : np.ndarray\n            Component concentrations.\n\n        Returns\n        -------\n        float\n            The negative log-likelihood value.\n        \"\"\"\n        log_gamma = self._log_gamma(alpha, p, m, kappa)\n        log_norm = np.logaddexp.reduce(log_gamma, axis=0)\n        return -float(np.sum(log_norm))\n\n    def compute_BIC(self) -&gt; float:\n        \"\"\"\n        Computes the Bayesian Information Criterion (BIC) for model selection.\n\n        Returns\n        -------\n        float\n            The computed BIC value.\n        \"\"\"\n        if self.gamma_ is None:\n            raise ValueError(\"Model must be fitted before computing BIC.\")\n        nLL = self.compute_nLL(self.alpha, self.p_, self.m_, self.kappa_)\n        nparams = self.n_clusters * 3 - 1  # n_means + n_kappas + (n_ps - 1)\n        bic = 2 * nLL + np.log(self.n) * nparams\n\n        return bic\n\n    def predict_density(\n        self,\n        x: Optional[np.ndarray] = None,\n        unit: Union[str, None] = None,\n        full_cycle: Union[float, int, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Predicts density estimates for given points.\n\n        Parameters\n        ----------\n        x : np.ndarray, optional\n            Points at which to estimate the density.\n        unit : {\"degree\", \"radian\"}, optional\n            Specifies whether input data is in degrees or radians.\n        full_cycle : int, optional\n            Number of intervals for data conversion.\n\n        Returns\n        -------\n        np.ndarray\n            Estimated density at the provided points.\n        \"\"\"\n        unit = self.unit if unit is None else unit\n        full_cycle = self.full_cycle if full_cycle is None else full_cycle\n\n        if x is None:\n            x = np.linspace(0, 2 * np.pi, 400, endpoint=False)\n            if unit == \"degree\":\n                x = np.rad2deg(x)\n        x = np.asarray(x, dtype=float).reshape(-1)\n        alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n\n        density_components = np.array(\n            [\n                p_c * vonmises.pdf(alpha, mu=m_c, kappa=k_c)\n                for p_c, m_c, k_c in zip(self.p_, self.m_, self.kappa_)\n            ]\n        )\n        return density_components.sum(axis=0)\n\n    def predict_proba(\n        self,\n        x: np.ndarray,\n        unit: Union[str, None] = None,\n        full_cycle: Union[float, int, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Returns component posterior probabilities for new observations.\n        \"\"\"\n        if self.p_ is None or self.kappa_ is None or self.m_ is None:\n            raise ValueError(\"Model must be fitted before calling predict_proba().\")\n\n        unit = self.unit if unit is None else unit\n        full_cycle = self.full_cycle if full_cycle is None else full_cycle\n        x = np.asarray(x, dtype=float).reshape(-1)\n        alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n\n        log_gamma = self._log_gamma(alpha, self.p_, self.m_, self.kappa_)\n        log_norm = np.logaddexp.reduce(log_gamma, axis=0, keepdims=True)\n        return np.exp(log_gamma - log_norm)\n\n    def predict(\n        self,\n        x: np.ndarray,\n        unit: Union[str, None] = None,\n        full_cycle: Union[float, int, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Predicts cluster assignments for new data.\n\n        Parameters\n        ----------\n        x : np.ndarray\n            New data points in degrees or radians.\n\n        Returns\n        -------\n        np.ndarray\n            Predicted cluster labels.\n        \"\"\"\n        proba = self.predict_proba(x, unit=unit, full_cycle=full_cycle)\n        return proba.argmax(axis=0)\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MovM.fit","title":"<code>fit(X, verbose=0)</code>","text":"<p>Fits the mixture of von Mises model to the given data using the EM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data points in degrees or radians.</p> required <code>verbose</code> <code>bool or int</code> <p>If True, prints progress every iteration. If an integer, prints every <code>verbose</code> iterations.</p> <code>0</code> Updates <ul> <li>self.m : Fitted cluster means.</li> <li>self.kappa : Fitted concentration parameters.</li> <li>self.p : Fitted cluster probabilities.</li> <li>self.labels : Final cluster assignments.</li> </ul> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def fit(self, X: np.ndarray, verbose: Union[bool, int] = 0):\n    \"\"\"\n    Fits the mixture of von Mises model to the given data using the EM algorithm.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data points in degrees or radians.\n    verbose : bool or int, default=0\n        If True, prints progress every iteration. If an integer, prints every `verbose` iterations.\n\n    Updates\n    -------\n    - self.m : Fitted cluster means.\n    - self.kappa : Fitted concentration parameters.\n    - self.p : Fitted cluster probabilities.\n    - self.labels : Final cluster assignments.\n    \"\"\"\n    X = np.asarray(X, dtype=float).reshape(-1)\n    if X.size == 0:\n        raise ValueError(\"Input data must contain at least one observation.\")\n\n    alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n    self.data = X\n    self.alpha = alpha\n    self.n = n = alpha.size\n\n    means, kappa, p = self._initialize(alpha, self.n_clusters)\n\n    if verbose:\n        header = \"Iter\".ljust(10) + \"nLL\"\n        print(header)\n\n    nLL_history = np.full(self.n_iters, np.nan)\n\n    for iteration in range(self.n_iters):\n        log_responsibilities = self._log_gamma(alpha, p, means, kappa)\n        log_norm = np.logaddexp.reduce(log_responsibilities, axis=0)\n        gamma_normed = np.exp(log_responsibilities - log_norm)\n\n        # M-step updates\n        p = gamma_normed.sum(axis=1)\n        p /= p.sum()\n\n        means_updated = np.zeros_like(means)\n        resultants = np.zeros_like(means)\n        for c in range(self.n_clusters):\n            weights = gamma_normed[c]\n            if np.allclose(weights.sum(), 0.0):\n                means_updated[c] = means[c]\n                resultants[c] = 0.0\n            else:\n                mc, rc = circ_mean_and_r(alpha, w=weights)\n                means_updated[c] = mc\n                resultants[c] = rc\n\n        kappas = np.array([max(circ_kappa(r=rc), 1e-3) for rc in resultants])\n\n        means, kappa = means_updated, kappas\n\n        nLL = -np.sum(log_norm)\n        nLL_history[iteration] = nLL\n\n        if verbose and (iteration % int(verbose or 1) == 0):\n            print(f\"{iteration}\".ljust(10) + f\"{nLL:.3f}\")\n\n        if (\n            iteration &gt; self.burnin\n            and np.abs(nLL_history[iteration] - nLL_history[iteration - 1])\n            &lt; self.threshold\n        ):\n            self.converged = True\n            self.converged_iters = iteration + 1\n            if verbose:\n                print(f\"Converged at iter {iteration}. Final nLL = {nLL:.3f}\\n\")\n            break\n    else:\n        if verbose:\n            print(\n                f\"Reached max iter {self.n_iters}. Final nLL = {nLL:.3f}\\n\"\n            )\n\n    self.nLL = nLL_history[~np.isnan(nLL_history)]\n\n    self.m_ = means\n    self.r_ = resultants\n    self.p_ = p\n    self.kappa_ = kappa\n    self.params_ = [\n        {\"mu\": float(self.m_[i]), \"kappa\": float(self.kappa_[i])} for i in range(self.n_clusters)\n    ]\n    log_gamma_final = self._log_gamma(alpha, p, means, kappa)\n    log_norm_final = np.logaddexp.reduce(log_gamma_final, axis=0, keepdims=True)\n    gamma_final = np.exp(log_gamma_final - log_norm_final)\n    self.gamma_ = gamma_final\n    self.labels_ = gamma_final.argmax(axis=0)\n    return self\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MovM.compute_gamma","title":"<code>compute_gamma(alpha, p, m, kappa)</code>","text":"<p>Computes posterior probabilities (responsibilities) for each cluster.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Cluster assignment probabilities for each data point.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def compute_gamma(\n    self,\n    alpha: np.ndarray,\n    p: np.ndarray,\n    m: np.ndarray,\n    kappa: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"\n    Computes posterior probabilities (responsibilities) for each cluster.\n\n    Returns\n    -------\n    np.ndarray\n        Cluster assignment probabilities for each data point.\n    \"\"\"\n    log_gamma = self._log_gamma(alpha, p, m, kappa)\n    gamma = np.exp(log_gamma)\n    gamma /= gamma.sum(axis=0, keepdims=True)\n    return gamma\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MovM.compute_nLL","title":"<code>compute_nLL(alpha, p, m, kappa)</code>","text":"<p>Computes the negative log-likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Input data in radians.</p> required <code>p</code> <code>ndarray</code> <p>Component probabilities.</p> required <code>m</code> <code>ndarray</code> <p>Component means.</p> required <code>kappa</code> <code>ndarray</code> <p>Component concentrations.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The negative log-likelihood value.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def compute_nLL(\n    self,\n    alpha: np.ndarray,\n    p: np.ndarray,\n    m: np.ndarray,\n    kappa: np.ndarray,\n) -&gt; float:\n    \"\"\"\n    Computes the negative log-likelihood.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Input data in radians.\n    p : np.ndarray\n        Component probabilities.\n    m : np.ndarray\n        Component means.\n    kappa : np.ndarray\n        Component concentrations.\n\n    Returns\n    -------\n    float\n        The negative log-likelihood value.\n    \"\"\"\n    log_gamma = self._log_gamma(alpha, p, m, kappa)\n    log_norm = np.logaddexp.reduce(log_gamma, axis=0)\n    return -float(np.sum(log_norm))\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MovM.compute_BIC","title":"<code>compute_BIC()</code>","text":"<p>Computes the Bayesian Information Criterion (BIC) for model selection.</p> <p>Returns:</p> Type Description <code>float</code> <p>The computed BIC value.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def compute_BIC(self) -&gt; float:\n    \"\"\"\n    Computes the Bayesian Information Criterion (BIC) for model selection.\n\n    Returns\n    -------\n    float\n        The computed BIC value.\n    \"\"\"\n    if self.gamma_ is None:\n        raise ValueError(\"Model must be fitted before computing BIC.\")\n    nLL = self.compute_nLL(self.alpha, self.p_, self.m_, self.kappa_)\n    nparams = self.n_clusters * 3 - 1  # n_means + n_kappas + (n_ps - 1)\n    bic = 2 * nLL + np.log(self.n) * nparams\n\n    return bic\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MovM.predict_density","title":"<code>predict_density(x=None, unit=None, full_cycle=None)</code>","text":"<p>Predicts density estimates for given points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Points at which to estimate the density.</p> <code>None</code> <code>unit</code> <code>('degree', 'radian')</code> <p>Specifies whether input data is in degrees or radians.</p> <code>\"degree\"</code> <code>full_cycle</code> <code>int</code> <p>Number of intervals for data conversion.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Estimated density at the provided points.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict_density(\n    self,\n    x: Optional[np.ndarray] = None,\n    unit: Union[str, None] = None,\n    full_cycle: Union[float, int, None] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts density estimates for given points.\n\n    Parameters\n    ----------\n    x : np.ndarray, optional\n        Points at which to estimate the density.\n    unit : {\"degree\", \"radian\"}, optional\n        Specifies whether input data is in degrees or radians.\n    full_cycle : int, optional\n        Number of intervals for data conversion.\n\n    Returns\n    -------\n    np.ndarray\n        Estimated density at the provided points.\n    \"\"\"\n    unit = self.unit if unit is None else unit\n    full_cycle = self.full_cycle if full_cycle is None else full_cycle\n\n    if x is None:\n        x = np.linspace(0, 2 * np.pi, 400, endpoint=False)\n        if unit == \"degree\":\n            x = np.rad2deg(x)\n    x = np.asarray(x, dtype=float).reshape(-1)\n    alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n\n    density_components = np.array(\n        [\n            p_c * vonmises.pdf(alpha, mu=m_c, kappa=k_c)\n            for p_c, m_c, k_c in zip(self.p_, self.m_, self.kappa_)\n        ]\n    )\n    return density_components.sum(axis=0)\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MovM.predict_proba","title":"<code>predict_proba(x, unit=None, full_cycle=None)</code>","text":"<p>Returns component posterior probabilities for new observations.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict_proba(\n    self,\n    x: np.ndarray,\n    unit: Union[str, None] = None,\n    full_cycle: Union[float, int, None] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Returns component posterior probabilities for new observations.\n    \"\"\"\n    if self.p_ is None or self.kappa_ is None or self.m_ is None:\n        raise ValueError(\"Model must be fitted before calling predict_proba().\")\n\n    unit = self.unit if unit is None else unit\n    full_cycle = self.full_cycle if full_cycle is None else full_cycle\n    x = np.asarray(x, dtype=float).reshape(-1)\n    alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n\n    log_gamma = self._log_gamma(alpha, self.p_, self.m_, self.kappa_)\n    log_norm = np.logaddexp.reduce(log_gamma, axis=0, keepdims=True)\n    return np.exp(log_gamma - log_norm)\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MovM.predict","title":"<code>predict(x, unit=None, full_cycle=None)</code>","text":"<p>Predicts cluster assignments for new data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>New data points in degrees or radians.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted cluster labels.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict(\n    self,\n    x: np.ndarray,\n    unit: Union[str, None] = None,\n    full_cycle: Union[float, int, None] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts cluster assignments for new data.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        New data points in degrees or radians.\n\n    Returns\n    -------\n    np.ndarray\n        Predicted cluster labels.\n    \"\"\"\n    proba = self.predict_proba(x, unit=unit, full_cycle=full_cycle)\n    return proba.argmax(axis=0)\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MoKJ","title":"<code>MoKJ</code>","text":"<p>Mixture of Kato\u2013Jones (MoKJ) Clustering.</p> <p>EM algorithm for clustering circular data with a mixture of Kato\u2013Jones components (Kato &amp; Jones, 2015). Each component controls mean direction (mu), mean resultant length (gamma), and second-order moment magnitude/phase (rho, lam), thus flexibly capturing skewness and peakedness per mode.</p> References <ul> <li>Kato, S., &amp; Jones, M.C. (2015). A tractable and interpretable four-parameter   family of unimodal distributions on the circle. Biometrika, 102(1), 181\u2013190.</li> <li>Nagasaki, K., Kato, S., Nakanishi, W., &amp; Jones, M.C. (2024/2025).   Traffic count data analysis using mixtures of Kato\u2013Jones distributions.   JRSS C (Applied Statistics). (EM for KJ mixtures; reparametrization details.)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>burnin</code> <code>int</code> <p>Number of initial EM iterations before checking convergence.</p> <code>30</code> <code>n_clusters</code> <code>int</code> <p>Number of Kato\u2013Jones mixture components.</p> <code>5</code> <code>n_iters</code> <code>int</code> <p>Maximum EM iterations.</p> <code>100</code> <code>full_cycle</code> <code>int or float</code> <p>Used to convert degrees to radians when unit=\"degree\".</p> <code>360</code> <code>unit</code> <code>('degree', 'radian')</code> <p>Input unit of X.</p> <code>\"degree\"</code> <code>random_seed</code> <code>int or None</code> <p>RNG seed for initialization.</p> <code>2046</code> <code>threshold</code> <code>float</code> <p>Convergence threshold on |nLL_t - nLL_{t-1}|.</p> <code>1e-16</code> <code>mle_maxiter</code> <code>int</code> <p>Max iterations for per-component weighted MLE in M-step.</p> <code>500</code> <code>mle_ftol</code> <code>float</code> <p>Function tolerance for per-component weighted MLE.</p> <code>1e-9</code> <code>min_comp_weight</code> <code>float</code> <p>Minimum mixture weight; components below may be reinitialized/frozen.</p> <code>1e-6</code> Attributes (after fit) <p>converged : bool converged_iters : Optional[int] nLL : np.ndarray     Negative log-likelihood history (finite prefix). mu_ : np.ndarray  shape (K,) gamma_ : np.ndarray  shape (K,) rho_ : np.ndarray  shape (K,) lam_ : np.ndarray  shape (K,) p_ : np.ndarray  shape (K,)     Mixture weights. gamma_resp_ : np.ndarray  shape (K, n)     Responsibilities. labels_ : np.ndarray  shape (n,)     MAP component labels. data : np.ndarray     Original X as provided. alpha : np.ndarray     Data in radians. n : int params_ : list of dict or None     Per-component parameter dictionaries ({\"mu\", \"gamma\", \"rho\", \"lam\"}) after fit.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>class MoKJ:\n    \"\"\"\n    Mixture of Kato\u2013Jones (MoKJ) Clustering.\n\n    EM algorithm for clustering circular data with a mixture of Kato\u2013Jones\n    components (Kato &amp; Jones, 2015). Each component controls mean direction (mu),\n    mean resultant length (gamma), and second-order moment magnitude/phase (rho, lam),\n    thus flexibly capturing skewness and peakedness per mode.\n\n    References\n    ----------\n    - Kato, S., &amp; Jones, M.C. (2015). A tractable and interpretable four-parameter\n      family of unimodal distributions on the circle. *Biometrika*, 102(1), 181\u2013190.\n    - Nagasaki, K., Kato, S., Nakanishi, W., &amp; Jones, M.C. (2024/2025).\n      Traffic count data analysis using mixtures of Kato\u2013Jones distributions.\n      *JRSS C (Applied Statistics)*. (EM for KJ mixtures; reparametrization details.)\n\n    Parameters\n    ----------\n    burnin : int, default=30\n        Number of initial EM iterations before checking convergence.\n    n_clusters : int, default=5\n        Number of Kato\u2013Jones mixture components.\n    n_iters : int, default=100\n        Maximum EM iterations.\n    full_cycle : int or float, default=360\n        Used to convert degrees to radians when unit=\"degree\".\n    unit : {\"degree\", \"radian\"}, default=\"degree\"\n        Input unit of X.\n    random_seed : int or None, default=2046\n        RNG seed for initialization.\n    threshold : float, default=1e-16\n        Convergence threshold on |nLL_t - nLL_{t-1}|.\n    mle_maxiter : int, default=500\n        Max iterations for per-component weighted MLE in M-step.\n    mle_ftol : float, default=1e-9\n        Function tolerance for per-component weighted MLE.\n    min_comp_weight : float, default=1e-6\n        Minimum mixture weight; components below may be reinitialized/frozen.\n\n    Attributes (after fit)\n    ----------------------\n    converged : bool\n    converged_iters : Optional[int]\n    nLL : np.ndarray\n        Negative log-likelihood history (finite prefix).\n    mu_ : np.ndarray  shape (K,)\n    gamma_ : np.ndarray  shape (K,)\n    rho_ : np.ndarray  shape (K,)\n    lam_ : np.ndarray  shape (K,)\n    p_ : np.ndarray  shape (K,)\n        Mixture weights.\n    gamma_resp_ : np.ndarray  shape (K, n)\n        Responsibilities.\n    labels_ : np.ndarray  shape (n,)\n        MAP component labels.\n    data : np.ndarray\n        Original X as provided.\n    alpha : np.ndarray\n        Data in radians.\n    n : int\n    params_ : list of dict or None\n        Per-component parameter dictionaries ({\"mu\", \"gamma\", \"rho\", \"lam\"}) after fit.\n    \"\"\"\n\n    def __init__(\n        self,\n        burnin: int = 30,\n        n_clusters: int = 5,\n        n_iters: int = 100,\n        full_cycle: Union[int, float] = 360,\n        unit: str = \"degree\",\n        random_seed: Optional[int] = 2046,\n        threshold: float = 1e-16,\n        mle_maxiter: int = 500,\n        mle_ftol: float = 1e-9,\n        min_comp_weight: float = 1e-6,\n    ):\n        if burnin &lt; 0:\n            raise ValueError(\"`burnin` must be non-negative.\")\n        if n_clusters &lt;= 0:\n            raise ValueError(\"`n_clusters` must be a positive integer.\")\n        if n_iters &lt;= 0:\n            raise ValueError(\"`n_iters` must be a positive integer.\")\n        if threshold &lt;= 0:\n            raise ValueError(\"`threshold` must be positive.\")\n        if unit not in {\"degree\", \"radian\"}:\n            raise ValueError(\"`unit` must be either 'degree' or 'radian'.\")\n\n        self.burnin = burnin\n        self.threshold = threshold\n        self.n_clusters = n_clusters\n        self.n_iters = n_iters\n        self.full_cycle = full_cycle\n        self.unit = unit\n        self._rng = np.random.default_rng(random_seed)\n\n        self.mle_maxiter = int(mle_maxiter)\n        self.mle_ftol = float(mle_ftol)\n        self.min_comp_weight = float(min_comp_weight)\n        self._gamma_floor = 1e-4\n        self._gamma_margin = 5e-4\n        self._rho_margin = 5e-4\n        self._constraint_margin = 5e-4\n        self._s_shrink = 5e-3\n\n        self.converged = False\n        self.converged_iters: Optional[int] = None\n\n        self.mu_: Optional[np.ndarray] = None\n        self.gamma_: Optional[np.ndarray] = None\n        self.rho_: Optional[np.ndarray] = None\n        self.lam_: Optional[np.ndarray] = None\n        self.p_: Optional[np.ndarray] = None\n        self.gamma_resp_: Optional[np.ndarray] = None\n        self.labels_: Optional[np.ndarray] = None\n        self.nLL: Optional[np.ndarray] = None\n        self.data: Optional[np.ndarray] = None\n        self.alpha: Optional[np.ndarray] = None\n        self.n: Optional[int] = None\n        self.params_: Optional[List[Dict[str, float]]] = None\n\n    # ---------- initialization ----------\n\n    def _initialize(\n        self,\n        x_rad: np.ndarray,\n        n_clusters_init: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Random-assign points to K clusters (no empty clusters), then per-cluster\n        initialize KJ params via method-of-moments.\"\"\"\n        n = len(x_rad)\n        if n_clusters_init &gt; n:\n            raise ValueError(\"Number of clusters exceeds sample size during initialization.\")\n\n        labels = None\n        if \"CircKMeans\" in globals():\n            try:\n                seed = int(self._rng.integers(0, 2**32 - 1))\n                kmeans = CircKMeans(\n                    n_clusters=n_clusters_init,\n                    unit=\"radian\",\n                    metric=\"center\",\n                    random_seed=seed,\n                )\n                kmeans.fit(x_rad)\n                labels = kmeans.labels_.astype(int, copy=True)\n                if len({int(c) for c in labels}) &lt; n_clusters_init:\n                    labels = None\n            except Exception:\n                labels = None\n\n        if labels is None:\n            for _ in range(100):\n                candidate = self._rng.integers(n_clusters_init, size=n)\n                if all(np.any(candidate == c) for c in range(n_clusters_init)):\n                    labels = candidate\n                    break\n            else:\n                raise RuntimeError(\"Failed to initialize clusters without empty components.\")\n\n        mu0 = np.zeros(n_clusters_init, float)\n        gamma0 = np.zeros(n_clusters_init, float)\n        rho0 = np.zeros(n_clusters_init, float)\n        lam0 = np.zeros(n_clusters_init, float)\n\n        for c in range(n_clusters_init):\n            subset = x_rad[labels == c]\n            # Moments init (fast, robust). Your katojones.fit already wraps moments logic.\n            est = katojones.fit(subset, method=\"moments\", return_info=False)\n            mu0[c], gamma0[c], rho0[c], lam0[c] = self._regularise_params(est)\n\n        p0 = np.full(n_clusters_init, 1.0 / n_clusters_init, dtype=float)\n        return mu0, gamma0, rho0, lam0, p0\n\n    # ---------- regularisation helpers ----------\n\n    def _constraint_value(self, gamma: float, rho: float, lam: float) -&gt; float:\n        cos_lam = np.cos(lam)\n        sin_lam = np.sin(lam)\n        return (rho * cos_lam - gamma) ** 2 + (rho * sin_lam) ** 2\n\n    def _regularise_params(self, params: Tuple[float, float, float, float]) -&gt; Tuple[float, float, float, float]:\n        mu, gamma, rho, lam = params\n        mu = float(np.mod(mu, 2.0 * np.pi))\n        gamma = float(np.clip(gamma, self._gamma_floor, 1.0 - self._gamma_margin))\n        rho = float(np.clip(rho, 0.0, 1.0 - self._rho_margin))\n        lam = float(np.mod(lam, 2.0 * np.pi))\n\n        limit = (1.0 - gamma) ** 2\n        if limit &lt;= 0.0:\n            gamma = 1.0 - self._gamma_margin\n            limit = (1.0 - gamma) ** 2\n\n        if self._constraint_value(gamma, rho, lam) &gt;= limit - self._constraint_margin:\n            # steer back inside feasible disk\n            s, phi = katojones._aux_from_rho_lam(gamma, rho, lam)\n            s = float(np.clip(s, 0.0, 1.0 - self._s_shrink))\n            s *= (1.0 - self._s_shrink)\n            rho, lam = katojones._rho_lam_from_aux(gamma, s, phi)\n            rho = float(np.clip(rho, 0.0, 1.0 - self._rho_margin))\n            lam = float(np.mod(lam, 2.0 * np.pi))\n\n        return mu, gamma, rho, lam\n\n    def _violates_or_degenerate(self, params: Tuple[float, float, float, float]) -&gt; bool:\n        mu, gamma, rho, lam = params\n        if not np.all(np.isfinite([mu, gamma, rho, lam])):\n            return True\n        if gamma &lt;= self._gamma_floor or rho &gt;= 1.0 - self._rho_margin:\n            return True\n        limit = (1.0 - gamma) ** 2\n        if limit &lt;= 0.0:\n            return True\n        return self._constraint_value(gamma, rho, lam) &gt;= limit - self._constraint_margin / 2.0\n\n    # ---------- core likelihood pieces ----------\n\n    def _component_logpdf(\n        self,\n        alpha: np.ndarray,\n        mu: np.ndarray,\n        gamma: np.ndarray,\n        rho: np.ndarray,\n        lam: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Return array shape (K, n) of component log-densities.\"\"\"\n        K = mu.size\n        logs = np.vstack(\n            [\n                katojones.logpdf(alpha, mu=mu[k], gamma=gamma[k], rho=rho[k], lam=lam[k])\n                for k in range(K)\n            ]\n        )\n        return logs\n\n    def _log_gamma(self, alpha, p, mu, gamma, rho, lam) -&gt; np.ndarray:\n        \"\"\"Unnormalized log-responsibilities, shape (K, n).\"\"\"\n        log_mix = np.log(np.clip(p, 1e-300, None))[:, None]\n        log_comp = self._component_logpdf(alpha, mu, gamma, rho, lam)\n        return log_mix + log_comp\n\n    def _nll(self, alpha, p, mu, gamma, rho, lam) -&gt; float:\n        log_gamma = self._log_gamma(alpha, p, mu, gamma, rho, lam)\n        ll = np.sum(logsumexp(log_gamma, axis=0))\n        return float(-ll)\n\n    # ---------- public API ----------\n\n    def fit(self, X: np.ndarray, verbose: Union[bool, int] = 0):\n        \"\"\"\n        Fit the MoKJ model by EM.\n\n        Parameters\n        ----------\n        X : array-like, shape (n,)\n            Circular data in degrees or radians (see `unit`).\n        verbose : bool or int\n            If True, print progress each iteration; if int &gt; 0, print every `verbose` iters.\n        \"\"\"\n        X = np.asarray(X, dtype=float).reshape(-1)\n        if X.size == 0:\n            raise ValueError(\"Input data must contain at least one observation.\")\n        alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n\n        self.data = X\n        self.alpha = alpha\n        self.n = n = alpha.size\n\n        mu, gamma, rho, lam, p = self._initialize(alpha, self.n_clusters)\n\n        if verbose:\n            print(\"Iter\".ljust(10) + \"nLL\")\n\n        nLL_hist = np.full(self.n_iters, np.nan)\n        last_nll = np.inf\n\n        for it in range(self.n_iters):\n            # E-step\n            log_resp = self._log_gamma(alpha, p, mu, gamma, rho, lam)\n            log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n            resp = np.exp(log_resp - log_norm)  # (K, n)\n\n            # M-step: weights\n            p = resp.sum(axis=1)\n            p = np.clip(p, self.min_comp_weight, None)\n            p /= p.sum()\n\n            # M-step: per-component params via weighted MLE, with fallback to moments\n            mu_new = np.empty_like(mu)\n            gamma_new = np.empty_like(gamma)\n            rho_new = np.empty_like(rho)\n            lam_new = np.empty_like(lam)\n\n            for k in range(self.n_clusters):\n                w = resp[k]\n                wsum = float(w.sum())\n\n                moment_est = self._regularise_params(\n                    katojones.fit(alpha, method=\"moments\", weights=w, return_info=False)\n                )\n\n                if not np.isfinite(wsum) or wsum &lt;= self.min_comp_weight * n:\n                    # too small / degenerate: keep previous or reinit via moments\n                    mu_new[k], gamma_new[k], rho_new[k], lam_new[k] = moment_est\n                    continue\n\n                # Start from current params; do weighted MLE as in the EM literature\n                mle_params = None\n                initial_params = self._regularise_params((mu[k], gamma[k], rho[k], lam[k]))\n                for start_params in (initial_params, moment_est):\n                    try:\n                        est, _info = katojones.fit(\n                            alpha,\n                            method=\"mle\",\n                            weights=w,\n                            initial=start_params,\n                            optimizer=\"L-BFGS-B\",\n                            options={\"maxiter\": self.mle_maxiter, \"ftol\": self.mle_ftol},\n                            return_info=True,\n                        )\n                        est = self._regularise_params(est)\n                        if not self._violates_or_degenerate(est):\n                            mle_params = est\n                            break\n                    except Exception:\n                        continue\n\n                if mle_params is None:\n                    mle_params = moment_est\n\n                mu_new[k], gamma_new[k], rho_new[k], lam_new[k] = mle_params\n\n            mu, gamma, rho, lam = mu_new, gamma_new, rho_new, lam_new\n\n            # bookkeeping\n            nLL = self._nll(alpha, p, mu, gamma, rho, lam)\n            nLL_hist[it] = nLL\n            if verbose and (it % int(verbose or 1) == 0):\n                print(f\"{it}\".ljust(10) + f\"{nLL:.6f}\")\n\n            # convergence check\n            if it &gt; self.burnin and abs(last_nll - nLL) &lt; self.threshold:\n                self.converged = True\n                self.converged_iters = it + 1\n                if verbose:\n                    print(f\"Converged at iter {it}. Final nLL = {nLL:.6f}\\n\")\n                break\n            last_nll = nLL\n        else:\n            if verbose:\n                print(f\"Reached max iter {self.n_iters}. Final nLL = {nLL:.6f}\\n\")\n\n        # Save final state\n        self.nLL = nLL_hist[~np.isnan(nLL_hist)]\n        self.mu_, self.gamma_, self.rho_, self.lam_ = mu, gamma, rho, lam\n        self.p_ = p\n        self.params_ = [\n            {\n                \"mu\": float(mu[i]),\n                \"gamma\": float(gamma[i]),\n                \"rho\": float(rho[i]),\n                \"lam\": float(lam[i]),\n            }\n            for i in range(self.n_clusters)\n        ]\n        # final responsibilities &amp; labels\n        log_resp = self._log_gamma(alpha, p, mu, gamma, rho, lam)\n        log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n        self.gamma_resp_ = np.exp(log_resp - log_norm)\n        self.labels_ = self.gamma_resp_.argmax(axis=0)\n        return self\n\n    # ---------- utilities ----------\n\n    def compute_BIC(self) -&gt; float:\n        \"\"\"\n        Bayesian Information Criterion for the original KJ mixture.\n        Uses p = 4*K + (K-1) = 5K - 1 parameters.\n        \"\"\"\n        if self.gamma_resp_ is None:\n            raise ValueError(\"Model must be fitted before computing BIC.\")\n        nLL = self._nll(self.alpha, self.p_, self.mu_, self.gamma_, self.rho_, self.lam_)\n        nparams = 5 * self.n_clusters - 1\n        return 2 * nLL + np.log(self.n) * nparams\n\n    def predict_proba(\n        self,\n        x: np.ndarray,\n        unit: Union[str, None] = None,\n        full_cycle: Union[float, int, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Posterior component probabilities for new points.\n        \"\"\"\n        if self.p_ is None:\n            raise ValueError(\"Model must be fitted before calling predict_proba().\")\n        unit = self.unit if unit is None else unit\n        full_cycle = self.full_cycle if full_cycle is None else full_cycle\n        x = np.asarray(x, dtype=float).reshape(-1)\n        alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n        log_resp = self._log_gamma(alpha, self.p_, self.mu_, self.gamma_, self.rho_, self.lam_)\n        log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n        return np.exp(log_resp - log_norm)\n\n    def predict(\n        self,\n        x: np.ndarray,\n        unit: Union[str, None] = None,\n        full_cycle: Union[float, int, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"MAP assignments for new data.\"\"\"\n        return self.predict_proba(x, unit=unit, full_cycle=full_cycle).argmax(axis=0)\n\n    def predict_density(\n        self,\n        x: Optional[np.ndarray] = None,\n        unit: Union[str, None] = None,\n        full_cycle: Union[float, int, None] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Mixture density at points x.\n        \"\"\"\n        if self.p_ is None:\n            raise ValueError(\"Model must be fitted before calling predict_density().\")\n        unit = self.unit if unit is None else unit\n        full_cycle = self.full_cycle if full_cycle is None else full_cycle\n\n        if x is None:\n            x = np.linspace(0, 2 * np.pi, 400, endpoint=False)\n            if unit == \"degree\":\n                x = np.rad2deg(x)\n        x = np.asarray(x, dtype=float).reshape(-1)\n        alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n\n        dens = np.zeros_like(alpha, dtype=float)\n        for pc, muc, gc, rhoc, lamc in zip(self.p_, self.mu_, self.gamma_, self.rho_, self.lam_):\n            dens += pc * katojones.pdf(alpha, mu=muc, gamma=gc, rho=rhoc, lam=lamc)\n        return dens\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MoKJ.fit","title":"<code>fit(X, verbose=0)</code>","text":"<p>Fit the MoKJ model by EM.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n))</code> <p>Circular data in degrees or radians (see <code>unit</code>).</p> required <code>verbose</code> <code>bool or int</code> <p>If True, print progress each iteration; if int &gt; 0, print every <code>verbose</code> iters.</p> <code>0</code> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def fit(self, X: np.ndarray, verbose: Union[bool, int] = 0):\n    \"\"\"\n    Fit the MoKJ model by EM.\n\n    Parameters\n    ----------\n    X : array-like, shape (n,)\n        Circular data in degrees or radians (see `unit`).\n    verbose : bool or int\n        If True, print progress each iteration; if int &gt; 0, print every `verbose` iters.\n    \"\"\"\n    X = np.asarray(X, dtype=float).reshape(-1)\n    if X.size == 0:\n        raise ValueError(\"Input data must contain at least one observation.\")\n    alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n\n    self.data = X\n    self.alpha = alpha\n    self.n = n = alpha.size\n\n    mu, gamma, rho, lam, p = self._initialize(alpha, self.n_clusters)\n\n    if verbose:\n        print(\"Iter\".ljust(10) + \"nLL\")\n\n    nLL_hist = np.full(self.n_iters, np.nan)\n    last_nll = np.inf\n\n    for it in range(self.n_iters):\n        # E-step\n        log_resp = self._log_gamma(alpha, p, mu, gamma, rho, lam)\n        log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n        resp = np.exp(log_resp - log_norm)  # (K, n)\n\n        # M-step: weights\n        p = resp.sum(axis=1)\n        p = np.clip(p, self.min_comp_weight, None)\n        p /= p.sum()\n\n        # M-step: per-component params via weighted MLE, with fallback to moments\n        mu_new = np.empty_like(mu)\n        gamma_new = np.empty_like(gamma)\n        rho_new = np.empty_like(rho)\n        lam_new = np.empty_like(lam)\n\n        for k in range(self.n_clusters):\n            w = resp[k]\n            wsum = float(w.sum())\n\n            moment_est = self._regularise_params(\n                katojones.fit(alpha, method=\"moments\", weights=w, return_info=False)\n            )\n\n            if not np.isfinite(wsum) or wsum &lt;= self.min_comp_weight * n:\n                # too small / degenerate: keep previous or reinit via moments\n                mu_new[k], gamma_new[k], rho_new[k], lam_new[k] = moment_est\n                continue\n\n            # Start from current params; do weighted MLE as in the EM literature\n            mle_params = None\n            initial_params = self._regularise_params((mu[k], gamma[k], rho[k], lam[k]))\n            for start_params in (initial_params, moment_est):\n                try:\n                    est, _info = katojones.fit(\n                        alpha,\n                        method=\"mle\",\n                        weights=w,\n                        initial=start_params,\n                        optimizer=\"L-BFGS-B\",\n                        options={\"maxiter\": self.mle_maxiter, \"ftol\": self.mle_ftol},\n                        return_info=True,\n                    )\n                    est = self._regularise_params(est)\n                    if not self._violates_or_degenerate(est):\n                        mle_params = est\n                        break\n                except Exception:\n                    continue\n\n            if mle_params is None:\n                mle_params = moment_est\n\n            mu_new[k], gamma_new[k], rho_new[k], lam_new[k] = mle_params\n\n        mu, gamma, rho, lam = mu_new, gamma_new, rho_new, lam_new\n\n        # bookkeeping\n        nLL = self._nll(alpha, p, mu, gamma, rho, lam)\n        nLL_hist[it] = nLL\n        if verbose and (it % int(verbose or 1) == 0):\n            print(f\"{it}\".ljust(10) + f\"{nLL:.6f}\")\n\n        # convergence check\n        if it &gt; self.burnin and abs(last_nll - nLL) &lt; self.threshold:\n            self.converged = True\n            self.converged_iters = it + 1\n            if verbose:\n                print(f\"Converged at iter {it}. Final nLL = {nLL:.6f}\\n\")\n            break\n        last_nll = nLL\n    else:\n        if verbose:\n            print(f\"Reached max iter {self.n_iters}. Final nLL = {nLL:.6f}\\n\")\n\n    # Save final state\n    self.nLL = nLL_hist[~np.isnan(nLL_hist)]\n    self.mu_, self.gamma_, self.rho_, self.lam_ = mu, gamma, rho, lam\n    self.p_ = p\n    self.params_ = [\n        {\n            \"mu\": float(mu[i]),\n            \"gamma\": float(gamma[i]),\n            \"rho\": float(rho[i]),\n            \"lam\": float(lam[i]),\n        }\n        for i in range(self.n_clusters)\n    ]\n    # final responsibilities &amp; labels\n    log_resp = self._log_gamma(alpha, p, mu, gamma, rho, lam)\n    log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n    self.gamma_resp_ = np.exp(log_resp - log_norm)\n    self.labels_ = self.gamma_resp_.argmax(axis=0)\n    return self\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MoKJ.compute_BIC","title":"<code>compute_BIC()</code>","text":"<p>Bayesian Information Criterion for the original KJ mixture. Uses p = 4*K + (K-1) = 5K - 1 parameters.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def compute_BIC(self) -&gt; float:\n    \"\"\"\n    Bayesian Information Criterion for the original KJ mixture.\n    Uses p = 4*K + (K-1) = 5K - 1 parameters.\n    \"\"\"\n    if self.gamma_resp_ is None:\n        raise ValueError(\"Model must be fitted before computing BIC.\")\n    nLL = self._nll(self.alpha, self.p_, self.mu_, self.gamma_, self.rho_, self.lam_)\n    nparams = 5 * self.n_clusters - 1\n    return 2 * nLL + np.log(self.n) * nparams\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MoKJ.predict_proba","title":"<code>predict_proba(x, unit=None, full_cycle=None)</code>","text":"<p>Posterior component probabilities for new points.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict_proba(\n    self,\n    x: np.ndarray,\n    unit: Union[str, None] = None,\n    full_cycle: Union[float, int, None] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Posterior component probabilities for new points.\n    \"\"\"\n    if self.p_ is None:\n        raise ValueError(\"Model must be fitted before calling predict_proba().\")\n    unit = self.unit if unit is None else unit\n    full_cycle = self.full_cycle if full_cycle is None else full_cycle\n    x = np.asarray(x, dtype=float).reshape(-1)\n    alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n    log_resp = self._log_gamma(alpha, self.p_, self.mu_, self.gamma_, self.rho_, self.lam_)\n    log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n    return np.exp(log_resp - log_norm)\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MoKJ.predict","title":"<code>predict(x, unit=None, full_cycle=None)</code>","text":"<p>MAP assignments for new data.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict(\n    self,\n    x: np.ndarray,\n    unit: Union[str, None] = None,\n    full_cycle: Union[float, int, None] = None,\n) -&gt; np.ndarray:\n    \"\"\"MAP assignments for new data.\"\"\"\n    return self.predict_proba(x, unit=unit, full_cycle=full_cycle).argmax(axis=0)\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MoKJ.predict_density","title":"<code>predict_density(x=None, unit=None, full_cycle=None)</code>","text":"<p>Mixture density at points x.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict_density(\n    self,\n    x: Optional[np.ndarray] = None,\n    unit: Union[str, None] = None,\n    full_cycle: Union[float, int, None] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Mixture density at points x.\n    \"\"\"\n    if self.p_ is None:\n        raise ValueError(\"Model must be fitted before calling predict_density().\")\n    unit = self.unit if unit is None else unit\n    full_cycle = self.full_cycle if full_cycle is None else full_cycle\n\n    if x is None:\n        x = np.linspace(0, 2 * np.pi, 400, endpoint=False)\n        if unit == \"degree\":\n            x = np.rad2deg(x)\n    x = np.asarray(x, dtype=float).reshape(-1)\n    alpha = x if unit == \"radian\" else data2rad(x, k=full_cycle)\n\n    dens = np.zeros_like(alpha, dtype=float)\n    for pc, muc, gc, rhoc, lamc in zip(self.p_, self.mu_, self.gamma_, self.rho_, self.lam_):\n        dens += pc * katojones.pdf(alpha, mu=muc, gamma=gc, rho=rhoc, lam=lamc)\n    return dens\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.MoCD","title":"<code>MoCD</code>","text":"<p>Mixture of Circular Distributions (MoCD).</p> <p>This class generalises <code>MovM</code> to any circular distribution that exposes <code>logpdf</code> and <code>fit</code> methods accepting weighted observations.  All mixture components share the same distribution family (e.g. von Mises, wrapped Cauchy, wrapped normal, inverse Batschelet).  Users choose the underlying family and the EM algorithm re-estimates the component parameters and mixing weights.</p> Notes <ul> <li>The current implementation assumes each component uses the same   distribution.  Extending EM to support heterogeneous components   (different families per cluster) is feasible \u2013 responsibilities are still   well-defined \u2013 but requires bookkeeping for a potentially different set of   parameters and optimisation routines per component.  That design is left   for future work.</li> <li>The supplied distribution must expose <code>logpdf</code> and a <code>fit</code> method with   a <code>weights</code> keyword argument.  Most distributions in <code>pycircstat2</code>   follow that convention.</li> <li>Parameter order is inferred from <code>distribution.shapes</code> where available;   otherwise <code>param_names</code> must be provided.</li> <li>The current implementation restricts distributions to cardioid, Cartwright,   wrapped normal (<code>wrapnorm</code>), wrapped Cauchy (<code>wrapcauchy</code>), or von Mises   while other families are under investigation.</li> </ul> Source code in <code>pycircstat2/clustering.py</code> <pre><code>class MoCD:\n    \"\"\"\n    Mixture of Circular Distributions (MoCD).\n\n    This class generalises `MovM` to any circular distribution that exposes\n    ``logpdf`` and ``fit`` methods accepting weighted observations.  All mixture\n    components share the same distribution family (e.g. von Mises, wrapped Cauchy,\n    wrapped normal, inverse Batschelet).  Users choose the underlying family and\n    the EM algorithm re-estimates the component parameters and mixing weights.\n\n    Notes\n    -----\n    * The current implementation assumes each component uses **the same**\n      distribution.  Extending EM to support heterogeneous components\n      (different families per cluster) is feasible \u2013 responsibilities are still\n      well-defined \u2013 but requires bookkeeping for a potentially different set of\n      parameters and optimisation routines per component.  That design is left\n      for future work.\n    * The supplied distribution must expose ``logpdf`` and a ``fit`` method with\n      a ``weights`` keyword argument.  Most distributions in `pycircstat2`\n      follow that convention.\n    * Parameter order is inferred from ``distribution.shapes`` where available;\n      otherwise ``param_names`` must be provided.\n    * The current implementation restricts distributions to cardioid, Cartwright,\n      wrapped normal (``wrapnorm``), wrapped Cauchy (``wrapcauchy``), or von Mises\n      while other families are under investigation.\n    \"\"\"\n\n    def __init__(\n        self,\n        distribution: CircularContinuous = vonmises,\n        *,\n        param_names: Optional[List[str]] = None,\n        fit_method: Optional[Union[str, List[str], Tuple[str, ...]]] = \"auto\",\n        fit_kwargs: Optional[Dict[str, object]] = None,\n        n_clusters: int = 3,\n        n_iters: int = 100,\n        burnin: int = 20,\n        threshold: float = 1e-6,\n        unit: str = \"degree\",\n        full_cycle: Union[int, float] = 360,\n        random_seed: Optional[int] = None,\n    ) -&gt; None:\n        if not isinstance(distribution, CircularContinuous):\n            raise TypeError(\"`distribution` must be an instance of CircularContinuous (e.g. vonmises).\")\n        if n_clusters &lt;= 0:\n            raise ValueError(\"`n_clusters` must be positive.\")\n        if n_iters &lt;= 0:\n            raise ValueError(\"`n_iters` must be positive.\")\n        if burnin &lt; 0:\n            raise ValueError(\"`burnin` must be non-negative.\")\n        if threshold &lt;= 0:\n            raise ValueError(\"`threshold` must be positive.\")\n        if unit not in {\"degree\", \"radian\"}:\n            raise ValueError(\"`unit` must be either 'degree' or 'radian'.\")\n\n        self.distribution = distribution\n        distribution_name = getattr(self.distribution, \"name\", None)\n        if not distribution_name:\n            distribution_name = self.distribution.__class__.__name__\n        distribution_name_key = distribution_name.lower()\n        if distribution_name_key not in ALLOWED_MOCD_DISTRIBUTIONS:\n            allowed = \", \".join(sorted(ALLOWED_MOCD_DISTRIBUTIONS))\n            raise ValueError(\n                f\"`distribution` '{distribution_name}' is not currently supported by MoCD. \"\n                f\"Allowed options: {allowed}.\"\n            )\n\n        self.n_clusters = int(n_clusters)\n        self.n_iters = int(n_iters)\n        self.burnin = int(burnin)\n        self.threshold = float(threshold)\n        self.unit = unit\n        self.full_cycle = full_cycle\n        self.fit_kwargs = {} if fit_kwargs is None else dict(fit_kwargs)\n        self._rng = np.random.default_rng(random_seed)\n\n        fit_signature = inspect.signature(self.distribution.fit)\n        if \"weights\" not in fit_signature.parameters:\n            raise ValueError(\n                \"The selected distribution does not expose a `weights=` keyword in its fit method. \"\n                \"MoCD requires weighted fitting to perform the EM M-step.\"\n            )\n\n        inferred_names: List[str] = []\n        if param_names is not None:\n            inferred_names = list(param_names)\n        else:\n            shapes = getattr(self.distribution, \"shapes\", None)\n            if shapes:\n                inferred_names = [name.strip() for name in shapes.split(\",\") if name.strip()]\n\n        if not inferred_names:\n            raise ValueError(\n                \"`param_names` could not be inferred. Please provide the parameter order explicitly.\"\n            )\n\n        self.param_names = inferred_names\n        if \"method\" in self.fit_kwargs:\n            method_value = self.fit_kwargs.pop(\"method\")\n            self._method_candidates = [str(method_value).lower()]\n        else:\n            self._method_candidates = self._normalise_fit_method(fit_method)\n\n        distribution_name = distribution_name_key\n        if (\n            (fit_method is None or (isinstance(fit_method, str) and fit_method.lower() == \"auto\"))\n            and distribution_name in {\"vonmises_flattopped\", \"inverse_batschelet\"}\n        ):\n            self._method_candidates = [\"mle\"]\n\n        # Model attributes populated after fitting\n        self.converged: bool = False\n        self.converged_iters: Optional[int] = None\n        self.nLL: Optional[np.ndarray] = None\n        self.p_: Optional[np.ndarray] = None\n        self.params_: Optional[List[Dict[str, float]]] = None\n        self.param_matrix_: Optional[np.ndarray] = None\n        self.gamma_: Optional[np.ndarray] = None\n        self.labels_: Optional[np.ndarray] = None\n        self.alpha: Optional[np.ndarray] = None\n        self.data: Optional[np.ndarray] = None\n        self.n: Optional[int] = None\n\n    def _normalise_fit_method(\n        self, fit_method: Optional[Union[str, List[str], Tuple[str, ...]]]\n    ) -&gt; List[Optional[str]]:\n        if fit_method is None:\n            return [None]\n\n        if isinstance(fit_method, (list, tuple)):\n            if not fit_method:\n                return [None]\n            return [None if m is None else str(m).lower() for m in fit_method]\n\n        method_str = str(fit_method).lower()\n        if method_str == \"auto\":\n            return [\"moments\", \"mle\"]\n        return [method_str]\n\n    # ------------------------------------------------------------------ #\n    # Helper utilities\n    # ------------------------------------------------------------------ #\n    def _params_to_array(self, params: Dict[str, float]) -&gt; np.ndarray:\n        return np.array([float(params[name]) for name in self.param_names], dtype=float)\n\n    def _array_to_params(self, values: Union[Dict[str, float], Tuple[float, ...], List[float]]) -&gt; Dict[str, float]:\n        if isinstance(values, dict):\n            return {name: float(values[name]) for name in self.param_names}\n        arr = np.atleast_1d(values).astype(float)\n        if arr.size != len(self.param_names):\n            raise ValueError(\n                f\"Expected {len(self.param_names)} parameters, but got {arr.size}. \"\n                \"Please supply `param_names` matching the distribution.\"\n            )\n        return {name: float(arr[i]) for i, name in enumerate(self.param_names)}\n\n    def _fit_component(\n        self,\n        alpha: np.ndarray,\n        weights: np.ndarray,\n        current_params: Optional[Dict[str, float]] = None,\n    ) -&gt; Dict[str, float]:\n        weights = np.asarray(weights, dtype=float)\n        total_weight = float(np.sum(weights))\n        if not np.isfinite(total_weight) or total_weight &lt;= 1e-12:\n            if current_params is not None:\n                return current_params\n            weights = np.ones_like(weights, dtype=float)\n            total_weight = float(np.sum(weights))\n\n        last_error: Optional[Exception] = None\n        for method in self._method_candidates:\n            fit_options = dict(self.fit_kwargs)\n            if method is not None:\n                fit_options.setdefault(\"method\", method)\n            fit_options[\"weights\"] = weights\n\n            try:\n                params_est, _info = self.distribution.fit(alpha, return_info=True, **fit_options)\n            except TypeError:\n                fit_options.pop(\"return_info\", None)\n                try:\n                    params_est = self.distribution.fit(alpha, **fit_options)\n                except Exception as exc:  # pragma: no cover\n                    last_error = exc\n                    continue\n            except Exception as exc:\n                last_error = exc\n                continue\n\n            try:\n                return self._array_to_params(params_est)\n            except Exception as exc:  # pragma: no cover - defensive\n                last_error = exc\n                continue\n\n        raise RuntimeError(\n            \"Failed to fit mixture component; attempted methods \"\n            f\"{self._method_candidates} with last error: {last_error}\"\n        )\n\n    def _initialize(self, alpha: np.ndarray) -&gt; Tuple[List[Dict[str, float]], np.ndarray]:\n        n = alpha.size\n        if self.n_clusters &gt; n:\n            raise ValueError(\"Number of clusters cannot exceed number of observations during initialisation.\")\n\n        for _ in range(128):\n            labels = self._rng.integers(self.n_clusters, size=n)\n            if all(np.any(labels == c) for c in range(self.n_clusters)):\n                break\n        else:\n            raise RuntimeError(\"Failed to initialise mixture components without empty clusters.\")\n\n        params_list: List[Dict[str, float]] = []\n        for c in range(self.n_clusters):\n            mask = labels == c\n            count = int(mask.sum())\n            params = self._fit_component(alpha[mask], np.ones(count, dtype=float))\n            params_list.append(params)\n\n        p = np.full(self.n_clusters, 1.0 / self.n_clusters, dtype=float)\n        return params_list, p\n\n    def _log_gamma(\n        self,\n        alpha: np.ndarray,\n        p: np.ndarray,\n        params_list: List[Dict[str, float]],\n    ) -&gt; np.ndarray:\n        log_prob = np.vstack(\n            [\n                np.log(p[i] + 1e-32) + self.distribution.logpdf(alpha, **params_list[i])\n                for i in range(self.n_clusters)\n            ]\n        )\n        return log_prob\n\n    # ------------------------------------------------------------------ #\n    # Public API\n    # ------------------------------------------------------------------ #\n    def fit(self, X: np.ndarray, verbose: Union[bool, int] = 0) -&gt; \"MoCD\":\n        X = np.asarray(X, dtype=float).reshape(-1)\n        if X.size == 0:\n            raise ValueError(\"Input data must contain at least one observation.\")\n\n        alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n\n        self.data = X\n        self.alpha = alpha\n        self.n = alpha.size\n\n        params_list, p = self._initialize(alpha)\n\n        if verbose:\n            header = \"Iter\".ljust(10) + \"nLL\"\n            print(header)\n\n        nLL_history = np.full(self.n_iters, np.nan)\n\n        for iteration in range(self.n_iters):\n            log_resp = self._log_gamma(alpha, p, params_list)\n            log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n            gamma_normed = np.exp(log_resp - log_norm)\n\n            p = gamma_normed.sum(axis=1)\n            p /= p.sum()\n\n            params_updated: List[Dict[str, float]] = []\n            for c in range(self.n_clusters):\n                weights = gamma_normed[c]\n                if np.allclose(weights.sum(), 0.0):\n                    params_updated.append(params_list[c])\n                    continue\n                params_updated.append(self._fit_component(alpha, weights, current_params=params_list[c]))\n            params_list = params_updated\n\n            nLL = -float(np.sum(log_norm))\n            nLL_history[iteration] = nLL\n\n            if verbose and (iteration % int(verbose or 1) == 0):\n                print(f\"{iteration}\".ljust(10) + f\"{nLL:.3f}\")\n\n            if (\n                iteration &gt; self.burnin\n                and np.abs(nLL_history[iteration] - nLL_history[iteration - 1])\n                &lt; self.threshold\n            ):\n                self.converged = True\n                self.converged_iters = iteration + 1\n                if verbose:\n                    print(f\"Converged at iter {iteration}. Final nLL = {nLL:.3f}\\n\")\n                break\n        else:\n            if verbose:\n                print(f\"Reached max iter {self.n_iters}. Final nLL = {nLL_history[self.n_iters - 1]:.3f}\\n\")\n\n        self.nLL = nLL_history[~np.isnan(nLL_history)]\n        self.p_ = p\n        self.params_ = params_list\n        self.param_matrix_ = np.vstack([self._params_to_array(params) for params in params_list])\n\n        final_log = self._log_gamma(alpha, p, params_list)\n        final_norm = logsumexp(final_log, axis=0, keepdims=True)\n        gamma_final = np.exp(final_log - final_norm)\n        self.gamma_ = gamma_final\n        self.labels_ = gamma_final.argmax(axis=0)\n        return self\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.gamma_ is None or self.p_ is None or self.params_ is None:\n            raise ValueError(\"Model must be fitted before calling `predict_proba`.\")\n\n        X = np.asarray(X, dtype=float).reshape(-1)\n        alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n\n        log_resp = self._log_gamma(alpha, self.p_, self.params_)\n        log_norm = logsumexp(log_resp, axis=0, keepdims=True)\n        return np.exp(log_resp - log_norm)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        proba = self.predict_proba(X)\n        return proba.argmax(axis=0)\n\n    def score_samples(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.p_ is None or self.params_ is None:\n            raise ValueError(\"Model must be fitted before calling `score_samples`.\")\n\n        X = np.asarray(X, dtype=float).reshape(-1)\n        alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n        log_resp = self._log_gamma(alpha, self.p_, self.params_)\n        return logsumexp(log_resp, axis=0)\n\n    def score(self, X: np.ndarray) -&gt; float:\n        log_likelihood = self.score_samples(X)\n        return float(np.mean(log_likelihood))\n\n    def predict_density(\n        self,\n        X: Optional[np.ndarray] = None,\n        *,\n        unit: Optional[str] = None,\n        full_cycle: Optional[Union[int, float]] = None,\n    ) -&gt; np.ndarray:\n        if self.p_ is None or self.params_ is None:\n            raise ValueError(\"Model must be fitted before calling `predict_density`.\")\n\n        unit = self.unit if unit is None else unit\n        full_cycle = self.full_cycle if full_cycle is None else full_cycle\n\n        if X is None:\n            X = np.linspace(0.0, 2.0 * np.pi, 200, endpoint=False)\n            if unit == \"degree\":\n                X = np.rad2deg(X)\n\n        X = np.asarray(X, dtype=float).reshape(-1)\n        alpha = X if unit == \"radian\" else data2rad(X, k=full_cycle)\n\n        pdf_components = np.vstack(\n            [self.distribution.pdf(alpha, **params) for params in self.params_]\n        )\n        density = np.sum(self.p_[:, None] * pdf_components, axis=0)\n        return density\n\n    def bic(self) -&gt; float:\n        if self.alpha is None or self.p_ is None or self.params_ is None:\n            raise ValueError(\"Model must be fitted before computing BIC.\")\n        log_likelihood = self.score_samples(self.alpha)\n        nLL = -float(np.sum(log_likelihood))\n        n_params_component = len(self.param_names)\n        n_params_total = self.n_clusters * n_params_component + (self.n_clusters - 1)\n        return 2.0 * nLL + np.log(self.n) * n_params_total\n\n    # Aliases for compatibility with the MovM API\n    def predict_density_grid(self, X: Optional[np.ndarray] = None) -&gt; np.ndarray:\n        return self.predict_density(X)\n\n    def compute_BIC(self) -&gt; float:\n        return self.bic()\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircHAC","title":"<code>CircHAC</code>","text":"<p>Hierarchical agglomerative clustering for circular (1D) data, with optional dendrogram tracking.</p> <p>Each merge is recorded: (clusterA, clusterB, distance, new_cluster_size).</p> <p>This is a \"center-merge\" approach: each cluster is represented by its circular mean, and we merge the two clusters with the smallest absolute circular difference in means (using circ_dist). The merges form a dendrogram we can plot or output.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters desired.</p> <code>2</code> <code>n_init_clusters</code> <code>int or None</code> <p>If None, every point starts as its own cluster (default HAC). If a number, <code>CircKMeans</code> is used to pre-cluster data before HAC.</p> <code>None</code> <code>unit</code> <code>('radian', 'degree')</code> <p>If \"degree\", data is converted to radians internally.</p> <code>\"radian\"</code> <code>full_cycle</code> <code>int</code> <p>For data conversion if unit=\"degree\".</p> <code>360</code> <code>metric</code> <code>('center', 'geodesic', 'angularseparation', 'chord')</code> <p>The distance metric used to measure the difference between cluster centers. We'll take its absolute value so that it's a nonnegative distance.</p> <code>\"center\"</code> <code>random_seed</code> <code>int</code> <p>Not used by default, but if you add any random steps, you may set it here.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>centers_</code> <code>(ndarray, shape(k))</code> <p>Final cluster center angles (in radians).</p> <code>r_</code> <code>(ndarray, shape(k))</code> <p>Resultant vector length for each cluster.</p> <code>labels_</code> <code>(ndarray, shape(n_samples))</code> <p>Cluster assignment for each data point, in {0, ..., k-1}.</p> <code>merges_</code> <code>(ndarray, shape(m, 4))</code> <p>Dendrogram merge history: - merges_[step, 0] = ID of cluster A - merges_[step, 1] = ID of cluster B - merges_[step, 2] = distance used to merge - merges_[step, 3] = new cluster size after merge Note: these cluster IDs are the \"old\" ones, not necessarily 0..(k-1) at each step.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>class CircHAC:\n    \"\"\"\n    Hierarchical agglomerative clustering for circular (1D) data,\n    with optional dendrogram tracking.\n\n    Each merge is recorded: (clusterA, clusterB, distance, new_cluster_size).\n\n    This is a \"center-merge\" approach: each cluster is represented by its\n    circular mean, and we merge the two clusters with the smallest\n    *absolute* circular difference in means (using circ_dist).\n    The merges form a dendrogram we can plot or output.\n\n    Parameters\n    ----------\n    n_clusters : int, default=2\n        Number of clusters desired.\n    n_init_clusters : int or None, default=None\n        If None, every point starts as its own cluster (default HAC).\n        If a number, `CircKMeans` is used to pre-cluster data before HAC.\n    unit : {\"radian\", \"degree\"}, default=\"degree\"\n        If \"degree\", data is converted to radians internally.\n    full_cycle : int, default=360\n        For data conversion if unit=\"degree\".\n    metric : {\"center\", \"geodesic\", \"angularseparation\", \"chord\"}, default=\"center\"\n        The distance metric used to measure the difference between cluster centers.\n        We'll take its absolute value so that it's a nonnegative distance.\n    random_seed : int, optional\n        Not used by default, but if you add any random steps, you may set it here.\n\n    Attributes\n    ----------\n    centers_ : np.ndarray, shape (k,)\n        Final cluster center angles (in radians).\n    r_ : np.ndarray, shape (k,)\n        Resultant vector length for each cluster.\n    labels_ : np.ndarray, shape (n_samples,)\n        Cluster assignment for each data point, in {0, ..., k-1}.\n    merges_ : np.ndarray, shape (m, 4)\n        Dendrogram merge history:\n        - merges_[step, 0] = ID of cluster A\n        - merges_[step, 1] = ID of cluster B\n        - merges_[step, 2] = distance used to merge\n        - merges_[step, 3] = new cluster size after merge\n        Note: these cluster IDs are the \"old\" ones, not necessarily 0..(k-1) at each step.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_clusters: int = 2,\n        n_init_clusters: Optional[int] = None,\n        unit: str = \"degree\",\n        full_cycle: Union[int, float] = 360,\n        metric: str = \"center\",\n        random_seed: Optional[int] = None,\n    ):\n        if n_clusters &lt;= 0:\n            raise ValueError(\"`n_clusters` must be a positive integer.\")\n        if n_init_clusters is not None and n_init_clusters &lt;= 0:\n            raise ValueError(\"`n_init_clusters` must be positive when provided.\")\n        if unit not in {\"degree\", \"radian\"}:\n            raise ValueError(\"`unit` must be either 'degree' or 'radian'.\")\n        metric = metric.lower()\n        valid_metrics = {\"center\", \"geodesic\", \"angularseparation\", \"chord\"}\n        if metric not in valid_metrics:\n            raise ValueError(f\"`metric` must be one of {valid_metrics}.\")\n\n        self.n_clusters = n_clusters\n        self.n_init_clusters = n_init_clusters\n        self.unit = unit\n        self.full_cycle = full_cycle\n        self.metric = metric\n        self._rng = np.random.default_rng(random_seed)\n\n        self.centers_: Optional[np.ndarray] = None\n        self.r_: Optional[np.ndarray] = None\n        self.labels_: Optional[np.ndarray] = None\n        self.merges_: Optional[np.ndarray] = None\n        self.alpha: Optional[np.ndarray] = None\n        self.data: Optional[np.ndarray] = None\n\n    def _initialize_clusters(self, alpha: np.ndarray) -&gt; Dict[int, List[int]]:\n        n_samples = alpha.size\n        if (\n            self.n_init_clusters is None\n            or self.n_init_clusters &gt;= n_samples\n        ):\n            return {i: [i] for i in range(n_samples)}\n\n        # Pre-cluster using CircKMeans to obtain a manageable starting point\n        seed = int(self._rng.integers(0, 2**32 - 1))\n        kmeans = CircKMeans(\n            n_clusters=self.n_init_clusters,\n            unit=\"radian\",\n            metric=self.metric,\n            random_seed=seed,\n        )\n        kmeans.fit(alpha)\n\n        clusters: Dict[int, List[int]] = {}\n        for cid in range(self.n_init_clusters):\n            indices = np.where(kmeans.labels_ == cid)[0]\n            if indices.size:\n                clusters[cid] = indices.tolist()\n\n        if not clusters:\n            return {i: [i] for i in range(n_samples)}\n        return clusters\n\n    def fit(self, X):\n        \"\"\"\n        Perform agglomerative clustering on `X`.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            Input angles in degrees or radians.\n\n        Returns\n        -------\n        self : CircHAC\n        \"\"\"\n        self.data = X = np.asarray(X, dtype=float).reshape(-1)\n        if X.size == 0:\n            raise ValueError(\"Input data must contain at least one observation.\")\n\n        alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n        self.alpha = alpha\n\n        n = alpha.size\n        if n &lt;= self.n_clusters:\n            self.labels_ = np.arange(n, dtype=int)\n            self.centers_ = alpha.copy()\n            self.r_ = np.ones(n, dtype=float)\n            self.merges_ = np.empty((0, 4), dtype=float)\n            return self\n\n        clusters = self._initialize_clusters(alpha)\n        next_cluster_id = max(clusters.keys()) + 1 if clusters else 0\n        merges: List[List[float]] = []\n\n        while len(clusters) &gt; self.n_clusters:\n            means = {cid: circ_mean_and_r(alpha[indices])[0] for cid, indices in clusters.items()}\n            cluster_ids = list(clusters.keys())\n\n            best_dist = np.inf\n            best_pair: Optional[Tuple[int, int]] = None\n            for idx, cid_i in enumerate(cluster_ids):\n                for cid_j in cluster_ids[idx + 1 :]:\n                    dist_ij = circ_dist(means[cid_i], means[cid_j], metric=self.metric)\n                    if dist_ij &lt; best_dist:\n                        best_dist = dist_ij\n                        best_pair = (cid_i, cid_j)\n\n            if best_pair is None:\n                break\n\n            cid_i, cid_j = best_pair\n            merged_indices = clusters[cid_i] + clusters[cid_j]\n            merges.append([cid_i, cid_j, float(abs(best_dist)), float(len(merged_indices))])\n\n            del clusters[cid_i]\n            del clusters[cid_j]\n            clusters[next_cluster_id] = merged_indices\n            next_cluster_id += 1\n\n        final_ids = list(clusters.keys())\n        labels = np.empty(n, dtype=int)\n        centers = np.zeros(len(final_ids), dtype=float)\n        resultants = np.zeros(len(final_ids), dtype=float)\n        for new_label, cid in enumerate(final_ids):\n            indices = clusters[cid]\n            labels[indices] = new_label\n            mean_i, r_i = circ_mean_and_r(alpha[indices])\n            centers[new_label] = mean_i\n            resultants[new_label] = r_i\n\n        self.labels_ = labels\n        self.centers_ = centers\n        self.r_ = resultants\n        self.merges_ = np.array(merges, dtype=float) if merges else np.empty((0, 4), dtype=float)\n        return self\n\n    def predict(self, alpha):\n        \"\"\"\n        Assign new angles to the closest cluster center.\n\n        Parameters\n        ----------\n        alpha : array-like of shape (n_samples,)\n\n        Returns\n        -------\n        labels : np.ndarray of shape (n_samples,)\n        \"\"\"\n        if self.centers_ is None:\n            raise ValueError(\"Model must be fitted before calling predict().\")\n\n        alpha = np.asarray(alpha, dtype=float)\n        alpha = alpha if self.unit == \"radian\" else data2rad(alpha, k=self.full_cycle)\n\n        k = self.centers_.size\n        labels = np.zeros(alpha.size, dtype=int)\n        for i, angle in enumerate(alpha):\n            distances = [abs(circ_dist(angle, center, metric=self.metric)) for center in self.centers_]\n            labels[i] = int(np.argmin(distances))\n        return labels\n\n    def plot_dendrogram(self, ax=None, **kwargs):\n        \"\"\"\n        Plot a rudimentary dendrogram from merges_.\n\n        This is a basic approach that uses cluster IDs directly as \"labels\"\n        on the x-axis. Because cluster IDs might not be contiguous or in ascending\n        order, the result can look jumbled. A more sophisticated approach\n        would re-compute a consistent labeling for each step.\n\n        Parameters\n        ----------\n        ax : matplotlib Axes, optional\n            If None, create a new figure/axes.\n        **kwargs : dict\n            Passed along to ax.plot(), e.g. color, linewidth, etc.\n\n        Returns\n        -------\n        ax : matplotlib Axes\n        \"\"\"\n        import matplotlib.pyplot as plt\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(6, 4))\n        merges = self.merges_\n        if merges.size == 0:\n            ax.set_title(\"No merges recorded (maybe n &lt;= n_clusters?).\")\n            return ax\n\n        # merges_ is (step, 4): [clusterA, clusterB, dist, new_size]\n        # We want to plot something like a dendrogram:\n        #  - each row is a merge event\n        #  - x-axis might show cluster A and cluster B, y the 'distance'\n        # But cluster IDs might keep re-labelling, so a quick hack is we show them as is.\n\n        for step, (ca, cb, distval, new_size) in enumerate(merges):\n            ca = int(ca)\n            cb = int(cb)\n            # We'll draw a \"u\" connecting ca and cb at height distval\n            # Then the newly formed cluster could get ID=cb or something\n            # This is a naive approach that won't produce a fancy SciPy-like dendrogram\n            # but enough to illustrate what's happening.\n\n            x1, x2 = ca, cb\n            y = distval\n            # a line from (x1, 0) to (x1, y), from (x2, 0) to (x2, y),\n            # then a horizontal line across at y\n            # we can color them or style them with kwargs\n\n            ax.plot([x1, x1], [0, y], **kwargs)\n            ax.plot([x2, x2], [0, y], **kwargs)\n            ax.plot([x1, x2], [y, y], **kwargs)\n\n        ax.set_title(\"Rudimentary Dendrogram\")\n        ax.set_xlabel(\"Cluster ID (raw internal IDs)\")\n        ax.set_ylabel(\"Distance\")\n        return ax\n\n\n    def silhouette_score(self):\n        \"\"\"\n        Compute the average silhouette for a cluster assignment on circular data.\n\n        angles: np.ndarray shape (n,) in radians\n        labels: np.ndarray shape (n,) in {0,1,...,K-1}\n        metric: \"chord\", \"geodesic\", \"center\", etc.\n\n        Returns\n        -------\n        float\n            The mean silhouette over all points.\n        \"\"\"\n        angles = self.alpha\n        labels = self.labels_\n        metric = self.metric\n        n = len(angles)\n        if n &lt; 2:\n            return 0.0\n\n        silhouette_values = np.zeros(n, dtype=float)\n\n        # Precompute all pairwise distances\n        # shape =&gt; (n,n)\n        pairwise = circ_dist(angles[:,None], angles[None,:], metric=metric)\n        pairwise = np.abs(pairwise)  # ensure nonnegative\n\n        for i in range(n):\n            c_i = labels[i]\n            # points in cluster c_i\n            in_cluster_i = (labels == c_i)\n            # average distance to own cluster\n            # excluding the point itself\n            a_i = pairwise[i, in_cluster_i].mean() if in_cluster_i.sum() &gt; 1 else 0.0\n\n            # find min average distance to another cluster\n            b_i = np.inf\n            for c_other in np.unique(labels):\n                if c_other == c_i:\n                    continue\n                in_other = (labels == c_other)\n                dist_i_other = pairwise[i, in_other].mean()\n                if dist_i_other &lt; b_i:\n                    b_i = dist_i_other\n\n            silhouette_values[i] = (b_i - a_i) / max(a_i, b_i) if max(a_i, b_i) &gt; 0 else 0.0\n\n        return silhouette_values.mean()\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircHAC.fit","title":"<code>fit(X)</code>","text":"<p>Perform agglomerative clustering on <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input angles in degrees or radians.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>CircHAC</code> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def fit(self, X):\n    \"\"\"\n    Perform agglomerative clustering on `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input angles in degrees or radians.\n\n    Returns\n    -------\n    self : CircHAC\n    \"\"\"\n    self.data = X = np.asarray(X, dtype=float).reshape(-1)\n    if X.size == 0:\n        raise ValueError(\"Input data must contain at least one observation.\")\n\n    alpha = X if self.unit == \"radian\" else data2rad(X, k=self.full_cycle)\n    self.alpha = alpha\n\n    n = alpha.size\n    if n &lt;= self.n_clusters:\n        self.labels_ = np.arange(n, dtype=int)\n        self.centers_ = alpha.copy()\n        self.r_ = np.ones(n, dtype=float)\n        self.merges_ = np.empty((0, 4), dtype=float)\n        return self\n\n    clusters = self._initialize_clusters(alpha)\n    next_cluster_id = max(clusters.keys()) + 1 if clusters else 0\n    merges: List[List[float]] = []\n\n    while len(clusters) &gt; self.n_clusters:\n        means = {cid: circ_mean_and_r(alpha[indices])[0] for cid, indices in clusters.items()}\n        cluster_ids = list(clusters.keys())\n\n        best_dist = np.inf\n        best_pair: Optional[Tuple[int, int]] = None\n        for idx, cid_i in enumerate(cluster_ids):\n            for cid_j in cluster_ids[idx + 1 :]:\n                dist_ij = circ_dist(means[cid_i], means[cid_j], metric=self.metric)\n                if dist_ij &lt; best_dist:\n                    best_dist = dist_ij\n                    best_pair = (cid_i, cid_j)\n\n        if best_pair is None:\n            break\n\n        cid_i, cid_j = best_pair\n        merged_indices = clusters[cid_i] + clusters[cid_j]\n        merges.append([cid_i, cid_j, float(abs(best_dist)), float(len(merged_indices))])\n\n        del clusters[cid_i]\n        del clusters[cid_j]\n        clusters[next_cluster_id] = merged_indices\n        next_cluster_id += 1\n\n    final_ids = list(clusters.keys())\n    labels = np.empty(n, dtype=int)\n    centers = np.zeros(len(final_ids), dtype=float)\n    resultants = np.zeros(len(final_ids), dtype=float)\n    for new_label, cid in enumerate(final_ids):\n        indices = clusters[cid]\n        labels[indices] = new_label\n        mean_i, r_i = circ_mean_and_r(alpha[indices])\n        centers[new_label] = mean_i\n        resultants[new_label] = r_i\n\n    self.labels_ = labels\n    self.centers_ = centers\n    self.r_ = resultants\n    self.merges_ = np.array(merges, dtype=float) if merges else np.empty((0, 4), dtype=float)\n    return self\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircHAC.predict","title":"<code>predict(alpha)</code>","text":"<p>Assign new angles to the closest cluster center.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>array-like of shape (n_samples,)</code> required <p>Returns:</p> Name Type Description <code>labels</code> <code>np.ndarray of shape (n_samples,)</code> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict(self, alpha):\n    \"\"\"\n    Assign new angles to the closest cluster center.\n\n    Parameters\n    ----------\n    alpha : array-like of shape (n_samples,)\n\n    Returns\n    -------\n    labels : np.ndarray of shape (n_samples,)\n    \"\"\"\n    if self.centers_ is None:\n        raise ValueError(\"Model must be fitted before calling predict().\")\n\n    alpha = np.asarray(alpha, dtype=float)\n    alpha = alpha if self.unit == \"radian\" else data2rad(alpha, k=self.full_cycle)\n\n    k = self.centers_.size\n    labels = np.zeros(alpha.size, dtype=int)\n    for i, angle in enumerate(alpha):\n        distances = [abs(circ_dist(angle, center, metric=self.metric)) for center in self.centers_]\n        labels[i] = int(np.argmin(distances))\n    return labels\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircHAC.plot_dendrogram","title":"<code>plot_dendrogram(ax=None, **kwargs)</code>","text":"<p>Plot a rudimentary dendrogram from merges_.</p> <p>This is a basic approach that uses cluster IDs directly as \"labels\" on the x-axis. Because cluster IDs might not be contiguous or in ascending order, the result can look jumbled. A more sophisticated approach would re-compute a consistent labeling for each step.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>matplotlib Axes</code> <p>If None, create a new figure/axes.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Passed along to ax.plot(), e.g. color, linewidth, etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>matplotlib Axes</code> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def plot_dendrogram(self, ax=None, **kwargs):\n    \"\"\"\n    Plot a rudimentary dendrogram from merges_.\n\n    This is a basic approach that uses cluster IDs directly as \"labels\"\n    on the x-axis. Because cluster IDs might not be contiguous or in ascending\n    order, the result can look jumbled. A more sophisticated approach\n    would re-compute a consistent labeling for each step.\n\n    Parameters\n    ----------\n    ax : matplotlib Axes, optional\n        If None, create a new figure/axes.\n    **kwargs : dict\n        Passed along to ax.plot(), e.g. color, linewidth, etc.\n\n    Returns\n    -------\n    ax : matplotlib Axes\n    \"\"\"\n    import matplotlib.pyplot as plt\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 4))\n    merges = self.merges_\n    if merges.size == 0:\n        ax.set_title(\"No merges recorded (maybe n &lt;= n_clusters?).\")\n        return ax\n\n    # merges_ is (step, 4): [clusterA, clusterB, dist, new_size]\n    # We want to plot something like a dendrogram:\n    #  - each row is a merge event\n    #  - x-axis might show cluster A and cluster B, y the 'distance'\n    # But cluster IDs might keep re-labelling, so a quick hack is we show them as is.\n\n    for step, (ca, cb, distval, new_size) in enumerate(merges):\n        ca = int(ca)\n        cb = int(cb)\n        # We'll draw a \"u\" connecting ca and cb at height distval\n        # Then the newly formed cluster could get ID=cb or something\n        # This is a naive approach that won't produce a fancy SciPy-like dendrogram\n        # but enough to illustrate what's happening.\n\n        x1, x2 = ca, cb\n        y = distval\n        # a line from (x1, 0) to (x1, y), from (x2, 0) to (x2, y),\n        # then a horizontal line across at y\n        # we can color them or style them with kwargs\n\n        ax.plot([x1, x1], [0, y], **kwargs)\n        ax.plot([x2, x2], [0, y], **kwargs)\n        ax.plot([x1, x2], [y, y], **kwargs)\n\n    ax.set_title(\"Rudimentary Dendrogram\")\n    ax.set_xlabel(\"Cluster ID (raw internal IDs)\")\n    ax.set_ylabel(\"Distance\")\n    return ax\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircHAC.silhouette_score","title":"<code>silhouette_score()</code>","text":"<p>Compute the average silhouette for a cluster assignment on circular data.</p> <p>angles: np.ndarray shape (n,) in radians labels: np.ndarray shape (n,) in {0,1,...,K-1} metric: \"chord\", \"geodesic\", \"center\", etc.</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean silhouette over all points.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def silhouette_score(self):\n    \"\"\"\n    Compute the average silhouette for a cluster assignment on circular data.\n\n    angles: np.ndarray shape (n,) in radians\n    labels: np.ndarray shape (n,) in {0,1,...,K-1}\n    metric: \"chord\", \"geodesic\", \"center\", etc.\n\n    Returns\n    -------\n    float\n        The mean silhouette over all points.\n    \"\"\"\n    angles = self.alpha\n    labels = self.labels_\n    metric = self.metric\n    n = len(angles)\n    if n &lt; 2:\n        return 0.0\n\n    silhouette_values = np.zeros(n, dtype=float)\n\n    # Precompute all pairwise distances\n    # shape =&gt; (n,n)\n    pairwise = circ_dist(angles[:,None], angles[None,:], metric=metric)\n    pairwise = np.abs(pairwise)  # ensure nonnegative\n\n    for i in range(n):\n        c_i = labels[i]\n        # points in cluster c_i\n        in_cluster_i = (labels == c_i)\n        # average distance to own cluster\n        # excluding the point itself\n        a_i = pairwise[i, in_cluster_i].mean() if in_cluster_i.sum() &gt; 1 else 0.0\n\n        # find min average distance to another cluster\n        b_i = np.inf\n        for c_other in np.unique(labels):\n            if c_other == c_i:\n                continue\n            in_other = (labels == c_other)\n            dist_i_other = pairwise[i, in_other].mean()\n            if dist_i_other &lt; b_i:\n                b_i = dist_i_other\n\n        silhouette_values[i] = (b_i - a_i) / max(a_i, b_i) if max(a_i, b_i) &gt; 0 else 0.0\n\n    return silhouette_values.mean()\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircKMeans","title":"<code>CircKMeans</code>","text":"<p>K-Means clustering for circular (1D) data.</p> <p>This is analogous to standard K-Means, but uses circular distance and circular means. The algorithm is:</p> <p>1) Initialize cluster centers (angles in radians). 2) Assignment step:    Assign each data point to the cluster with the minimal    circular distance. 3) Update step:    Recompute each cluster center as the circular mean of    the assigned points. 4) Repeat until convergence or max_iters.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters to form.</p> <code>2</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>100</code> <code>metric</code> <code>('center', 'chord', 'geodesic', 'angularseparation')</code> <p>The distance measure used for assignment.</p> <code>\"center\"</code> <code>unit</code> <code>('degree', 'radian')</code> <p>Whether input data is in degrees or radians. If \"degree\", we convert to radians internally.</p> <code>\"degree\",\"radian\"</code> <code>full_cycle</code> <code>int</code> <p>For data conversion if unit=\"degree\".</p> <code>360</code> <code>tol</code> <code>float</code> <p>Convergence threshold. If centers move less than <code>tol</code> in total, the algorithm stops.</p> <code>1e-6</code> <code>random_seed</code> <code>int</code> <p>For reproducible initialization.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>centers_</code> <code>(ndarray, shape(n_clusters))</code> <p>The final cluster center angles (in radians).</p> <code>labels_</code> <code>(ndarray, shape(n_samples))</code> <p>The assigned cluster indices for each data point.</p> <code>inertia_</code> <code>float</code> <p>The final sum of distances (or sum of squared distances) if you prefer, from each point to its cluster center. By default, we store sum of chosen distance measure.</p> Source code in <code>pycircstat2/clustering.py</code> <pre><code>class CircKMeans:\n    \"\"\"\n    K-Means clustering for circular (1D) data.\n\n    This is analogous to standard K-Means, but uses circular\n    distance and circular means. The algorithm is:\n\n    1) Initialize cluster centers (angles in radians).\n    2) Assignment step:\n       Assign each data point to the cluster with the minimal\n       circular distance.\n    3) Update step:\n       Recompute each cluster center as the circular mean of\n       the assigned points.\n    4) Repeat until convergence or max_iters.\n\n    Parameters\n    ----------\n    n_clusters : int, default=2\n        Number of clusters to form.\n    max_iter : int, default=100\n        Maximum number of iterations.\n    metric : {\"center\", \"chord\", \"geodesic\", \"angularseparation\"}, default=\"chord\"\n        The distance measure used for assignment.\n    unit : {\"degree\",\"radian\"}, default=\"degree\"\n        Whether input data is in degrees or radians.\n        If \"degree\", we convert to radians internally.\n    full_cycle : int, default=360\n        For data conversion if unit=\"degree\".\n    tol : float, default=1e-6\n        Convergence threshold. If centers move less than `tol` in total,\n        the algorithm stops.\n    random_seed : int, default=None\n        For reproducible initialization.\n\n    Attributes\n    ----------\n    centers_ : np.ndarray, shape (n_clusters,)\n        The final cluster center angles (in radians).\n    labels_ : np.ndarray, shape (n_samples,)\n        The assigned cluster indices for each data point.\n    inertia_ : float\n        The final sum of distances (or sum of squared distances) if you prefer,\n        from each point to its cluster center. By default, we store\n        sum of chosen distance measure.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_clusters=2,\n        max_iter=100,\n        metric=\"center\",\n        unit=\"degree\",\n        full_cycle=360,\n        tol=1e-6,\n        random_seed=None\n    ):\n        self.n_clusters = n_clusters\n        self.max_iter = max_iter\n        self.metric = metric\n        self.unit = unit\n        self.full_cycle = full_cycle\n        self.tol = tol\n        self.random_seed = random_seed\n\n        self.centers_ = None\n        self.labels_ = None\n        self.inertia_ = None\n\n    def fit(self, X):\n        \"\"\"\n        Fit the K-means on 1D circular data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples,)\n            Angles in degrees (if self.unit==\"degree\") or radians.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.data = X = np.asarray(X, dtype=float)\n        if self.unit == \"degree\":\n            self.alpha = alpha = data2rad(X, k=self.full_cycle)\n        else:\n            self.alpha = alpha = X\n\n        rng = np.random.default_rng(self.random_seed)\n\n        n_samples = len(alpha)\n        if n_samples &lt; self.n_clusters:\n            # trivial: each point is its own cluster\n            self.labels_ = np.arange(n_samples)\n            self.centers_ = alpha.copy()\n            self.inertia_ = 0.0\n            return self\n\n        # 1) initialize cluster centers by picking random points from data\n        init_indices = rng.choice(n_samples, size=self.n_clusters, replace=False)\n        centers = alpha[init_indices]\n\n        labels = np.zeros(n_samples, dtype=int)\n        for iteration in range(self.max_iter):\n            # 2) assignment step\n            dist_mat = np.zeros((self.n_clusters, n_samples))\n            for c in range(self.n_clusters):\n                # measure distance from alpha to center[c]\n                dist_mat[c] = np.abs(circ_dist(alpha, centers[c], metric=self.metric))\n\n            labels_new = dist_mat.argmin(axis=0)\n\n            # 3) update step\n            new_centers = np.zeros_like(centers)\n            for c in range(self.n_clusters):\n                mask = (labels_new == c)\n                if np.any(mask):\n                    # circular mean of assigned points\n                    m, _ = circ_mean_and_r(alpha[mask])\n                    new_centers[c] = m\n                else:\n                    # if no points assigned, keep old center or random re-init\n                    new_centers[c] = centers[c]\n\n            # check for shift\n            shift = np.sum(np.abs(np.angle(np.exp(1j*centers) / np.exp(1j*new_centers))))\n            # or a simpler approach: sum of circ_dist(centers, new_centers)\n            # shift = float(np.sum(np.abs(circ_dist(centers, new_centers, metric=self.metric))))\n\n            labels = labels_new\n            centers = new_centers\n\n            if shift &lt; self.tol:\n                break\n\n        # final\n        self.centers_ = centers\n        self.labels_ = labels\n\n        # compute final inertia =&gt; sum of distances from points to assigned center\n        total_dist = 0.0\n        for c in range(self.n_clusters):\n            mask = (labels == c)\n            if np.any(mask):\n                dvals = np.abs(circ_dist(alpha[mask], centers[c], metric=self.metric))\n                total_dist += dvals.sum()\n        self.inertia_ = total_dist\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict cluster assignment for new data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples,)\n\n        Returns\n        -------\n        labels : np.ndarray, shape (n_samples,)\n        \"\"\"\n        if self.centers_ is None:\n            raise ValueError(\"Model not fitted. Call fit() first.\")\n\n        X = np.asarray(X, dtype=float)\n        if self.unit == \"degree\":\n            alpha = data2rad(X, k=self.full_cycle)\n        else:\n            alpha = X\n\n        n_samples = len(alpha)\n        dist_mat = np.zeros((self.n_clusters, n_samples))\n        for c in range(self.n_clusters):\n            dist_mat[c] = np.abs(circ_dist(alpha, self.centers_[c], metric=self.metric))\n        return dist_mat.argmin(axis=0)\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircKMeans.fit","title":"<code>fit(X)</code>","text":"<p>Fit the K-means on 1D circular data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples))</code> <p>Angles in degrees (if self.unit==\"degree\") or radians.</p> required <p>Returns:</p> Type Description <code>self</code> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def fit(self, X):\n    \"\"\"\n    Fit the K-means on 1D circular data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples,)\n        Angles in degrees (if self.unit==\"degree\") or radians.\n\n    Returns\n    -------\n    self\n    \"\"\"\n    self.data = X = np.asarray(X, dtype=float)\n    if self.unit == \"degree\":\n        self.alpha = alpha = data2rad(X, k=self.full_cycle)\n    else:\n        self.alpha = alpha = X\n\n    rng = np.random.default_rng(self.random_seed)\n\n    n_samples = len(alpha)\n    if n_samples &lt; self.n_clusters:\n        # trivial: each point is its own cluster\n        self.labels_ = np.arange(n_samples)\n        self.centers_ = alpha.copy()\n        self.inertia_ = 0.0\n        return self\n\n    # 1) initialize cluster centers by picking random points from data\n    init_indices = rng.choice(n_samples, size=self.n_clusters, replace=False)\n    centers = alpha[init_indices]\n\n    labels = np.zeros(n_samples, dtype=int)\n    for iteration in range(self.max_iter):\n        # 2) assignment step\n        dist_mat = np.zeros((self.n_clusters, n_samples))\n        for c in range(self.n_clusters):\n            # measure distance from alpha to center[c]\n            dist_mat[c] = np.abs(circ_dist(alpha, centers[c], metric=self.metric))\n\n        labels_new = dist_mat.argmin(axis=0)\n\n        # 3) update step\n        new_centers = np.zeros_like(centers)\n        for c in range(self.n_clusters):\n            mask = (labels_new == c)\n            if np.any(mask):\n                # circular mean of assigned points\n                m, _ = circ_mean_and_r(alpha[mask])\n                new_centers[c] = m\n            else:\n                # if no points assigned, keep old center or random re-init\n                new_centers[c] = centers[c]\n\n        # check for shift\n        shift = np.sum(np.abs(np.angle(np.exp(1j*centers) / np.exp(1j*new_centers))))\n        # or a simpler approach: sum of circ_dist(centers, new_centers)\n        # shift = float(np.sum(np.abs(circ_dist(centers, new_centers, metric=self.metric))))\n\n        labels = labels_new\n        centers = new_centers\n\n        if shift &lt; self.tol:\n            break\n\n    # final\n    self.centers_ = centers\n    self.labels_ = labels\n\n    # compute final inertia =&gt; sum of distances from points to assigned center\n    total_dist = 0.0\n    for c in range(self.n_clusters):\n        mask = (labels == c)\n        if np.any(mask):\n            dvals = np.abs(circ_dist(alpha[mask], centers[c], metric=self.metric))\n            total_dist += dvals.sum()\n    self.inertia_ = total_dist\n    return self\n</code></pre>"},{"location":"reference/clustering/#pycircstat2.clustering.CircKMeans.predict","title":"<code>predict(X)</code>","text":"<p>Predict cluster assignment for new data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples))</code> required <p>Returns:</p> Name Type Description <code>labels</code> <code>(ndarray, shape(n_samples))</code> Source code in <code>pycircstat2/clustering.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict cluster assignment for new data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples,)\n\n    Returns\n    -------\n    labels : np.ndarray, shape (n_samples,)\n    \"\"\"\n    if self.centers_ is None:\n        raise ValueError(\"Model not fitted. Call fit() first.\")\n\n    X = np.asarray(X, dtype=float)\n    if self.unit == \"degree\":\n        alpha = data2rad(X, k=self.full_cycle)\n    else:\n        alpha = X\n\n    n_samples = len(alpha)\n    dist_mat = np.zeros((self.n_clusters, n_samples))\n    for c in range(self.n_clusters):\n        dist_mat[c] = np.abs(circ_dist(alpha, self.centers_[c], metric=self.metric))\n    return dist_mat.argmin(axis=0)\n</code></pre>"},{"location":"reference/correlation/","title":"Correlation","text":""},{"location":"reference/correlation/#pycircstat2.correlation.circ_corrcc","title":"<code>circ_corrcc(a, b, method='fl', test=False, strict=True)</code>","text":"<p>Angular-Angular / Spherical Correlation.</p> <p>Three methods are available:</p> <ul> <li>'fl' (Fisher &amp; Lee, 1983): T-linear association. The correlation coefficient</li> </ul> \\[ r = \\frac{\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sin(a_{ij}) \\sin(b_{ij})}{\\sqrt{\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sin^2(a_{ij}) \\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sin^2(b_{ij})}} \\] <ul> <li>'js' (Jammalamadaka &amp; SenGupta, 2001)</li> </ul> \\[ r = \\frac{\\sum \\sin(a_i - \\bar{a}) \\sin(b_i - \\bar{b})}{\\sqrt{\\sum \\sin^2(a_i - \\bar{a}) \\sum \\sin^2(b_i - \\bar{b})}} \\] <ul> <li>'nonparametric'</li> </ul> \\[ r = \\frac{\\sum \\cos(C \\cdot \\text{rankdiff})^2 + \\sum \\sin(C \\cdot \\text{rankdiff})^2}{n^2} - \\frac{\\sum \\cos(C \\cdot \\text{ranksum})^2 + \\sum \\sin(C \\cdot \\text{ranksum})^2}{n^2} \\] <p>, where \\(C = 2\\pi / n\\) and \\(\\text{rankdiff} = \\text{rank}_a - \\text{rank}_b\\) and \\(\\text{ranksum} = \\text{rank}_a + \\text{rank}_b\\).</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[Circular, ndarray, Sequence[float]]</code> <p>Angles in radian</p> required <code>b</code> <code>Union[Circular, ndarray, Sequence[float]]</code> <p>Angles in radian</p> required <code>method</code> <code>str</code> <ul> <li>'fl' (Fisher &amp; Lee, 1983): T-linear association. The correlation coefficient   is computed as:</li> <li>'js' (Jammalamadaka &amp; SenGupta, 2001)</li> <li>'nonparametric'</li> </ul> <code>'fl'</code> <code>test</code> <code>bool</code> <p>Return significant test results.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>Strict mode. If True, raise an error when mean direction is not significant. Only for method=\"js\" (Jammalamadaka &amp; SenGupta, 2001).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>r</code> <code>float</code> <p>Correlation coefficient.</p> <code>reject</code> <code>bool</code> <p>Return significant test if <code>test</code> is set to True.</p> Source code in <code>pycircstat2/correlation.py</code> <pre><code>def circ_corrcc(\n    a: Union[Circular, np.ndarray, Sequence[float]],\n    b: Union[Circular, np.ndarray, Sequence[float]],\n    method: str = \"fl\",\n    test: bool = False,\n    strict: bool = True,\n) -&gt; Union[float, CorrelationResult]:\n    r\"\"\"\n    Angular-Angular / Spherical Correlation.\n\n    Three methods are available:\n\n    - 'fl' (Fisher &amp; Lee, 1983): T-linear association. The correlation coefficient\n\n    $$\n    r = \\frac{\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sin(a_{ij}) \\sin(b_{ij})}{\\sqrt{\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sin^2(a_{ij}) \\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sin^2(b_{ij})}}\n    $$\n\n    - 'js' (Jammalamadaka &amp; SenGupta, 2001)\n\n    $$\n    r = \\frac{\\sum \\sin(a_i - \\bar{a}) \\sin(b_i - \\bar{b})}{\\sqrt{\\sum \\sin^2(a_i - \\bar{a}) \\sum \\sin^2(b_i - \\bar{b})}}\n    $$\n\n    - 'nonparametric'\n\n    $$\n    r = \\frac{\\sum \\cos(C \\cdot \\text{rankdiff})^2 + \\sum \\sin(C \\cdot \\text{rankdiff})^2}{n^2} - \\frac{\\sum \\cos(C \\cdot \\text{ranksum})^2 + \\sum \\sin(C \\cdot \\text{ranksum})^2}{n^2}\n    $$\n\n    , where $C = 2\\pi / n$ and $\\text{rankdiff} = \\text{rank}_a - \\text{rank}_b$ and $\\text{ranksum} = \\text{rank}_a + \\text{rank}_b$.\n\n\n    Parameters\n    ----------\n    a: Circular or np.ndarray\n        Angles in radian\n    b: Circular or np.ndarray\n        Angles in radian\n    method: str\n        - 'fl' (Fisher &amp; Lee, 1983): T-linear association. The correlation coefficient\n          is computed as:\n        - 'js' (Jammalamadaka &amp; SenGupta, 2001)\n        - 'nonparametric'\n    test: bool\n        Return significant test results.\n    strict: bool\n        Strict mode. If True, raise an error when mean direction is\n        not significant. Only for method=\"js\" (Jammalamadaka &amp; SenGupta, 2001).\n\n    Returns\n    -------\n    r: float\n        Correlation coefficient.\n    reject: bool\n        Return significant test if `test` is set to True.\n    \"\"\"\n\n    method = method.lower()\n    if method == \"fl\":  # Fisher &amp; Lee (1983)\n        _corr = _circ_corrcc_fl\n    elif method == \"js\":  # Jammalamadaka &amp; SenGupta (2001)\n        _corr = _circ_corrcc_js\n    elif method == \"nonparametric\":\n        _corr = _circ_corrcc_np\n    else:\n        raise ValueError(\"Invalid method. Choose from 'fl', 'js', or 'nonparametric'.\")\n\n    result = _corr(a, b, test, strict)\n\n    return result if test else result.r\n</code></pre>"},{"location":"reference/correlation/#pycircstat2.correlation.circ_corrcl","title":"<code>circ_corrcl(a, x)</code>","text":"<p>Angular-Linear / Cylindrical Correlation based on Mardia (1972).</p> <p>Also known as Linear-circular or C-linear association (Fisher, 1993).</p> \\[ r = \\sqrt{\\frac{r_{xc}^2 + r_{xs}^2 - 2r_{xc}r_{xs}r_{cs}}{1 - r_{cs}^2}} \\] <p>where \\(r_{xc}\\), \\(r_{xs}\\), and \\(r_{cs}\\) are the correlation coefficients between \\(\\cos(a)\\) and \\(x\\), \\(x\\) and \\(\\sin(a)\\), and \\(\\sin(a)\\) and \\(\\cos(a)\\), respectively.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[Circular, ndarray, Sequence[float]]</code> <p>Angles in radian</p> required <code>x</code> <code>Union[ndarray, Sequence[float]]</code> <p>Linear variable</p> required <p>Returns:</p> Name Type Description <code>ral</code> <code>float</code> <p>correlation coefficient.</p> <code>pval</code> <code>float</code> Reference <p>P658-659, Section 27.15(b) of Example 27.21 (Zar, 2010).</p> Source code in <code>pycircstat2/correlation.py</code> <pre><code>def circ_corrcl(\n    a: Union[Circular, np.ndarray, Sequence[float]],\n    x: Union[np.ndarray, Sequence[float]],\n) -&gt; CorrelationResult:\n    r\"\"\"Angular-Linear / Cylindrical Correlation based on Mardia (1972).\n\n    Also known as Linear-circular or C-linear association (Fisher, 1993).\n\n    $$\n    r = \\sqrt{\\frac{r_{xc}^2 + r_{xs}^2 - 2r_{xc}r_{xs}r_{cs}}{1 - r_{cs}^2}}\n    $$\n\n    where $r_{xc}$, $r_{xs}$, and $r_{cs}$ are the correlation coefficients between\n    $\\cos(a)$ and $x$, $x$ and $\\sin(a)$, and $\\sin(a)$ and $\\cos(a)$, respectively.\n\n    Parameters\n    ----------\n    a: Circular or np.ndarray\n        Angles in radian\n    x: np.ndarray\n        Linear variable\n\n    Returns\n    -------\n    ral: float\n        correlation coefficient.\n    pval: float\n\n    Reference\n    ----\n    P658-659, Section 27.15(b) of Example 27.21 (Zar, 2010).\n    \"\"\"\n\n    a_alpha, _ = _coerce_angles(a)\n    x_arr = np.asarray(x, dtype=float)\n    if x_arr.ndim != 1:\n        raise ValueError(\"`x` must be a one-dimensional array.\")\n    if a_alpha.size != x_arr.size:\n        raise ValueError(\"`a` and `x` must be the same length.\")\n    if a_alpha.size &lt; 3:\n        raise ValueError(\"At least three paired observations are required.\")\n\n    n = a_alpha.size\n\n    cos_a = np.cos(a_alpha)\n    sin_a = np.sin(a_alpha)\n\n    rxc = np.corrcoef(cos_a, x_arr)[0, 1]\n    rxs = np.corrcoef(x_arr, sin_a)[0, 1]\n    rcs = np.corrcoef(sin_a, cos_a)[0, 1]\n\n    num = rxc**2 + rxs**2 - 2 * rxc * rxs * rcs\n    den = 1 - rcs**2\n    if np.isclose(den, 0.0):\n        raise ValueError(\"Degenerate data produced division by zero in denominator.\")\n    r = np.sqrt(max(num / den, 0.0))\n\n    pval = 1 - chi2(df=2).cdf(n * r**2)\n\n    return CorrelationResult(r=float(r), p_value=float(pval))\n</code></pre>"},{"location":"reference/descriptive/","title":"Descriptive Statistics","text":""},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_r","title":"<code>circ_r(alpha=None, w=None, Cbar=None, Sbar=None)</code>","text":"<p>Circular mean resultant vector length (r).</p> \\[ r = \\sqrt{\\bar{C}^2 + \\bar{S}^2} \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>Cbar</code> <code>Optional[float]</code> <p>Precomputed intermediate values</p> <code>None</code> <code>Sbar</code> <code>Optional[float]</code> <p>Precomputed intermediate values</p> <code>None</code> <p>Returns:</p> Name Type Description <code>r</code> <code>float</code> <p>Resultant vector length</p> References <p>Implementation of Example 26.5 (Zar, 2010)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_r(\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    Cbar: Optional[float] = None,\n    Sbar: Optional[float] = None,\n) -&gt; float:\n    r\"\"\"\n    Circular mean resultant vector length (r).\n\n    $$\n    r = \\sqrt{\\bar{C}^2 + \\bar{S}^2}\n    $$\n\n    Parameters\n    ----------\n    alpha: np.array (n, )\n        Angles in radian.\n    w: np.array (n,)\n        Frequencies or weights\n    Cbar, Sbar: float\n        Precomputed intermediate values\n\n    Returns\n    -------\n    r: float\n        Resultant vector length\n\n    References\n    ----------\n    Implementation of Example 26.5 (Zar, 2010)\n    \"\"\"\n    if Cbar is None or Sbar is None:\n        if alpha is None:\n            raise ValueError(\"`alpha` is required if `Cbar` and `Sbar` are not provided.\")\n        w = np.ones_like(alpha) if w is None else w\n        Cbar, Sbar = compute_C_and_S(alpha, w)\n\n    r = np.sqrt(Cbar**2 + Sbar**2)\n\n    return r\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean","title":"<code>circ_mean(alpha, w=None)</code>","text":"<p>Circular mean (m).</p> \\[\\cos\\bar\\theta = C/R,\\space \\sin\\bar\\theta = S/R\\] <p>or </p> \\[ \\bar\\theta = \\begin{cases}  \\tan^{-1}\\left(S/C\\right), &amp; \\text{if } S &gt; 0, C &gt; 0 \\\\  \\tan^{-1}\\left(S/C\\right) + \\pi, &amp; \\text{if } C &lt; 0 \\\\  \\tan^{-1}\\left(S/C\\right) + 2\\pi, &amp; \\text{S &lt; 0, C &gt; 0} \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <p>Returns:</p> Name Type Description <code>m</code> <code>float or NaN</code> <p>Circular mean</p> Note <p>Implementation of Example 26.5 (Zar, 2010)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_mean(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n) -&gt; float:\n    r\"\"\"\n    Circular mean (m).\n\n    $$\\cos\\bar\\theta = C/R,\\space \\sin\\bar\\theta = S/R$$\n\n    or \n\n    $$\n    \\bar\\theta =\n    \\begin{cases} \n    \\tan^{-1}\\left(S/C\\right), &amp; \\text{if } S &gt; 0, C &gt; 0 \\\\ \n    \\tan^{-1}\\left(S/C\\right) + \\pi, &amp; \\text{if } C &lt; 0 \\\\ \n    \\tan^{-1}\\left(S/C\\right) + 2\\pi, &amp; \\text{S &lt; 0, C &gt; 0}\n    \\end{cases}\n    $$\n\n    Parameters\n    ----------\n    alpha: np.array (n, )\n        Angles in radian.\n    w: np.array (n,)\n        Frequencies or weights\n\n    Returns\n    -------\n    m: float or NaN\n        Circular mean\n\n    Note\n    ----\n    Implementation of Example 26.5 (Zar, 2010)\n    \"\"\"\n    if w is None:\n        w = np.ones_like(alpha)\n\n    # mean resultant vector length\n    Cbar, Sbar = compute_C_and_S(alpha, w)\n    r = circ_r(alpha, w, Cbar, Sbar)\n\n    # angular mean\n    if np.isclose(r, 0):\n        m = np.nan\n    else:\n        m = np.arctan2(Sbar, Cbar)\n\n    return float(angmod(m))\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_and_r","title":"<code>circ_mean_and_r(alpha, w=None)</code>","text":"<p>Circular mean (m) and resultant vector length (r).</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <p>Returns:</p> Name Type Description <code>m</code> <code>float or NaN</code> <p>Circular mean</p> <code>r</code> <code>float</code> <p>Resultant vector length</p> Note <p>Implementation of Example 26.5 (Zar, 2010)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_mean_and_r(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Circular mean (m) and resultant vector length (r).\n\n    Parameters\n    ----------\n    alpha: np.array (n, )\n        Angles in radian.\n    w: np.array (n,)\n        Frequencies or weights\n\n    Returns\n    -------\n    m: float or NaN\n        Circular mean\n    r: float\n        Resultant vector length\n\n    Note\n    ----\n    Implementation of Example 26.5 (Zar, 2010)\n    \"\"\"\n    if w is None:\n        w = np.ones_like(alpha)\n\n    # mean resultant vector length\n    Cbar, Sbar = compute_C_and_S(alpha, w)\n    r = circ_r(alpha, w, Cbar, Sbar)\n\n    # angular mean\n    if np.isclose(r, 0.0, atol=1e-12):\n        return float(np.nan), float(r)\n\n    m = np.arctan2(Sbar, Cbar)\n\n    return float(angmod(m)), float(r)\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_and_r_of_means","title":"<code>circ_mean_and_r_of_means(circs=None, ms=None, rs=None)</code>","text":"<p>The Mean of a set of Mean Angles</p> <p>Parameters:</p> Name Type Description Default <code>circs</code> <code>Union[list, None]</code> <p>a list of Circular Objects</p> <code>None</code> <code>ms</code> <code>Optional[ndarray]</code> <p>a set of mean angles in radian</p> <code>None</code> <code>rs</code> <code>Optional[ndarray]</code> <p>a set of mean resultant vector lengths</p> <code>None</code> <p>Returns:</p> Name Type Description <code>m</code> <code>float</code> <p>mean of means in radian</p> <code>r</code> <code>float</code> <p>mean of mean resultant vector lengths</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_mean_and_r_of_means(\n    circs: Union[list, None] = None,\n    ms: Optional[np.ndarray] = None,\n    rs: Optional[np.ndarray] = None,\n) -&gt; Tuple[float, float]:\n    \"\"\"The Mean of a set of Mean Angles\n\n    Parameters\n    ----------\n    circs: list\n        a list of Circular Objects\n\n    ms: np.array (n, )\n        a set of mean angles in radian\n\n    rs: np.array (n, )\n        a set of mean resultant vector lengths\n\n    Returns\n    -------\n    m: float\n        mean of means in radian\n\n    r: float\n        mean of mean resultant vector lengths\n\n    \"\"\"\n    if circs is None:\n        if ms is None or rs is None:\n            raise ValueError(\"If `circs` is None, then `ms` and `rs` must be provided.\")\n        ms_arr = np.asarray(ms, dtype=float)\n        rs_arr = np.asarray(rs, dtype=float)\n    else:\n        extracted = [(circ.mean, circ.r) for circ in circs]\n        if len(extracted) == 0:\n            raise ValueError(\"`circs` must contain at least one element.\")\n        arr = np.asarray(extracted, dtype=float)\n        ms_arr, rs_arr = arr[:, 0], arr[:, 1]\n\n    if ms_arr.ndim != 1 or rs_arr.ndim != 1:\n        raise ValueError(\"`ms` and `rs` must be one-dimensional sequences of equal length.\")\n\n    if ms_arr.size != rs_arr.size or ms_arr.size == 0:\n        raise ValueError(\"`ms` and `rs` must be non-empty and have the same length.\")\n\n    X = np.mean(np.cos(ms_arr) * rs_arr)\n    Y = np.mean(np.sin(ms_arr) * rs_arr)\n    r = np.hypot(X, Y)\n\n    if np.isclose(r, 0.0, atol=1e-12):\n        return float(np.nan), float(r)\n\n    m = angmod(np.arctan2(Y, X))\n\n    return float(m), float(r)\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_moment","title":"<code>circ_moment(alpha, w=None, p=1, mean=None, centered=False)</code>","text":"<p>Compute the p-th circular moment.</p> \\[ m^{\\prime}_{p} = \\bar{C}_{p} + i\\bar{S}_{p} \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights. If None, equal weights are used.</p> <code>None</code> <code>p</code> <code>int</code> <p>Order of the moment to compute.</p> <code>1</code> <code>mean</code> <code>Union[float, ndarray, None]</code> <p>Precomputed circular mean. If None, mean is computed internally.</p> <code>None</code> <code>centered</code> <code>bool</code> <p>If True, center alpha by subtracting the mean.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>mp</code> <code>complex</code> <p>The p-th circular moment as a complex number.</p> Note <p>Implementation of Equation 2.24 (Fisher, 1993).</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_moment(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n    p: int = 1,\n    mean: Union[float, np.ndarray, None] = None,\n    centered: bool = False,\n) -&gt; complex:\n    r\"\"\"\n    Compute the p-th circular moment.\n\n    $$\n    m^{\\prime}_{p} = \\bar{C}_{p} + i\\bar{S}_{p}\n    $$\n\n    Parameters\n    ----------\n    alpha: np.ndarray\n        Angles in radian.\n    w: np.ndarray, optional\n        Frequencies or weights. If None, equal weights are used.\n    p: int, optional\n        Order of the moment to compute.\n    mean: float, optional\n        Precomputed circular mean. If None, mean is computed internally.\n    centered: bool, optional\n        If True, center alpha by subtracting the mean.\n\n    Returns\n    -------\n    mp: complex\n        The p-th circular moment as a complex number.\n\n    Note\n    ----\n    Implementation of Equation 2.24 (Fisher, 1993).\n    \"\"\"\n    if w is None:\n        w = np.ones_like(alpha)\n\n    if mean is None:\n        mean = circ_mean(alpha, w) if centered else 0.0\n\n    Cbar, Sbar = compute_C_and_S(alpha, w, p, mean)\n\n    return Cbar + 1j * Sbar\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_dispersion","title":"<code>circ_dispersion(alpha, w=None, mean=None)</code>","text":"<p>Sample Circular Dispersion, defined by Equation 2.28 (Fisher, 1993):</p> \\[ \\hat\\delta = (1 - \\hat\\rho_{2})/(2 \\hat\\rho_{1}^{2}) \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>mean</code> <p>Precomputed circular mean.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dispersion</code> <code>float</code> <p>Sample Circular Dispersion</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_dispersion(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n    mean=None,\n) -&gt; float:\n    r\"\"\"\n    Sample Circular Dispersion, defined by Equation 2.28 (Fisher, 1993):\n\n    $$\n    \\hat\\delta = (1 - \\hat\\rho_{2})/(2 \\hat\\rho_{1}^{2})\n    $$\n\n    Parameters\n    ----------\n\n    alpha: np.array, (n, )\n        Angles in radian.\n    w: None or np.array, (n)\n        Frequencies or weights\n    mean: None or float\n        Precomputed circular mean.\n\n    Returns\n    -------\n    dispersion: float\n        Sample Circular Dispersion\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    mp1 = circ_moment(alpha=alpha, w=w, p=1, mean=mean, centered=False)  # eq(2.26)\n    mp2 = circ_moment(alpha=alpha, w=w, p=2, mean=mean, centered=False)  # eq(2.27)\n\n    r1 = np.abs(mp1)\n    r2 = np.abs(mp2)\n\n    dispersion = (1 - r2) / (2 * r1**2)  # eq(2.28)\n\n    return dispersion\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_skewness","title":"<code>circ_skewness(alpha, w=None)</code>","text":"<p>Circular skewness, as defined by Equation 2.29 (Fisher, 1993):</p> \\[\\hat s = [\\hat\\rho_2 \\sin(\\hat\\mu_2 - 2 \\hat\\mu_1)] / (1 - \\hat\\rho_1)^{\\frac{3}{2}}\\] <p>But unlike the implementation of Fisher (1993), here we followed Pewsey et al. (2014) by NOT centering the second moment.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <p>Returns:</p> Name Type Description <code>skewness</code> <code>float</code> <p>Circular Skewness</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_skewness(alpha: np.ndarray, w: Optional[np.ndarray] = None) -&gt; float:\n    r\"\"\"\n    Circular skewness, as defined by Equation 2.29 (Fisher, 1993):\n\n    $$\\hat s = [\\hat\\rho_2 \\sin(\\hat\\mu_2 - 2 \\hat\\mu_1)] / (1 - \\hat\\rho_1)^{\\frac{3}{2}}$$\n\n    But unlike the implementation of Fisher (1993), here we followed Pewsey et al. (2014) by NOT centering the second moment.\n\n    Parameters\n    ----------\n\n    alpha: np.array, (n, )\n        Angles in radian.\n    w: None or np.array, (n)\n        Frequencies or weights\n\n    Returns\n    -------\n    skewness: float\n        Circular Skewness\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    mp1 = circ_moment(alpha=alpha, w=w, p=1, mean=None, centered=False)\n    mp2 = circ_moment(alpha=alpha, w=w, p=2, mean=None, centered=False)  # eq(2.27)\n\n    u1, r1 = convert_moment(mp1)\n    u2, r2 = convert_moment(mp2)\n\n    skewness = (r2 * np.sin(u2 - 2 * u1)) / (1 - r1) ** 1.5\n\n    return skewness\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_kurtosis","title":"<code>circ_kurtosis(alpha, w=None)</code>","text":"<p>Circular kurtosis, as defined by Equation 2.30 (Fisher, 1993):</p> \\[\\hat k = [\\hat\\rho_2 \\cos(\\hat\\mu_2 - 2 \\hat\\mu_1) - \\hat\\rho_1^4] / (1 - \\hat\\rho_1)^{2}\\] <p>But unlike the implementation of Fisher (1993), here we followed Pewsey et al. (2014) by NOT centering the second moment.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <p>Returns:</p> Name Type Description <code>kurtosis</code> <code>float</code> <p>Circular Kurtosis</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_kurtosis(alpha: np.ndarray, w: Optional[np.ndarray] = None) -&gt; float:\n    r\"\"\"\n    Circular kurtosis, as defined by Equation 2.30 (Fisher, 1993):\n\n    $$\\hat k = [\\hat\\rho_2 \\cos(\\hat\\mu_2 - 2 \\hat\\mu_1) - \\hat\\rho_1^4] / (1 - \\hat\\rho_1)^{2}$$\n\n    But unlike the implementation of Fisher (1993), here we followed Pewsey et al. (2014) by **NOT** centering the second moment.\n\n    Parameters\n    ----------\n\n    alpha: np.array, (n, )\n        Angles in radian.\n    w: None or np.array, (n)\n        Frequencies or weights\n\n    Returns\n    -------\n    kurtosis: float\n        Circular Kurtosis\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    mp1 = circ_moment(alpha=alpha, w=w, p=1, mean=None, centered=False)\n    mp2 = circ_moment(alpha=alpha, w=w, p=2, mean=None, centered=False)  # eq(2.27)\n\n    u1, r1 = convert_moment(mp1)\n    u2, r2 = convert_moment(mp2)\n\n    kurtosis = (r2 * np.cos(u2 - 2 * u1) - r1**4) / (1 - r1) ** 2\n\n    return kurtosis\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.angular_var","title":"<code>angular_var(alpha=None, w=None, r=None, bin_size=None)</code>","text":"<p>Angular variance</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>r</code> <code>Optional[float]</code> <p>Resultant vector length</p> <code>None</code> <code>bin_size</code> <code>Optional[float]</code> <p>Interval size of grouped data. Needed for correcting biased r.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>angular_variance</code> <code>float</code> <p>Angular variance, range from 0 to 2.</p> References <ul> <li>Batschlet (1965, 1981), from Section 26.5 of Zar (2010)</li> </ul> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def angular_var(\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    r: Optional[float] = None,\n    bin_size: Optional[float] = None,\n) -&gt; float:\n    r\"\"\"\n    Angular variance\n\n    Parameters\n    ----------\n    alpha: np.array (n, ) or None\n        Angles in radian.\n    w: np.array (n,) or None\n        Frequencies or weights\n    r: float or None\n        Resultant vector length\n    bin_size: float\n        Interval size of grouped data. Needed for correcting biased r.\n\n    Returns\n    -------\n    angular_variance: float\n        Angular variance, range from 0 to 2.\n\n    References\n    ----------\n    - Batschlet (1965, 1981), from Section 26.5 of Zar (2010)\n    \"\"\"\n\n    variance = circ_var(alpha=alpha, w=w, r=r, bin_size=bin_size)\n    angular_variance = 2 * variance\n    return angular_variance\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.angular_std","title":"<code>angular_std(alpha=None, w=None, r=None, bin_size=None)</code>","text":"<p>Angular (standard) deviation</p> \\[ s = \\sqrt{2V} = \\sqrt{2(1 - r)} \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>r</code> <code>Optional[float]</code> <p>Resultant vector length</p> <code>None</code> <code>bin_size</code> <code>Optional[float]</code> <p>Interval size of grouped data. Needed for correcting biased r.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>angular_std</code> <code>float</code> <p>Angular (standard) deviation, range from 0 to sqrt(2).</p> References <ul> <li>Equation 26.20 of Zar (2010)</li> </ul> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def angular_std(\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    r: Optional[float] = None,\n    bin_size: Optional[float] = None,\n) -&gt; float:\n    r\"\"\"\n    Angular (standard) deviation\n\n    $$\n    s = \\sqrt{2V} = \\sqrt{2(1 - r)}\n    $$\n\n    Parameters\n    ----------\n    alpha: np.array (n, ) or None\n        Angles in radian.\n    w: np.array (n,) or None\n        Frequencies or weights\n    r: float or None\n        Resultant vector length\n    bin_size: float\n        Interval size of grouped data. Needed for correcting biased r.\n\n    Returns\n    -------\n    angular_std: float\n        Angular (standard) deviation, range from 0 to sqrt(2).\n\n    References\n    ----------\n    - Equation 26.20 of Zar (2010)\n    \"\"\"\n\n    angular_variance = angular_var(alpha=alpha, w=w, r=r, bin_size=bin_size)\n    angular_std = np.sqrt(angular_variance)\n    return angular_std\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_var","title":"<code>circ_var(alpha=None, w=None, r=None, bin_size=None)</code>","text":"<p>Circular variance</p> \\[ V = 1 - r \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>r</code> <code>Optional[float]</code> <p>Resultant vector length</p> <code>None</code> <code>bin_size</code> <code>Optional[float]</code> <p>Interval size of grouped data. Needed for correcting biased r.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>variance</code> <code>float</code> <p>Circular variance, range from 0 to 1.</p> References <ul> <li>Equation 2.11 of Fisher (1993)</li> <li>Equation 26.17 of Zar (2010)</li> </ul> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_var(\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    r: Optional[float] = None,\n    bin_size: Optional[float] = None,\n) -&gt; float:\n    r\"\"\"\n    Circular variance\n\n    $$ V = 1 - r $$\n\n    Parameters\n    ----------\n    alpha: np.array (n, ) or None\n        Angles in radian.\n    w: np.array (n,) or None\n        Frequencies or weights\n    r: float or None\n        Resultant vector length\n    bin_size: float\n        Interval size of grouped data. Needed for correcting biased r.\n\n    Returns\n    -------\n    variance: float\n        Circular variance, range from 0 to 1.\n\n    References\n    ----------\n    - Equation 2.11 of Fisher (1993)\n    - Equation 26.17 of Zar (2010)\n    \"\"\"\n\n    # If `r` is provided, use it directly\n    if r is None:\n        if alpha is None:\n            raise ValueError(\"If `r` is None, then `alpha` is required to compute it.\")\n        r = circ_r(alpha, w)  # `circ_r` already handles `w=None` as `np.ones_like(alpha)`\n\n    # Determine bin_size if not explicitly provided\n    if bin_size is None and w is not None and not np.all(w == w[0]):\n        if alpha is None:\n            raise ValueError(\"If `bin_size` is None but `w` is provided, `alpha` must be given.\")\n        bin_size = float(np.diff(alpha).min())\n\n    # Correct `r` if binning is applied\n    rc = r if bin_size is None or bin_size == 0 else r * (bin_size / (2 * np.sin(bin_size / 2)))\n\n    variance = 1 - rc\n\n    return variance\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_std","title":"<code>circ_std(alpha=None, w=None, r=None, bin_size=None)</code>","text":"<p>Circular standard deviation (s).</p> \\[ s = \\sqrt{-2 \\ln(1 - V)} \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>r</code> <code>Optional[float]</code> <p>Resultant vector length</p> <code>None</code> <code>bin_size</code> <code>Optional[float]</code> <p>Interval size of grouped data. Needed for correcting biased r.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>s</code> <code>float</code> <p>Circular standard deviation.</p> References <p>Implementation of Equation 26.15-16/20-21 (Zar, 2010)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_std(\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    r: Optional[float] = None,\n    bin_size: Optional[float] = None,\n) -&gt; float:\n    r\"\"\"\n    Circular standard deviation (s).\n\n    $$ s = \\sqrt{-2 \\ln(1 - V)} $$\n\n    Parameters\n    ----------\n    alpha: np.array (n, ) or None\n        Angles in radian.\n    w: np.array (n,) or None\n        Frequencies or weights\n    r: float or None\n        Resultant vector length\n    bin_size: float\n        Interval size of grouped data.\n        Needed for correcting biased r.\n\n    Returns\n    -------\n    s: float\n        Circular standard deviation.\n\n    References\n    ----------\n    Implementation of Equation 26.15-16/20-21 (Zar, 2010)\n    \"\"\"\n    var = circ_var(alpha=alpha, w=w, r=r, bin_size=bin_size)\n\n    # circular standard deviation\n    s = np.sqrt(-2 * np.log(1 - var))  # eq(26.21)\n\n    return s\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_median","title":"<code>circ_median(alpha, w=None, method='deviation', return_average=True, average_method='all', verbose=False)</code>","text":"<p>Circular median.</p> <p>Two ways to compute the circular median for ungrouped data (Fisher, 1993):</p> <ul> <li><code>deviation</code>: find the angle that has the minimal mean deviation.</li> <li><code>count</code>: find the angle that has the equally devide the number of points on the right and left of it.</li> </ul> <p>For grouped data, we use the method described in Mardia (1972).</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>method</code> <code>str</code> <ul> <li>For ungrouped data, there are two ways</li> <li>To compute the medians:<ul> <li>deviation</li> <li>count</li> </ul> </li> <li>Set to <code>none</code> to return np.nan.</li> </ul> <code>'deviation'</code> <code>return_average</code> <code>bool</code> <p>Return the average of the median</p> <code>True</code> <code>average_method</code> <code>str</code> <ul> <li>all: circular mean of all medians</li> <li>unique: circular mean of unique medians</li> </ul> <code>'all'</code> <p>Returns:</p> Name Type Description <code>median</code> <code>float or NaN</code> References <ul> <li>For ungrouped data: Section 2.3.2 of Fisher (1993)</li> <li>For grouped data: Mardia (1972)</li> </ul> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_median(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n    method: str = \"deviation\",\n    return_average: bool = True,\n    average_method: str = \"all\",\n    verbose: bool = False,\n) -&gt; Union[float, np.ndarray]:\n    r\"\"\"\n    Circular median.\n\n    Two ways to compute the circular median for ungrouped data (Fisher, 1993):\n\n    - `deviation`: find the angle that has the minimal mean deviation.\n    - `count`: find the angle that has the equally devide the number of points on the right and left of it.\n\n    For grouped data, we use the method described in Mardia (1972).\n\n    Parameters\n    ----------\n    alpha: np.array (n, )\n        Angles in radian.\n    w: np.array (n,) or None\n        Frequencies or weights\n    method: str\n        - For ungrouped data, there are two ways\n        - To compute the medians:\n            - deviation\n            - count\n        - Set to `none` to return np.nan.\n    return_average: bool\n        Return the average of the median\n    average_method: str\n        - all: circular mean of all medians\n        - unique: circular mean of unique medians\n\n    Returns\n    -------\n    median: float or NaN\n\n    References\n    ----------\n    - For ungrouped data: Section 2.3.2 of Fisher (1993)\n    - For grouped data: Mardia (1972)\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    # edge cases for early exit\n    # if all points coincide, return the first point\n    if np.isclose(circ_r(alpha, w), 1.0, atol=1e-12):\n        if verbose:\n            print(\"All points coincide, returning the first point as median.\")\n        return alpha[0]\n\n    # grouped data\n    if not np.all(w == 1):\n        median = _circ_median_grouped(alpha, w)\n    # ungrouped data\n    else:\n        # find which data point that can divide the dataset into two half\n        if method == \"count\":\n            median = _circ_median_count(alpha)\n        # find the angle that has the minimal mean deviation\n        elif method == \"deviation\":\n            median = _circ_median_mean_deviation(alpha)\n        elif method == \"none\" or method is None:\n            median = np.nan\n        else:\n            raise ValueError(\n                f\"Method `{method}` for `circ_median` is not supported.\\nTry `deviation` or `count`\"\n            )\n\n    if return_average:\n        if average_method == \"all\":\n            # Circular mean of all medians\n            median = circ_mean(alpha=np.asarray(median))\n        elif average_method == \"unique\":\n            # Circular mean of unique medians\n            median = circ_mean(alpha=np.unique(median))\n        else:\n            raise ValueError(\n                f\"Average method `{average_method}` is not supported.\\nTry `all` or `unique`.\"\n            )\n\n    return angmod(median)\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_deviation_chunked","title":"<code>circ_mean_deviation_chunked(alpha, beta, chunk_size=1000)</code>","text":"<p>Optimized circular mean deviation with chunking.</p> \\[ \\delta = \\pi - \\frac{1}{n} \\sum^{n}_{1}\\left| \\pi - \\left| \\alpha - \\beta \\right| \\right| \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>array - like</code> <p>Data in radians.</p> required <code>beta</code> <code>array - like</code> <p>Reference angles in radians.</p> required <code>chunk_size</code> <code>int</code> <p>Number of rows to process in chunks (must be positive).</p> <code>1000</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Circular mean deviation.</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_mean_deviation_chunked(\n    alpha: Union[np.ndarray, float, int, list],\n    beta: Union[np.ndarray, float, int, list],\n    chunk_size: int = 1000,\n) -&gt; np.ndarray:\n    r\"\"\"\n    Optimized circular mean deviation with chunking.\n\n    $$\n    \\delta = \\pi - \\frac{1}{n} \\sum^{n}_{1}\\left| \\pi - \\left| \\alpha - \\beta \\right| \\right|\n    $$\n\n    Parameters\n    ----------\n    alpha : array-like\n        Data in radians.\n    beta : array-like\n        Reference angles in radians.\n    chunk_size : int\n        Number of rows to process in chunks (must be positive).\n\n    Returns\n    -------\n    np.ndarray\n        Circular mean deviation.\n    \"\"\"\n\n    if chunk_size &lt;= 0:\n        raise ValueError(\"`chunk_size` must be a positive integer.\")\n\n    alpha_arr = np.atleast_1d(np.asarray(alpha, dtype=float))\n    beta_arr = np.atleast_1d(np.asarray(beta, dtype=float))\n\n    result = np.empty(beta_arr.size, dtype=float)\n\n    for start in range(0, beta_arr.size, chunk_size):\n        stop = start + chunk_size\n        beta_chunk = beta_arr[start:stop]\n        angdist = np.pi - np.abs(np.pi - np.abs(alpha_arr - beta_chunk[:, None]))\n        chunk_mean = np.round(np.mean(angdist, axis=1), 5)\n        result[start : start + beta_chunk.size] = chunk_mean\n\n    return result\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_deviation","title":"<code>circ_mean_deviation(alpha, beta)</code>","text":"<p>Circular mean deviation.</p> \\[ \\delta = \\pi - \\left| \\pi - \\left| \\alpha - \\beta \\right| \\right| / n \\] <p>It is the mean angular distance from one data point to all others. The circular median of a set of data should be the point with minimal circular mean deviation.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Union[ndarray, float, int, list]</code> <p>Data in radian.</p> required <code>beta</code> <code>Union[ndarray, float, int, list]</code> <p>reference angle in radian.</p> required <p>Returns:</p> Type Description <code>circular mean deviation: np.array</code> Note <p>eq 2.32, Section 2.3.2, Fisher (1993)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_mean_deviation(\n    alpha: Union[np.ndarray, float, int, list],\n    beta: Union[np.ndarray, float, int, list],\n) -&gt; np.ndarray:\n    r\"\"\"\n    Circular mean deviation.\n\n    $$\n    \\delta = \\pi - \\left| \\pi - \\left| \\alpha - \\beta \\right| \\right| / n\n    $$\n\n    It is the mean angular distance from one data point to all others.\n    The circular median of a set of data should be the point with minimal\n    circular mean deviation.\n\n    Parameters\n    ---------\n    alpha: np.array, int or float\n        Data in radian.\n    beta: np.array, int or float\n        reference angle in radian.\n\n    Returns\n    -------\n    circular mean deviation: np.array\n\n    Note\n    ----\n    eq 2.32, Section 2.3.2, Fisher (1993)\n    \"\"\"\n    alpha_arr = np.atleast_1d(np.asarray(alpha, dtype=float))\n    beta_arr = np.atleast_1d(np.asarray(beta, dtype=float))\n\n    mean_dist = np.mean(\n        np.abs(np.pi - np.abs(alpha_arr - beta_arr[:, None])),\n        axis=1,\n    )\n    return np.round(np.pi - mean_dist, 5)\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_ci","title":"<code>circ_mean_ci(alpha=None, w=None, mean=None, r=None, n=None, ci=0.95, method='approximate', B=2000)</code>","text":"<p>Confidence interval of circular mean.</p> <p>There are three methods to compute the confidence interval of circular mean:</p> <ul> <li><code>approximate</code>: for n &gt; 8</li> <li><code>bootstrap</code>: for 8 &lt; n &lt; 25</li> <li><code>dispersion</code>: for n &gt;= 25</li> </ul>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_ci--approximate-method","title":"Approximate Method","text":"<p>For n as small as 8, and r \\(\\le\\) 0.9, r \\(&gt;\\) \\(\\sqrt{\\chi^{2}_{\\alpha, 1}/2n}\\), the confidence interval can be approximated by:</p> \\[ \\delta = \\arccos\\left(\\sqrt{\\frac{2n(2R^{2} - n\\chi^{2}_{\\alpha, 1})}{4n - \\chi^{2}_{\\alpha, 1}}} /R \\right) \\] <p>For r \\(\\ge\\) 0.9,</p> \\[ \\delta = \\arccos \\left(\\sqrt{n^2 - (n^2 - R^2)e^{\\chi^2_{\\alpha, 1}/n} } /R \\right) \\]"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_ci--bootstrap-method","title":"Bootstrap Method","text":"<p>For 8 \\(&lt;\\) n \\(&lt;\\) 25, the confidence interval can be computed by bootstrapping the data.</p>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_mean_ci--dispersion-method","title":"Dispersion Method","text":"<p>For n \\(\\ge\\) 25, the confidence interval can be computed by the circular dispersion:</p> \\[ \\hat\\sigma = \\hat\\delta / n\\] <p>where \\(\\hat\\delta\\) is the sample circular dispersion (see <code>circ_dispersion</code>). The confidence interval is then:</p> \\[(\\hat\\mu - \\sin^-1(z_{\\frac{1}{2}\\alpha}\\hat\\sigma),\\space \\hat\\mu + \\sin^-1(z_{\\frac{1}{2}\\alpha} \\hat\\sigma))\\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <code>mean</code> <code>Optional[float]</code> <p>Precomputed circular mean.</p> <code>None</code> <code>r</code> <code>Optional[float]</code> <p>Precomputed resultant vector length.</p> <code>None</code> <code>n</code> <code>Union[int, None]</code> <p>Sample size.</p> <code>None</code> <code>ci</code> <code>float</code> <p>Confidence interval (default is 0.95).</p> <code>0.95</code> <code>method</code> <code>str</code> <ul> <li>approximate: for n &gt; 8</li> <li>bootstrap: for n &lt; 25</li> <li>dispersion: for n &gt;= 25</li> </ul> <code>'approximate'</code> <code>B</code> <code>int</code> <p>Number of samples for bootstrap.</p> <code>2000</code> <p>Returns:</p> Name Type Description <code>lower_bound</code> <code>float</code> <p>Lower bound of the confidence interval.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound of the confidence</p> References <ul> <li>Section 26.7, Zar (2010)</li> <li>Section 4.4.4a/b, Fisher (1993)</li> </ul> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_mean_ci(\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    mean: Optional[float] = None,\n    r: Optional[float] = None,\n    n: Union[int, None] = None,\n    ci: float = 0.95,\n    method: str = \"approximate\",\n    B: int = 2000,  # number of samples for bootstrap\n) -&gt; tuple[float, float]:\n    r\"\"\"\n    Confidence interval of circular mean.\n\n    There are three methods to compute the confidence interval of circular mean:\n\n    - `approximate`: for n &gt; 8\n    - `bootstrap`: for 8 &lt; n &lt; 25\n    - `dispersion`: for n &gt;= 25\n\n    ### Approximate Method\n\n    For n as small as 8, and r $\\le$ 0.9, r $&gt;$ $\\sqrt{\\chi^{2}_{\\alpha, 1}/2n}$, the confidence interval can be approximated by:\n\n    $$\n    \\delta = \\arccos\\left(\\sqrt{\\frac{2n(2R^{2} - n\\chi^{2}_{\\alpha, 1})}{4n - \\chi^{2}_{\\alpha, 1}}} /R \\right)\n    $$\n\n    For r $\\ge$ 0.9,\n\n    $$\n    \\delta = \\arccos \\left(\\sqrt{n^2 - (n^2 - R^2)e^{\\chi^2_{\\alpha, 1}/n} } /R \\right)\n    $$\n\n    ### Bootstrap Method\n\n    For 8 $&lt;$ n $&lt;$ 25, the confidence interval can be computed by bootstrapping the data.\n\n    ### Dispersion Method\n\n    For n $\\ge$ 25, the confidence interval can be computed by the circular dispersion:\n\n    $$ \\hat\\sigma = \\hat\\delta / n$$\n\n    where $\\hat\\delta$ is the sample circular dispersion (see `circ_dispersion`). The confidence interval is then:\n\n    $$(\\hat\\mu - \\sin^-1(z_{\\frac{1}{2}\\alpha}\\hat\\sigma),\\space \\hat\\mu + \\sin^-1(z_{\\frac{1}{2}\\alpha} \\hat\\sigma))$$\n\n    Parameters\n    ----------\n    alpha: np.array (n, )\n        Angles in radian.\n    w: np.array (n,) or None\n        Frequencies or weights\n    mean: float or None\n        Precomputed circular mean.\n    r: float or None\n        Precomputed resultant vector length.\n    n: int or None\n        Sample size.\n    ci: float\n        Confidence interval (default is 0.95).\n    method: str\n        - approximate: for n &gt; 8\n        - bootstrap: for n &lt; 25\n        - dispersion: for n &gt;= 25\n    B: int\n        Number of samples for bootstrap.\n\n    Returns\n    -------\n    lower_bound: float\n        Lower bound of the confidence interval.\n    upper_bound: float\n        Upper bound of the confidence\n\n    References\n    ----------\n    - Section 26.7, Zar (2010)\n    - Section 4.4.4a/b, Fisher (1993)\n    \"\"\"\n\n\n\n    #  n &gt; 8, according to Ch 26.7 (Zar, 2010)\n    if method == \"approximate\":\n        (lb, ub) = _circ_mean_ci_approximate(\n            alpha=alpha, w=w, mean=mean, r=r, n=n, ci=ci\n        )\n\n    # n &lt; 25, according to 4.4.4a (Fisher, 1993, P75)\n    elif method == \"bootstrap\" and alpha is not None:\n        (lb, ub) = _circ_mean_ci_bootstrap(alpha=alpha, B=B, ci=ci)\n\n    # n &gt;= 25, according to 4.4.4b (Fisher, 1993, P75)\n    elif method == \"dispersion\" and alpha is not None:\n        (lb, ub) = _circ_mean_ci_dispersion(alpha=alpha, w=w, mean=mean, ci=ci)\n\n    else:\n        raise ValueError(\n            f\"Method `{method}` for `circ_mean_ci` is not supported.\\nTry `dispersion`, `approximate` or `bootstrap`\"\n        )\n\n    return float(angmod(lb)), float(angmod(ub))\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_median_ci","title":"<code>circ_median_ci(median=None, alpha=None, w=None, method='deviation', ci=0.95)</code>","text":"<p>Confidence interval for circular median</p> <p>For n &gt; 15, the confidence interval can be computed by:</p> \\[ m = 1 + \\text{integer part of} \\frac{1}{2} n^{1/2} z_{\\frac{1}{2}\\alpha} \\] <p>For n \\(\\le\\) 15, the confidence interval can be selected from the table in Fisher (1993).</p> <p>Parameters:</p> Name Type Description Default <code>median</code> <code>Optional[float]</code> <p>Circular median.</p> <code>None</code> <code>alpha</code> <code>Optional[ndarray]</code> <p>Data in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies or weights</p> <code>None</code> <p>Returns:</p> Type Description <code>lower, upper, ci: tuple</code> <p>confidence intervals and alpha-level</p> Note <p>Implementation of section 4.4.2 (Fisher,1993)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_median_ci(\n    median: Optional[float] = None,\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    method: str = \"deviation\",\n    ci: float = 0.95,\n) -&gt; tuple:\n    r\"\"\"Confidence interval for circular median\n\n    For n &gt; 15, the confidence interval can be computed by:\n\n    $$\n    m = 1 + \\text{integer part of} \\frac{1}{2} n^{1/2} z_{\\frac{1}{2}\\alpha}\n    $$\n\n    For n $\\le$ 15, the confidence interval can be selected from the table in Fisher (1993).\n\n    Parameters\n    ----------\n    median: float or None\n        Circular median.\n    alpha: np.array or None\n        Data in radian.\n    w: np.array or None\n        Frequencies or weights\n\n    Returns\n    -------\n    lower, upper, ci: tuple\n        confidence intervals and alpha-level\n\n    Note\n    ----\n    Implementation of section 4.4.2 (Fisher,1993)\n    \"\"\"\n\n    if median is None:\n        if alpha is None:\n            raise ValueError(\"If `median` is None, then `alpha` is needed.\")\n        if w is None:\n            w = np.ones_like(alpha)\n        median = float(circ_median(alpha=alpha, w=w, method=method, return_average=True))\n\n    if alpha is None:\n        raise ValueError(\n            \"`alpha` is needed for computing the confidence interval for circular median.\"\n        )\n\n    n = len(alpha)\n    alpha = np.sort(alpha)\n\n    if n &gt; 15:\n        z = norm.ppf(1 - 0.5 * (1 - ci))\n\n        offset = int(1 + np.floor(0.5 * np.sqrt(n) * z))  # fisher:eq(4.19)\n\n        # idx_median = np.where(alpha.round(5) &lt; np.round(median, 5))[0][-1]\n        arr = np.where(alpha.round(5) &lt; np.round(median, 5))[0]\n        if len(arr) == 0:\n            # That means median is smaller than alpha[0] (to 5 decimals).\n            # In a circular sense, the \u201cclosest index below\u201d is alpha[-1].\n            idx_median = len(alpha) - 1\n        else:\n            idx_median = arr[-1]\n\n        idx_lb = idx_median - offset + 1\n        idx_ub = idx_median + offset\n        if np.round(median, 5) in alpha.round(5):  # don't count the median per se\n            idx_ub += 1\n\n        if idx_ub &gt; n:\n            idx_ub = idx_ub - n\n\n        if idx_lb &lt; 0:\n            idx_lb = n + idx_lb\n\n        lower, upper = alpha[int(idx_lb)], alpha[int(idx_ub)]\n\n        if not is_within_circular_range(median, lower, upper):\n            lower, upper = upper, lower\n\n    # selected confidence intervals for the median direction for n &lt; 15\n    # from A6, Fisher, 1993.\n    # We only return the widest CI if there are more than one in the table.\n\n    elif n == 3:\n        lower, upper = alpha[0], alpha[2]\n        ci = 0.75\n    elif n == 4:\n        lower, upper = alpha[0], alpha[3]\n        ci = 0.875\n    elif n == 5:\n        lower, upper = alpha[0], alpha[4]\n        ci = 0.937\n    elif n == 6:\n        lower, upper = alpha[0], alpha[5]\n        ci = 0.97\n    elif n == 7:\n        lower, upper = alpha[0], alpha[6]\n        ci = 0.984\n    elif n == 8:\n        lower, upper = alpha[0], alpha[7]\n        ci = 0.992\n    elif n == 9:\n        lower, upper = alpha[0], alpha[8]\n        ci = 0.996\n    elif n == 10:\n        lower, upper = alpha[1], alpha[8]\n        ci = 0.978\n    elif n == 11:\n        lower, upper = alpha[1], alpha[9]\n        ci = 0.99\n    elif n == 12:\n        lower, upper = alpha[2], alpha[9]\n        ci = 0.962\n    elif n == 13:\n        lower, upper = alpha[2], alpha[10]\n        ci = 0.978\n    elif n == 14:\n        lower, upper = alpha[3], alpha[10]\n        ci = 0.937\n    elif n == 15:\n        lower, upper = alpha[2], alpha[12]\n        ci = 0.965\n    else:\n        lower, upper = np.nan, np.nan\n\n    return (angmod(lower), angmod(upper), ci)\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_kappa","title":"<code>circ_kappa(r, n=None)</code>","text":"<p>Estimate kappa by approximation.</p> \\[ \\hat\\kappa_{ML} = \\begin{cases}  2r + r^3 + 5r^5/6, , &amp; \\text{if } r &lt; 0.53  \\\\  -0.4 + 1.39 r + 0.43 / (1 - r) , &amp; \\text{if } 0.53 \\le r &lt; 0.85\\\\     1 / (r^3 - 4r^2 + 3r), &amp; \\text{if } r \\ge 0.85 \\end{cases} \\] <p>For \\(n \\le 15\\):</p> \\[ \\hat\\kappa = \\begin{cases}     \\max\\left(\\hat\\kappa - \\frac{2}{n\\hat\\kappa}, 0\\right), &amp; \\text{if } \\hat\\kappa &lt; 2 \\\\     \\frac{(n - 1)^3 \\hat\\kappa}{n^3 + n}, &amp; \\text{if } \\hat\\kappa \\ge 2 \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>r</code> <code>float</code> <p>Resultant vector length</p> required <code>n</code> <code>Union[int, None]</code> <p>Sample size. If n is not None, the adjustment for small sample size will be applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>kappa</code> <code>float</code> <p>Concentration parameter</p> Reference <p>Section 4.5.5 (P88, Fisher, 1993)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_kappa(r: float, n: Union[int, None] = None) -&gt; float:\n    r\"\"\"Estimate kappa by approximation.\n\n    $$\n    \\hat\\kappa_{ML} =\n    \\begin{cases}\n     2r + r^3 + 5r^5/6, , &amp; \\text{if } r &lt; 0.53  \\\\\n     -0.4 + 1.39 r + 0.43 / (1 - r) , &amp; \\text{if } 0.53 \\le r &lt; 0.85\\\\\n        1 / (r^3 - 4r^2 + 3r), &amp; \\text{if } r \\ge 0.85\n    \\end{cases}\n    $$\n\n    For $n \\le 15$:\n\n    $$\n    \\hat\\kappa =\n    \\begin{cases}\n        \\max\\left(\\hat\\kappa - \\frac{2}{n\\hat\\kappa}, 0\\right), &amp; \\text{if } \\hat\\kappa &lt; 2 \\\\\n        \\frac{(n - 1)^3 \\hat\\kappa}{n^3 + n}, &amp; \\text{if } \\hat\\kappa \\ge 2\n    \\end{cases}\n    $$\n\n\n    Parameters\n    ----------\n    r: float\n        Resultant vector length\n    n: int or None\n        Sample size. If n is not None, the adjustment for small sample size will be applied.\n\n    Returns\n    -------\n    kappa: float\n        Concentration parameter\n\n    Reference\n    ---------\n    Section 4.5.5 (P88, Fisher, 1993)\n    \"\"\"\n\n    # eq 4.40\n    if r &lt; 0.53:\n        kappa = 2 * r + r**3 + 5 * r**5 / 6\n    elif r &lt; 0.85:\n        kappa = -0.4 + 1.39 * r + 0.43 / (1 - r)\n    else:\n        nom = r**3 - 4 * r**2 + 3 * r\n        if nom != 0:\n            kappa = 1 / nom\n        else:\n            # not sure how to handle this...\n            kappa = 1e-16\n\n    # eq 4.41\n    if n is not None:\n        if n &lt;= 15 and r &lt; 0.7:\n            if kappa &lt; 2:\n                kappa = np.max([kappa - 2 * 1 / (n * kappa), 0])\n            else:\n                kappa = (n - 1) ** 3 * kappa / (n**3 + n)\n\n    return kappa\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_dist","title":"<code>circ_dist(x, y=None, metric='center', return_sum=False)</code>","text":"<p>Compute the element-wise circular distance between two arrays of angles.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>First sample of circular data (radians).</p> required <code>y</code> <code>array - like</code> <p>Second sample of circular data (radians). If None, computes element-wise distances within <code>x</code> itself.</p> <code>None</code> <code>metric</code> <code>str</code> <p>Distance metric to use, options: - \"center\" (default): Standard circular difference wrapped to [-\u03c0, \u03c0]. - \"geodesic\": \u03c0 - |\u03c0 - |x - y||. - \"angularseparation\": 1 - cos(x - y). - \"chord\": sqrt(2 * (1 - cos(x - y))).</p> <code>'center'</code> <code>return_sum</code> <code>bool</code> <p>If True, returns the sum of all computed distances (like R's <code>dist.circular()</code>).</p> <code>False</code> <p>Returns:</p> Type Description <code>array</code> <p>Element-wise distance values based on the chosen metric.</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_dist(\n    x: Union[np.ndarray, float],\n    y: Optional[Union[np.ndarray, float]] = None,\n    metric: str = \"center\",\n    return_sum: bool = False,\n) -&gt; Union[np.ndarray, float]:\n    r\"\"\"\n    Compute the element-wise circular distance between two arrays of angles.\n\n    Parameters\n    ----------\n    x : array-like\n        First sample of circular data (radians).\n    y : array-like, optional\n        Second sample of circular data (radians). If None, computes element-wise\n        distances within `x` itself.\n    metric : str, optional\n        Distance metric to use, options:\n        - \"center\" (default): Standard circular difference wrapped to [-\u03c0, \u03c0].\n        - \"geodesic\": \u03c0 - |\u03c0 - |x - y||.\n        - \"angularseparation\": 1 - cos(x - y).\n        - \"chord\": sqrt(2 * (1 - cos(x - y))).\n    return_sum : bool, optional\n        If True, returns the sum of all computed distances (like R's `dist.circular()`).\n\n    Returns\n    -------\n    array\n        Element-wise distance values based on the chosen metric.\n    \"\"\"\n    x = np.asarray(x)\n\n    if y is None:\n        y = x\n\n    y = np.asarray(y)\n\n    # Ensure broadcasting works without explicit shape checks\n    try:\n        np.broadcast_shapes(x.shape, y.shape)\n    except ValueError:\n        raise ValueError(\n            f\"Shapes {x.shape} and {y.shape} are incompatible for broadcasting.\"\n        )\n\n    if metric == \"center\":\n        distances = np.angle(np.exp(1j * x) / np.exp(1j * y))\n\n    elif metric == \"geodesic\":\n        distances = np.pi - np.abs(np.pi - np.abs(x - y))\n\n    elif metric == \"angularseparation\":\n        distances = 1 - np.cos(x - y)\n\n    elif metric == \"chord\":\n        distances = np.sqrt(2 * (1 - np.cos(x - y)))\n\n    else:\n        raise ValueError(f\"Unknown metric: {metric}\")\n\n    return np.sum(distances).astype(float) if return_sum else distances\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_pairdist","title":"<code>circ_pairdist(x, y=None, metric='center', return_sum=False)</code>","text":"<p>Compute the pairwise circular distance between all elements in <code>x</code> and <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>First sample of circular data (radians).</p> required <code>y</code> <code>array - like</code> <p>Second sample of circular data (radians). If None, computes pairwise distances within <code>x</code> itself.</p> <code>None</code> <code>metric</code> <code>str</code> <p>Distance metric to use (same options as <code>circ_dist</code>).</p> <code>'center'</code> <code>return_sum</code> <code>bool</code> <p>If True, returns the sum of all computed distances (like R's <code>dist.circular()</code>).</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Pairwise distance matrix where entry (i, j) is the circular distance between x[i] and y[j] based on the chosen metric.</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_pairdist(\n    x: np.ndarray,\n    y: Optional[np.ndarray] = None,\n    metric: str = \"center\",\n    return_sum: bool = False,\n) -&gt; Union[np.ndarray, float]:\n    r\"\"\"\n    Compute the pairwise circular distance between all elements in `x` and `y`.\n\n    Parameters\n    ----------\n    x : array-like\n        First sample of circular data (radians).\n    y : array-like, optional\n        Second sample of circular data (radians). If None, computes pairwise\n        distances within `x` itself.\n    metric : str, optional\n        Distance metric to use (same options as `circ_dist`).\n    return_sum : bool, optional\n        If True, returns the sum of all computed distances (like R's `dist.circular()`).\n\n    Returns\n    -------\n    ndarray\n        Pairwise distance matrix where entry (i, j) is the circular distance\n        between x[i] and y[j] based on the chosen metric.\n    \"\"\"\n    x = np.asarray(x)\n\n    # If y is not provided, compute pairwise distances within x\n    if y is None:\n        y = x\n\n    y = np.asarray(y)\n\n    # Reshape to allow broadcasting for pairwise computation\n    x_reshaped = x[:, None]  # Shape (n, 1)\n    y_reshaped = y[None, :]  # Shape (1, m)\n\n    return circ_dist(x_reshaped, y_reshaped, metric=metric, return_sum=return_sum)\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.convert_moment","title":"<code>convert_moment(mp)</code>","text":"<p>Convert complex moment to polar coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>mp</code> <code>complex</code> <p>Complex moment</p> required <p>Returns:</p> Name Type Description <code>u</code> <code>float</code> <p>Angle in radian</p> <code>r</code> <code>float</code> <p>Magnitude</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def convert_moment(\n    mp: complex,\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Convert complex moment to polar coordinates.\n\n    Parameters\n    ----------\n    mp: complex\n        Complex moment\n\n    Returns\n    -------\n    u: float\n        Angle in radian\n    r: float\n        Magnitude\n\n    \"\"\"\n\n    u = float(angmod(float(np.angle(mp))))\n    r = np.abs(mp)\n\n    return u, r\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.compute_C_and_S","title":"<code>compute_C_and_S(alpha, w, p=1, mean=0.0)</code>","text":"<p>Compute the intermediate values Cbar and Sbar.</p> \\[ \\displaylines{ \\bar{C}_{p} = \\frac{\\sum_{i=1}^{n} w_{i} \\cos(p(\\alpha_{i} - \\mu))}{n} \\\\ \\bar{S}_{p} = \\frac{\\sum_{i=1}^{n} w_{i} \\sin(p(\\alpha_{i} - \\mu))}{n} } \\] <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>ndarray</code> <p>Frequencies or weights.</p> required <code>p</code> <code>int</code> <p>Order of the moment (default is 1, for the first moment).</p> <code>1</code> <code>mean</code> <code>Union[float, ndarray]</code> <p>Mean angle (\u03bc) to center the computation (default is 0.0).</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>Cbar</code> <code>float</code> <p>Weighted mean cosine for the given moment.</p> <code>Sbar</code> <code>float</code> <p>Weighted mean sine for the given moment.</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def compute_C_and_S(\n    alpha: np.ndarray,\n    w: np.ndarray,\n    p: int = 1,\n    mean: Union[float, np.ndarray] = 0.0,\n) -&gt; Tuple[float, float]:\n    r\"\"\"\n    Compute the intermediate values Cbar and Sbar.\n\n    $$\n    \\displaylines{\n    \\bar{C}_{p} = \\frac{\\sum_{i=1}^{n} w_{i} \\cos(p(\\alpha_{i} - \\mu))}{n} \\\\\n    \\bar{S}_{p} = \\frac{\\sum_{i=1}^{n} w_{i} \\sin(p(\\alpha_{i} - \\mu))}{n}\n    }\n    $$\n\n    Parameters\n    ----------\n    alpha: np.ndarray\n        Angles in radian.\n    w: np.ndarray\n        Frequencies or weights.\n    p: int, optional\n        Order of the moment (default is 1, for the first moment).\n    mean: float, optional\n        Mean angle (\u03bc) to center the computation (default is 0.0).\n\n    Returns\n    -------\n    Cbar: float\n        Weighted mean cosine for the given moment.\n    Sbar: float\n        Weighted mean sine for the given moment.\n    \"\"\"\n    n = np.sum(w)\n    Cbar = np.sum(w * np.cos(p * (alpha - mean))) / n\n    Sbar = np.sum(w * np.sin(p * (alpha - mean))) / n\n\n    return Cbar, Sbar\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.compute_hdi","title":"<code>compute_hdi(samples, ci=0.95)</code>","text":"<p>Compute the Highest Density Interval (HDI) for circular data.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>ndarray</code> <p>Bootstrap samples of the circular mean in radians.</p> required <code>ci</code> <code>float</code> <p>Credible interval (default is 0.95 for 95% HDI).</p> <code>0.95</code> <p>Returns:</p> Name Type Description <code>hdi</code> <code>tuple</code> <p>Lower and upper bounds of the HDI in radians.</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def compute_hdi(samples: np.ndarray, ci: float = 0.95) -&gt; tuple[float, float]:\n    \"\"\"\n    Compute the Highest Density Interval (HDI) for circular data.\n\n    Parameters\n    ----------\n    samples : np.ndarray\n        Bootstrap samples of the circular mean in radians.\n    ci : float, optional\n        Credible interval (default is 0.95 for 95% HDI).\n\n    Returns\n    -------\n    hdi : tuple\n        Lower and upper bounds of the HDI in radians.\n    \"\"\"\n    if not (0 &lt; ci &lt; 1):\n        raise ValueError(\"`ci` must be between 0 and 1 (exclusive).\")\n\n    wrapped_samples = angmod(samples)\n    sorted_samples = np.sort(wrapped_samples)\n    n_samples = sorted_samples.size\n\n    if n_samples == 0:\n        raise ValueError(\"Insufficient data to compute HDI.\")\n\n    window_size = max(1, int(np.floor(ci * n_samples)))\n    window_size = min(window_size, n_samples)\n\n    extended_samples = np.concatenate((sorted_samples, sorted_samples + 2 * np.pi))\n\n    best_width = np.inf\n    best_lower = float(sorted_samples[0])\n    best_upper = float(sorted_samples[0])\n\n    for start in range(n_samples):\n        stop = start + window_size - 1\n        upper = float(extended_samples[stop])\n        lower = float(sorted_samples[start])\n        width = upper - lower\n        if width &lt; best_width:\n            best_width = width\n            best_lower, best_upper = lower, upper\n\n    return float(angmod(best_lower)), float(angmod(best_upper))\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.compute_smooth_params","title":"<code>compute_smooth_params(r, n)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>r</code> <code>float</code> <p>resultant vector length</p> required <code>n</code> <code>int</code> <p>sample size</p> required <p>Returns:</p> Name Type Description <code>h</code> <code>float</code> <p>smoothing parameter</p> Reference <p>Section 2.2 (P26, Fisher, 1993)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def compute_smooth_params(r: float, n: int) -&gt; float:\n    \"\"\"\n    Parameters\n    ----------\n    r: float\n        resultant vector length\n    n: int\n        sample size\n\n    Returns\n    -------\n    h: float\n        smoothing parameter\n\n    Reference\n    ---------\n    Section 2.2 (P26, Fisher, 1993)\n    \"\"\"\n\n    kappa = circ_kappa(r, n)\n    zeta = 1 / np.sqrt(kappa)  # eq 2.3\n    h = np.sqrt(7) * zeta / np.power(n, 0.2)  # eq 2.4\n\n    return h\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.nonparametric_density_estimation","title":"<code>nonparametric_density_estimation(alpha, h, radius=1)</code>","text":"<p>Nonparametric density estimates with a quartic kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian</p> required <code>h</code> <code>float</code> <p>Smoothing parameters</p> required <code>radius</code> <code>float</code> <p>radius of the plotted circle</p> <code>1</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray(100)</code> <p>grid</p> <code>f</code> <code>ndarray(100)</code> <p>density</p> Reference <p>Section 2.2 (P26, Fisher, 1993)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def nonparametric_density_estimation(\n    alpha: np.ndarray,  # angles in radian\n    h: float,  # smoothing parameters\n    radius: float = 1,  # radius of the plotted circle\n) -&gt; tuple:\n    \"\"\"Nonparametric density estimates with\n    a quartic kernel function.\n\n    Parameters\n    ----------\n    alpha: np.ndarray (n, )\n        Angles in radian\n    h: float\n        Smoothing parameters\n    radius: float\n        radius of the plotted circle\n\n    Returns\n    -------\n    x: np.ndarray (100, )\n        grid\n    f: np.ndarray (100, )\n        density\n\n    Reference\n    ---------\n    Section 2.2 (P26, Fisher, 1993)\n    \"\"\"\n\n    # vectorized version of step 3\n    a = np.asarray(alpha, dtype=float)\n    n = len(a)\n    x = np.linspace(0, 2 * np.pi, 100)\n    d = np.abs(x[:, None] - a)\n    e = np.minimum(d, 2 * np.pi - d)\n    e = np.minimum(e, h)\n    weight_sum = np.sum((1 - e**2 / h**2) ** 2, axis=1)\n    f = 0.9375 * weight_sum / (n * h)\n\n    f = radius * np.sqrt(1 + np.pi * f) - radius\n\n    return x, f\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_range","title":"<code>circ_range(alpha)</code>","text":"<p>Compute the circular range of angular data.</p> <p>The circular range is the difference between the maximum and minimum angles in the dataset, adjusted for circular continuity.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radians.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Circular range, a measure of clustering (higher = more clustered).</p> Reference <p>P162, Section 7.2.3 of Jammalamadaka, S. Rao and SenGupta, A. (2001)</p> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_range(alpha: np.ndarray) -&gt; np.float64:\n    \"\"\"\n    Compute the circular range of angular data.\n\n    The circular range is the difference between the maximum and minimum angles\n    in the dataset, adjusted for circular continuity.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Angles in radians.\n\n    Returns\n    -------\n    float\n        Circular range, a measure of clustering (higher = more clustered).\n\n    Reference\n    ---------\n    P162, Section 7.2.3 of Jammalamadaka, S. Rao and SenGupta, A. (2001)\n    \"\"\"\n    alpha = np.sort(alpha % (2 * np.pi))  # Convert to [0, 2\u03c0) and sort\n    spacings = np.diff(alpha, prepend=alpha[-1] - 2 * np.pi)  # Compute spacings\n    return 2 * np.pi - np.max(spacings)  # Circular range\n</code></pre>"},{"location":"reference/descriptive/#pycircstat2.descriptive.circ_quantile","title":"<code>circ_quantile(alpha, probs=np.array([0, 0.25, 0.5, 0.75, 1.0]), type=7)</code>","text":"<p>Compute quantiles for circular data.</p> <p>This function computes quantiles for circular data by shifting the data to be centered around the circular median, applying a linear quantile function, and then shifting back.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of circular data (radians).</p> required <code>probs</code> <code>float or ndarray</code> <p>Probabilities at which to compute quantiles. Default is <code>[0, 0.25, 0.5, 0.75, 1.0]</code>.</p> <code>array([0, 0.25, 0.5, 0.75, 1.0])</code> <code>type</code> <code>int</code> <p>Quantile algorithm type (default <code>7</code>, matches R\u2019s default quantile type).</p> <code>7</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Circular quantiles.</p> References <ul> <li>R's <code>quantile.circular</code> from the <code>circular</code> package.</li> <li>Fisher (1993), Section 2.3.2.</li> </ul> Source code in <code>pycircstat2/descriptive.py</code> <pre><code>def circ_quantile(\n    alpha: np.ndarray,\n    probs: Union[float, np.ndarray] = np.array([0, 0.25, 0.5, 0.75, 1.0]),\n    type: int = 7,\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute quantiles for circular data.\n\n    This function computes quantiles for circular data by shifting the\n    data to be centered around the circular median, applying a linear quantile function,\n    and then shifting back.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Sample of circular data (radians).\n    probs : float or np.ndarray, optional\n        Probabilities at which to compute quantiles. Default is `[0, 0.25, 0.5, 0.75, 1.0]`.\n    type : int, optional\n        Quantile algorithm type (default `7`, matches R\u2019s default quantile type).\n\n    Returns\n    -------\n    np.ndarray\n        Circular quantiles.\n\n    References\n    ----------\n    - R's `quantile.circular` from the `circular` package.\n    - Fisher (1993), Section 2.3.2.\n    \"\"\"\n\n    # Convert to numpy array\n    alpha = np.asarray(alpha)\n    probs = np.atleast_1d(probs)\n\n    # Compute circular median\n    circular_median = circ_median(alpha)\n\n    # If the median is NaN (e.g., uniform data), return NaNs\n    if np.isnan(circular_median):\n        return np.full_like(probs, np.nan)\n\n    # Transform data relative to circular median\n    shifted_alpha = (alpha - circular_median) % (2 * np.pi)\n    shifted_alpha = np.where(\n        shifted_alpha &gt; np.pi, shifted_alpha - 2 * np.pi, shifted_alpha\n    )\n\n    # Compute linear quantiles on transformed data\n    linear_quantiles = np.quantile(\n        shifted_alpha, probs, method=\"linear\" if type == 7 else \"midpoint\"\n    )\n\n    # Transform back to original circular space\n    circular_quantiles = (linear_quantiles + circular_median) % (2 * np.pi)\n\n    return circular_quantiles\n</code></pre>"},{"location":"reference/distributions/","title":"Distributions","text":""},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous","title":"<code>CircularContinuous</code>","text":"<p>               Bases: <code>rv_continuous</code></p> <p>Base class for circular distributions with fixed loc=0 and scale=1.</p> Notes <ul> <li><code>loc</code>/<code>scale</code> are intentionally removed from the user-facing API; the   named parameters of each distribution already encode location/shape.</li> <li>Circular data are assumed to be pre-wrapped to <code>[0, 2\u03c0)</code>; for   convenience every public method wraps angular inputs to that support.</li> <li>Cumulative methods remain circular/periodic under this wrapping   (<code>cdf(\u03b8 + 2\u03c0) == cdf(\u03b8)</code>).</li> </ul> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class CircularContinuous(rv_continuous):\n    \"\"\"\n    Base class for circular distributions with fixed loc=0 and scale=1.\n\n    Notes\n    -----\n    - ``loc``/``scale`` are intentionally removed from the user-facing API; the\n      named parameters of each distribution already encode location/shape.\n    - Circular data are assumed to be pre-wrapped to ``[0, 2\u03c0)``; for\n      convenience every public method wraps angular inputs to that support.\n    - Cumulative methods remain circular/periodic under this wrapping\n      (``cdf(\u03b8 + 2\u03c0) == cdf(\u03b8)``).\n    \"\"\"\n\n    _loc_default = 0.0\n    _scale_default = 1.0\n\n    def __init__(\n        self,\n        momtype=1,\n        a=None,\n        b=None,\n        *,\n        support=None,\n        xtol=1e-14,\n        badvalue=None,\n        name=None,\n        longname=None,\n        shapes=None,\n        seed=None,\n    ):\n        if support is not None:\n            support_a, support_b = support\n            if a is None:\n                a = support_a\n            if b is None:\n                b = support_b\n        if a is None:\n            a = 0.0\n        if b is None:\n            b = 2 * np.pi\n\n        super().__init__(\n            momtype=momtype,\n            a=a,\n            b=b,\n            xtol=xtol,\n            badvalue=badvalue,\n            name=name,\n            longname=longname,\n            shapes=shapes,\n            seed=seed,\n        )\n\n        self._circular_arg_wrapped = False\n        self._wrap_arg_parsers()\n        self._lower_bound, self._period = self._compute_period()\n        self._normalization_cache = {}\n\n    # ------------------------------------------------------------------\n    # Internal helpers\n    # ------------------------------------------------------------------\n    def _get_normalization_cache(self):\n        cache = getattr(self, \"_normalization_cache\", None)\n        if cache is None:\n            cache = {}\n            self._normalization_cache = cache\n        return cache\n\n    def _clear_normalization_cache(self):\n        self._normalization_cache = {}\n\n    def _wrap_arg_parsers(self):\n        \"\"\"Ensure internal arg-parsing keeps loc/scale fixed to defaults.\"\"\"\n        if getattr(self, \"_circular_arg_wrapped\", False):\n            return\n\n        for attr in (\"_parse_args\", \"_parse_args_rvs\", \"_parse_args_stats\"):\n            original = getattr(self, attr)\n\n            def wrapper(this, *args, __orig=original, __name=attr, **kwargs):\n                clean_kwargs = this._clean_loc_scale_kwargs(kwargs, caller=__name)\n                return __orig(*args, **clean_kwargs)\n\n            setattr(self, attr, types.MethodType(wrapper, self))\n\n        self._circular_arg_wrapped = True\n\n    def _compute_period(self):\n        try:\n            lower = float(self.a)\n            upper = float(self.b)\n        except (TypeError, ValueError):\n            return None, None\n\n        period = upper - lower\n        if not np.isfinite(period) or period &lt;= 0:\n            return None, None\n        return lower, period\n\n    def _wrap_angles(self, values):\n        if self._period is None or self._lower_bound is None:\n            return values\n\n        try:\n            arr = np.asarray(values, dtype=float)\n        except (TypeError, ValueError):\n            return values\n\n        if arr.size == 0:\n            return arr\n\n        wrapped = np.mod(arr - self._lower_bound, self._period) + self._lower_bound\n        upper_bound = self._lower_bound + self._period\n        if np.isfinite(upper_bound):\n            tol = np.finfo(float).eps * max(1.0, abs(upper_bound))\n            if np.isscalar(values):\n                if np.isclose(values, upper_bound, rtol=0.0, atol=tol):\n                    return upper_bound\n            else:\n                mask = np.isclose(arr, upper_bound, rtol=0.0, atol=tol)\n                if np.any(mask):\n                    wrapped = wrapped.copy()\n                    wrapped[mask] = upper_bound\n        if np.isscalar(values):\n            return float(wrapped)\n        return wrapped\n\n    def _init_rng(self, random_state):\n        \"\"\"\n        Normalize the ``random_state`` argument to a NumPy ``Generator``.\n\n        Accepts integers, ``RandomState`` instances, ``Generator`` objects, or\n        ``None`` (in which case the distribution's cached generator is used).\n        \"\"\"\n        candidate = random_state if random_state is not None else getattr(self, \"_random_state\", None)\n\n        if isinstance(candidate, np.random.Generator):\n            return candidate\n\n        if isinstance(candidate, np.random.RandomState):\n            seed = candidate.randint(0, 2**32)\n            generator = np.random.default_rng(seed)\n            if random_state is None:\n                self._random_state = generator\n            return generator\n\n        if candidate is None:\n            generator = np.random.default_rng()\n            self._random_state = generator\n            return generator\n\n        try:\n            generator = np.random.default_rng(candidate)\n        except TypeError as err:  # pragma: no cover - defensive branch\n            raise TypeError(\n                \"random_state must be None, an int seed, RandomState, or Generator.\"\n            ) from err\n\n        if random_state is None:\n            self._random_state = generator\n\n        return generator\n\n    def _prepare_call_kwargs(self, kwargs, caller):\n        if not kwargs:\n            return {}\n        return self._clean_loc_scale_kwargs(dict(kwargs), caller=caller)\n\n    def _separate_shape_parameters(self, args, kwargs, caller):\n        \"\"\"\n        Split positional/keyword shape parameters from kwargs for functions that\n        delegate to SciPy helpers lacking keyword support (e.g. ``expect``).\n        \"\"\"\n        if not kwargs:\n            return tuple(args), {}\n\n        remaining_kwargs = dict(kwargs)\n        shape_args = list(args)\n\n        shapespec = getattr(self, \"shapes\", None)\n        if shapespec:\n            shape_names = [name.strip() for name in shapespec.split(\",\") if name.strip()]\n            for idx, name in enumerate(shape_names):\n                if name not in remaining_kwargs:\n                    continue\n                value = remaining_kwargs.pop(name)\n                if idx &lt; len(shape_args):\n                    existing = shape_args[idx]\n                    try:\n                        equal = np.allclose(existing, value)\n                    except Exception:\n                        equal = existing == value\n                    if not equal:\n                        raise TypeError(\n                            f\"{self._dist_name(caller)} received conflicting values for `{name}`.\"\n                        )\n                else:\n                    shape_args.append(value)\n\n        return tuple(shape_args), remaining_kwargs\n\n    def _clean_loc_scale_kwargs(self, kwargs, *, caller):\n        if not kwargs:\n            return kwargs\n\n        cleaned = kwargs\n        mutated = False\n\n        if \"loc\" in kwargs:\n            loc_val = kwargs[\"loc\"]\n            if not self._is_default_value(loc_val, self._loc_default):\n                raise TypeError(\n                    f\"{self._dist_name(caller)} does not support a free `loc` parameter.\"\n                )\n            cleaned = dict(cleaned) if not mutated else cleaned\n            cleaned.pop(\"loc\", None)\n            mutated = True\n\n        if \"scale\" in kwargs:\n            scale_val = kwargs[\"scale\"]\n            if not self._is_default_value(scale_val, self._scale_default):\n                raise TypeError(\n                    f\"{self._dist_name(caller)} does not support a free `scale` parameter.\"\n                )\n            if not mutated:\n                cleaned = dict(cleaned)\n                mutated = True\n            cleaned.pop(\"scale\", None)\n            mutated = True\n\n        forbidden_aliases = (\"floc\", \"fscale\", \"fix_loc\", \"fix_scale\")\n        for alias in forbidden_aliases:\n            if alias in kwargs:\n                raise TypeError(\n                    f\"{self._dist_name(caller)} does not support `{alias}`; the distribution fixes location/scale.\"\n                )\n\n        return cleaned if mutated else kwargs\n\n    def _is_default_value(self, value, default):\n        try:\n            arr = np.asarray(value)\n        except Exception:  # pragma: no cover - defensive\n            return False\n        if arr.size == 0:\n            return True\n        try:\n            return np.allclose(arr, default)\n        except TypeError:  # pragma: no cover - fallback if casting fails\n            return False\n\n    def _dist_name(self, caller: str) -&gt; str:\n        dist_name = getattr(self, \"name\", None)\n        if dist_name:\n            return f\"{dist_name}.{caller}\"\n        return f\"{self.__class__.__name__}.{caller}\"\n\n    def _normalization_cache_key(self, *params):\n        key_components = []\n        for param in params:\n            try:\n                arr = np.asarray(param, dtype=float)\n            except (TypeError, ValueError):\n                return None\n            if arr.ndim &gt; 1 or arr.size &gt; 1:\n                return None\n            try:\n                scalar = arr.item() if isinstance(arr, np.ndarray) else float(arr)\n            except (TypeError, ValueError):\n                try:\n                    scalar = float(arr)\n                except (TypeError, ValueError):\n                    return None\n            key_components.append(float(scalar))\n        return tuple(key_components)\n\n    def _get_cached_normalizer(self, compute, *params):\n        key = self._normalization_cache_key(*params)\n        if key is None:\n            return compute()\n        cache = self._get_normalization_cache()\n        if key not in cache:\n            cache[key] = compute()\n        return cache[key]\n\n    def freeze(self, *args, **kwds) -&gt; \"CircularContinuousFrozen\":\n        \"\"\"\n        Return a frozen circular distribution while enforcing fixed loc/scale.\n        \"\"\"\n        call_kwargs = self._prepare_call_kwargs(kwds, \"freeze\")\n        return CircularContinuousFrozen(self, *args, **call_kwargs)\n\n    __call__ = freeze\n\n    # ------------------------------------------------------------------\n    # Public overrides\n    # ------------------------------------------------------------------\n    def pdf(self, x, *args, **kwargs):\n        call_kwargs = self._prepare_call_kwargs(kwargs, \"pdf\")\n        return super().pdf(self._wrap_angles(x), *args, **call_kwargs)\n\n    def logpdf(self, x, *args, **kwargs):\n        call_kwargs = self._prepare_call_kwargs(kwargs, \"logpdf\")\n        return super().logpdf(self._wrap_angles(x), *args, **call_kwargs)\n\n    def cdf(self, x, *args, **kwargs):\n        call_kwargs = self._prepare_call_kwargs(kwargs, \"cdf\")\n        return super().cdf(self._wrap_angles(x), *args, **call_kwargs)\n\n    def logcdf(self, x, *args, **kwargs):\n        call_kwargs = self._prepare_call_kwargs(kwargs, \"logcdf\")\n        return super().logcdf(self._wrap_angles(x), *args, **call_kwargs)\n\n    def sf(self, x, *args, **kwargs):\n        call_kwargs = self._prepare_call_kwargs(kwargs, \"sf\")\n        return super().sf(self._wrap_angles(x), *args, **call_kwargs)\n\n    def logsf(self, x, *args, **kwargs):\n        call_kwargs = self._prepare_call_kwargs(kwargs, \"logsf\")\n        return super().logsf(self._wrap_angles(x), *args, **call_kwargs)\n\n    def nnlf(self, theta, x):\n        return super().nnlf(theta, self._wrap_angles(x))\n\n    def fit(self, data, *args, **kwds):\n        kwds = self._sanitize_fit_kwargs(kwds)\n        wrapped_data = self._wrap_angles(data)\n        return super().fit(wrapped_data, *args, **kwds)\n\n    def fit_loc_scale(self, *args, **kwargs):  # pragma: no cover - API guard\n        raise NotImplementedError(\n            \"Circular distributions have fixed location and scale; use `fit` for shape parameters only.\"\n        )\n\n    def _sanitize_fit_kwargs(self, kwds):\n        if not kwds:\n            kwds = {}\n        else:\n            kwds = dict(kwds)\n\n        # Reject attempts to seed loc/scale with non-default values.\n        for key, default in ((\"loc\", self._loc_default), (\"scale\", self._scale_default)):\n            if key in kwds:\n                if not self._is_default_value(kwds[key], default):\n                    raise TypeError(\n                        f\"{self._dist_name('fit')} fixes `{key}` to {default}; remove the argument.\"\n                    )\n                kwds.pop(key)\n\n        for key in (\"fix_loc\", \"fix_scale\"):\n            if key in kwds:\n                raise TypeError(\n                    f\"{self._dist_name('fit')} does not expose `{key}`; the distribution is already fixed.\"\n                )\n\n        for key, default in ((\"floc\", self._loc_default), (\"fscale\", self._scale_default)):\n            if key in kwds:\n                if not self._is_default_value(kwds[key], default):\n                    raise TypeError(\n                        f\"{self._dist_name('fit')} requires `{key}` == {default}.\"\n                    )\n                kwds.pop(key)\n\n        kwds[\"floc\"] = self._loc_default\n        kwds[\"fscale\"] = self._scale_default\n        return kwds\n\n    def _attach_methods(self):  # pragma: no cover - mirrors parent for pickling\n        super()._attach_methods()\n        # Reapply wrappers; _attach_methods is used during unpickling.\n        self._circular_arg_wrapped = False\n        self._wrap_arg_parsers()\n\n    def _wrap_direction(self, angle: float) -&gt; float:\n        \"\"\"\n        Wrap a direction onto the distribution's support if known, otherwise [0, 2\u03c0).\n        \"\"\"\n        if self._lower_bound is not None and self._period is not None:\n            return float(self._wrap_angles(angle))\n        return float(angmod(angle))\n\n    # ------------------------------------------------------------------\n    # Numeric integration helpers\n    # ------------------------------------------------------------------\n    def _cdf_integral(\n        self,\n        x,\n        integrand,\n        params,\n        *,\n        lower=None,\n        upper=None,\n        epsabs=1e-9,\n        epsrel=1e-9,\n        limit=200,\n    ):\n        \"\"\"\n        Numerically integrate a one-dimensional PDF to obtain CDF values.\n\n        Evaluates the cumulative integral of ``integrand`` from ``lower`` to each\n        point in ``x``, reusing work across sorted evaluation points to minimise\n        the number of quadrature calls.\n        \"\"\"\n        if np.isscalar(x):\n            x_vals = np.array([float(x)], dtype=float)\n            scalar_input = True\n        else:\n            x_arr = np.asarray(x, dtype=float)\n            x_vals = x_arr.ravel()\n            scalar_input = False\n            original_shape = x_arr.shape\n\n        if x_vals.size == 0:\n            if scalar_input:\n                return float()\n            return np.empty(original_shape, dtype=float)\n\n        params = tuple(params)\n        lower_bound = float(self.a if lower is None else lower)\n        upper_bound = float(self.b if upper is None else upper)\n\n        def scalar_integrand(value, *args):\n            out = integrand(value, *args)\n            arr = np.asarray(out, dtype=float)\n            if arr.ndim == 0:\n                return float(arr)\n            return float(arr.reshape(-1)[0])\n\n        results = np.zeros_like(x_vals, dtype=float)\n        sorted_indices = np.argsort(x_vals, kind=\"mergesort\")\n        sorted_vals = x_vals[sorted_indices]\n\n        cumulative = 0.0\n        current = lower_bound\n\n        for order_idx, orig_idx in enumerate(sorted_indices):\n            value = float(sorted_vals[order_idx])\n\n            if not np.isfinite(value):\n                results[orig_idx] = np.nan\n                continue\n\n            if value &lt;= lower_bound:\n                results[orig_idx] = 0.0\n                continue\n\n            clipped = min(value, upper_bound)\n            if clipped &gt; current + 1e-15:\n                segment, _ = quad(\n                    scalar_integrand,\n                    current,\n                    clipped,\n                    args=params,\n                    epsabs=epsabs,\n                    epsrel=epsrel,\n                    limit=limit,\n                )\n                cumulative += segment\n                current = clipped\n\n            if value &gt;= upper_bound:\n                cumulative = 1.0\n                current = upper_bound\n                results[orig_idx] = 1.0\n            else:\n                results[orig_idx] = cumulative\n\n        results = np.clip(results, 0.0, 1.0)\n\n        if scalar_input:\n            return float(results[0])\n        return results.reshape(original_shape)\n\n    def _cdf_from_pdf(self, x, *params, **quad_kwargs):\n        \"\"\"Convenience wrapper around `_cdf_integral` using ``self._pdf``.\"\"\"\n        return self._cdf_integral(\n            x,\n            self._pdf,\n            params,\n            lower=quad_kwargs.pop(\"lower\", None),\n            upper=quad_kwargs.pop(\"upper\", None),\n            epsabs=quad_kwargs.pop(\"epsabs\", 1e-9),\n            epsrel=quad_kwargs.pop(\"epsrel\", 1e-9),\n            limit=quad_kwargs.pop(\"limit\", 200),\n        )\n\n    # ------------------------------------------------------------------\n    # Circular descriptive helpers\n    # ------------------------------------------------------------------\n    def trig_moment(self, p: int = 1, *args, **kwargs) -&gt; complex:\n        \"\"\"\n        Circular (trigonometric) moment m_p = E[e^{i p \u0398}] = C_p + i S_p.\n\n        Falls back to numeric evaluation via ``self.expect``; subclasses may\n        override with closed-form expressions.\n        \"\"\"\n        shape_args, non_shape_kwargs = self._separate_shape_parameters(args, kwargs, \"trig_moment\")\n        call_kwargs = self._prepare_call_kwargs(non_shape_kwargs, \"trig_moment\")\n        C_p = float(\n            np.asarray(self.expect(lambda x: np.cos(p * x), args=shape_args, **call_kwargs))\n        )\n        S_p = float(\n            np.asarray(self.expect(lambda x: np.sin(p * x), args=shape_args, **call_kwargs))\n        )\n        return complex(C_p, S_p)\n\n    def r(self, *args, **kwargs) -&gt; float:\n        \"\"\"Mean resultant length R = |m\u2081|.\"\"\"\n        m1 = self.trig_moment(1, *args, **kwargs)\n        return float(np.clip(abs(m1), 0.0, 1.0))\n\n    def mean(self, *args, **kwargs) -&gt; float:\n        \"\"\"Circular mean direction \u03bc = arg(m\u2081).\"\"\"\n        m1 = self.trig_moment(1, *args, **kwargs)\n        R = np.clip(abs(m1), 0.0, 1.0)\n        if np.isclose(R, 0.0, atol=1e-12):\n            return float(\"nan\")\n        return self._wrap_direction(np.angle(m1))\n\n    def median(self, *args, **kwargs) -&gt; float:\n        \"\"\"Circular median (50% quantile).\"\"\"\n        call_kwargs = self._prepare_call_kwargs(kwargs, \"median\")\n        return float(super().ppf(0.5, *args, **call_kwargs))\n\n    def var(self, *args, **kwargs) -&gt; float:\n        \"\"\"Circular variance V = 1 - R.\"\"\"\n        return float(1.0 - self.r(*args, **kwargs))\n\n    def std(self, *args, **kwargs) -&gt; float:\n        \"\"\"Circular standard deviation s = sqrt(-2 ln R).\"\"\"\n        R = np.clip(self.r(*args, **kwargs), 0.0, 1.0)\n        if np.isclose(R, 0.0, atol=1e-12):\n            return float(\"inf\")\n        return float(np.sqrt(max(0.0, -2.0 * np.log(np.clip(R, np.finfo(float).tiny, 1.0)))))\n\n    def dispersion(self, *args, **kwargs) -&gt; float:\n        \"\"\"Circular dispersion \u03b4\u0302 = (1 - \u03c1\u2082) / (2 \u03c1\u2081\u00b2).\"\"\"\n        m1 = self.trig_moment(1, *args, **kwargs)\n        r1 = np.clip(abs(m1), 0.0, 1.0)\n        if np.isclose(r1, 0.0, atol=1e-12):\n            return float(\"inf\")\n        m2 = self.trig_moment(2, *args, **kwargs)\n        r2 = np.clip(abs(m2), 0.0, 1.0)\n        return float((1.0 - r2) / (2.0 * r1 * r1))\n\n    def skewness(self, *args, **kwargs) -&gt; float:\n        \"\"\"Pewsey-style circular skewness.\"\"\"\n        m1 = self.trig_moment(1, *args, **kwargs)\n        u1 = np.angle(m1)\n        r1 = np.clip(abs(m1), 0.0, 1.0)\n        m2 = self.trig_moment(2, *args, **kwargs)\n        u2 = np.angle(m2)\n        r2 = np.clip(abs(m2), 0.0, 1.0)\n\n        denom_base = max(0.0, 1.0 - r1)\n        if np.isclose(denom_base, 0.0, atol=1e-12):\n            return float(\"nan\")\n        denom = denom_base**1.5\n        return float((r2 * np.sin(u2 - 2.0 * u1)) / denom)\n\n    def kurtosis(self, *args, **kwargs) -&gt; float:\n        \"\"\"Pewsey-style circular kurtosis.\"\"\"\n        m1 = self.trig_moment(1, *args, **kwargs)\n        u1 = np.angle(m1)\n        r1 = np.clip(abs(m1), 0.0, 1.0)\n        m2 = self.trig_moment(2, *args, **kwargs)\n        u2 = np.angle(m2)\n        r2 = np.clip(abs(m2), 0.0, 1.0)\n\n        denom_base = max(0.0, 1.0 - r1)\n        if np.isclose(denom_base, 0.0, atol=1e-12):\n            return float(\"nan\")\n        denom = denom_base**2\n        return float((r2 * np.cos(u2 - 2.0 * u1) - r1**4) / denom)\n\n    def stats(self, *args, **kwargs):\n        \"\"\"Convenience bundle of circular descriptive statistics.\"\"\"\n        m1 = self.trig_moment(1, *args, **kwargs)\n        r1 = np.clip(abs(m1), 0.0, 1.0)\n        u1 = np.angle(m1)\n\n        r1_is_zero = np.isclose(r1, 0.0, atol=1e-12)\n        mean_val = float(\"nan\") if r1_is_zero else self._wrap_direction(u1)\n\n        m2 = self.trig_moment(2, *args, **kwargs)\n        r2 = np.clip(abs(m2), 0.0, 1.0)\n        u2 = np.angle(m2)\n\n        denom_base = max(0.0, 1.0 - r1)\n        if np.isclose(denom_base, 0.0, atol=1e-12):\n            skew = float(\"nan\")\n            kurt = float(\"nan\")\n        else:\n            skew = float((r2 * np.sin(u2 - 2.0 * u1)) / (denom_base**1.5))\n            kurt = float((r2 * np.cos(u2 - 2.0 * u1) - r1**4) / (denom_base**2))\n\n        std_val = float(\"inf\") if r1_is_zero else float(\n            np.sqrt(max(0.0, -2.0 * np.log(np.clip(r1, np.finfo(float).tiny, 1.0))))\n        )\n        dispersion_val = float(\"inf\") if r1_is_zero else float((1.0 - r2) / (2.0 * r1 * r1))\n\n        return {\n            \"mean\": mean_val,\n            \"median\": self.median(*args, **kwargs),\n            \"r\": float(r1),\n            \"var\": float(1.0 - r1),\n            \"std\": std_val,\n            \"dispersion\": dispersion_val,\n            \"skewness\": skew,\n            \"kurtosis\": kurt,\n        }\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.freeze","title":"<code>freeze(*args, **kwds)</code>","text":"<p>Return a frozen circular distribution while enforcing fixed loc/scale.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def freeze(self, *args, **kwds) -&gt; \"CircularContinuousFrozen\":\n    \"\"\"\n    Return a frozen circular distribution while enforcing fixed loc/scale.\n    \"\"\"\n    call_kwargs = self._prepare_call_kwargs(kwds, \"freeze\")\n    return CircularContinuousFrozen(self, *args, **call_kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.trig_moment","title":"<code>trig_moment(p=1, *args, **kwargs)</code>","text":"<p>Circular (trigonometric) moment m_p = E[e^{i p \u0398}] = C_p + i S_p.</p> <p>Falls back to numeric evaluation via <code>self.expect</code>; subclasses may override with closed-form expressions.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def trig_moment(self, p: int = 1, *args, **kwargs) -&gt; complex:\n    \"\"\"\n    Circular (trigonometric) moment m_p = E[e^{i p \u0398}] = C_p + i S_p.\n\n    Falls back to numeric evaluation via ``self.expect``; subclasses may\n    override with closed-form expressions.\n    \"\"\"\n    shape_args, non_shape_kwargs = self._separate_shape_parameters(args, kwargs, \"trig_moment\")\n    call_kwargs = self._prepare_call_kwargs(non_shape_kwargs, \"trig_moment\")\n    C_p = float(\n        np.asarray(self.expect(lambda x: np.cos(p * x), args=shape_args, **call_kwargs))\n    )\n    S_p = float(\n        np.asarray(self.expect(lambda x: np.sin(p * x), args=shape_args, **call_kwargs))\n    )\n    return complex(C_p, S_p)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.r","title":"<code>r(*args, **kwargs)</code>","text":"<p>Mean resultant length R = |m\u2081|.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def r(self, *args, **kwargs) -&gt; float:\n    \"\"\"Mean resultant length R = |m\u2081|.\"\"\"\n    m1 = self.trig_moment(1, *args, **kwargs)\n    return float(np.clip(abs(m1), 0.0, 1.0))\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.mean","title":"<code>mean(*args, **kwargs)</code>","text":"<p>Circular mean direction \u03bc = arg(m\u2081).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def mean(self, *args, **kwargs) -&gt; float:\n    \"\"\"Circular mean direction \u03bc = arg(m\u2081).\"\"\"\n    m1 = self.trig_moment(1, *args, **kwargs)\n    R = np.clip(abs(m1), 0.0, 1.0)\n    if np.isclose(R, 0.0, atol=1e-12):\n        return float(\"nan\")\n    return self._wrap_direction(np.angle(m1))\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.median","title":"<code>median(*args, **kwargs)</code>","text":"<p>Circular median (50% quantile).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def median(self, *args, **kwargs) -&gt; float:\n    \"\"\"Circular median (50% quantile).\"\"\"\n    call_kwargs = self._prepare_call_kwargs(kwargs, \"median\")\n    return float(super().ppf(0.5, *args, **call_kwargs))\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.var","title":"<code>var(*args, **kwargs)</code>","text":"<p>Circular variance V = 1 - R.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def var(self, *args, **kwargs) -&gt; float:\n    \"\"\"Circular variance V = 1 - R.\"\"\"\n    return float(1.0 - self.r(*args, **kwargs))\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.std","title":"<code>std(*args, **kwargs)</code>","text":"<p>Circular standard deviation s = sqrt(-2 ln R).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def std(self, *args, **kwargs) -&gt; float:\n    \"\"\"Circular standard deviation s = sqrt(-2 ln R).\"\"\"\n    R = np.clip(self.r(*args, **kwargs), 0.0, 1.0)\n    if np.isclose(R, 0.0, atol=1e-12):\n        return float(\"inf\")\n    return float(np.sqrt(max(0.0, -2.0 * np.log(np.clip(R, np.finfo(float).tiny, 1.0)))))\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.dispersion","title":"<code>dispersion(*args, **kwargs)</code>","text":"<p>Circular dispersion \u03b4\u0302 = (1 - \u03c1\u2082) / (2 \u03c1\u2081\u00b2).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def dispersion(self, *args, **kwargs) -&gt; float:\n    \"\"\"Circular dispersion \u03b4\u0302 = (1 - \u03c1\u2082) / (2 \u03c1\u2081\u00b2).\"\"\"\n    m1 = self.trig_moment(1, *args, **kwargs)\n    r1 = np.clip(abs(m1), 0.0, 1.0)\n    if np.isclose(r1, 0.0, atol=1e-12):\n        return float(\"inf\")\n    m2 = self.trig_moment(2, *args, **kwargs)\n    r2 = np.clip(abs(m2), 0.0, 1.0)\n    return float((1.0 - r2) / (2.0 * r1 * r1))\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.skewness","title":"<code>skewness(*args, **kwargs)</code>","text":"<p>Pewsey-style circular skewness.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def skewness(self, *args, **kwargs) -&gt; float:\n    \"\"\"Pewsey-style circular skewness.\"\"\"\n    m1 = self.trig_moment(1, *args, **kwargs)\n    u1 = np.angle(m1)\n    r1 = np.clip(abs(m1), 0.0, 1.0)\n    m2 = self.trig_moment(2, *args, **kwargs)\n    u2 = np.angle(m2)\n    r2 = np.clip(abs(m2), 0.0, 1.0)\n\n    denom_base = max(0.0, 1.0 - r1)\n    if np.isclose(denom_base, 0.0, atol=1e-12):\n        return float(\"nan\")\n    denom = denom_base**1.5\n    return float((r2 * np.sin(u2 - 2.0 * u1)) / denom)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.kurtosis","title":"<code>kurtosis(*args, **kwargs)</code>","text":"<p>Pewsey-style circular kurtosis.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def kurtosis(self, *args, **kwargs) -&gt; float:\n    \"\"\"Pewsey-style circular kurtosis.\"\"\"\n    m1 = self.trig_moment(1, *args, **kwargs)\n    u1 = np.angle(m1)\n    r1 = np.clip(abs(m1), 0.0, 1.0)\n    m2 = self.trig_moment(2, *args, **kwargs)\n    u2 = np.angle(m2)\n    r2 = np.clip(abs(m2), 0.0, 1.0)\n\n    denom_base = max(0.0, 1.0 - r1)\n    if np.isclose(denom_base, 0.0, atol=1e-12):\n        return float(\"nan\")\n    denom = denom_base**2\n    return float((r2 * np.cos(u2 - 2.0 * u1) - r1**4) / denom)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuous.stats","title":"<code>stats(*args, **kwargs)</code>","text":"<p>Convenience bundle of circular descriptive statistics.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def stats(self, *args, **kwargs):\n    \"\"\"Convenience bundle of circular descriptive statistics.\"\"\"\n    m1 = self.trig_moment(1, *args, **kwargs)\n    r1 = np.clip(abs(m1), 0.0, 1.0)\n    u1 = np.angle(m1)\n\n    r1_is_zero = np.isclose(r1, 0.0, atol=1e-12)\n    mean_val = float(\"nan\") if r1_is_zero else self._wrap_direction(u1)\n\n    m2 = self.trig_moment(2, *args, **kwargs)\n    r2 = np.clip(abs(m2), 0.0, 1.0)\n    u2 = np.angle(m2)\n\n    denom_base = max(0.0, 1.0 - r1)\n    if np.isclose(denom_base, 0.0, atol=1e-12):\n        skew = float(\"nan\")\n        kurt = float(\"nan\")\n    else:\n        skew = float((r2 * np.sin(u2 - 2.0 * u1)) / (denom_base**1.5))\n        kurt = float((r2 * np.cos(u2 - 2.0 * u1) - r1**4) / (denom_base**2))\n\n    std_val = float(\"inf\") if r1_is_zero else float(\n        np.sqrt(max(0.0, -2.0 * np.log(np.clip(r1, np.finfo(float).tiny, 1.0))))\n    )\n    dispersion_val = float(\"inf\") if r1_is_zero else float((1.0 - r2) / (2.0 * r1 * r1))\n\n    return {\n        \"mean\": mean_val,\n        \"median\": self.median(*args, **kwargs),\n        \"r\": float(r1),\n        \"var\": float(1.0 - r1),\n        \"std\": std_val,\n        \"dispersion\": dispersion_val,\n        \"skewness\": skew,\n        \"kurtosis\": kurt,\n    }\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.CircularContinuousFrozen","title":"<code>CircularContinuousFrozen</code>","text":"<p>               Bases: <code>rv_continuous_frozen</code></p> <p>Frozen circular distribution exposing circular descriptive helpers.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class CircularContinuousFrozen(rv_continuous_frozen):\n    \"\"\"Frozen circular distribution exposing circular descriptive helpers.\"\"\"\n\n    def _call_dist_method(self, name, *args, **kwargs):\n        call_kwargs = dict(self.kwds)\n        call_kwargs.update(kwargs)\n        call_args = self.args + args\n        return getattr(self.dist, name)(*call_args, **call_kwargs)\n\n    def trig_moment(self, p: int = 1, *args, **kwargs) -&gt; complex:\n        call_kwargs = dict(self.kwds)\n        call_kwargs.update(kwargs)\n        call_args = self.args + args\n        return self.dist.trig_moment(p, *call_args, **call_kwargs)\n\n    def r(self, *args, **kwargs) -&gt; float:\n        return self._call_dist_method(\"r\", *args, **kwargs)\n\n    def dispersion(self, *args, **kwargs) -&gt; float:\n        return self._call_dist_method(\"dispersion\", *args, **kwargs)\n\n    def skewness(self, *args, **kwargs) -&gt; float:\n        return self._call_dist_method(\"skewness\", *args, **kwargs)\n\n    def kurtosis(self, *args, **kwargs) -&gt; float:\n        return self._call_dist_method(\"kurtosis\", *args, **kwargs)\n\n    def stats(self, *args, **kwargs):\n        return self._call_dist_method(\"stats\", *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.circularuniform_gen","title":"<code>circularuniform_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Continuous Circular Uniform Distribution</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Percent-point function (inverse of CDF).</p> <code>rvs</code> <p>Random variates.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class circularuniform_gen(CircularContinuous):\n    \"\"\"Continuous Circular Uniform Distribution\n\n    ![circularuniform](../images/circ-mod-circularuniform.png)\n\n    Methods\n    -------\n    pdf(x)\n        Probability density function.\n\n    cdf(x)\n        Cumulative distribution function.\n\n    ppf(q)\n        Percent-point function (inverse of CDF).\n\n    rvs(size, random_state) \n        Random variates.\n    \"\"\"\n\n    def _pdf(self, x):\n        return 1 / (2 * np.pi)\n\n    def pdf(self, x, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Circular Uniform distribution.\n\n        $$\n        f(\\theta) = \\frac{1}{2\\pi}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n        return super().pdf(x, *args, **kwargs)\n\n    def _cdf(self, x):\n        return x / (2 * np.pi)\n\n    def cdf(self, x, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the Circular Uniform distribution.\n\n        $$\n        F(\\theta) = \\frac{\\theta}{2\\pi}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative distribution function evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, *args, **kwargs)\n\n    def _ppf(self, q):\n        return 2 * np.pi * q\n\n    def ppf(self, q, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (inverse of the CDF) of the Circular Uniform distribution.\n\n        $$\n        Q(q) = F^{-1}(q) = 2\\pi q, \\space 0 \\leq q \\leq 1\n        $$\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles to evaluate.\n\n        Returns\n        -------\n        ppf_values : array_like\n            Values at the given quantiles.\n        \"\"\"\n        return super().ppf(q, *args, **kwargs)\n\n    def _rvs(self, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n        return rng.uniform(0.0, 2 * np.pi, size=size)\n\n    def rvs(self, size=None, random_state=None):\n        \"\"\"\n        Random variate generation for the circular uniform distribution.\n\n        Parameters\n        ----------\n        size : int or tuple of ints, optional\n            Number of samples to draw. If ``None`` (default), return a single value.\n        random_state : np.random.Generator, np.random.RandomState, or None, optional\n            Random number generator to use. If ``None``, fall back to the\n            distribution's internal generator.\n\n        Returns\n        -------\n        samples : ndarray or float\n            Samples drawn uniformly from the interval ``[0, 2\u03c0)``.\n        \"\"\"\n        return self._rvs(size=size, random_state=random_state)\n\n    def fit(self, data):\n        \"\"\"\n        The circular uniform distribution has no free parameters to estimate,\n        so calling ``fit`` is undefined. A ``NotImplementedError`` is raised to\n        signal that users should rely on descriptive helpers (e.g.,\n        ``circ_mean_and_r``) instead of maximum-likelihood fitting.\n        \"\"\"\n        raise NotImplementedError(\n            \"circularuniform.fit() is undefined: the distribution has no parameters to estimate.\"\n        )\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.circularuniform_gen.pdf","title":"<code>pdf(x, *args, **kwargs)</code>","text":"<p>Probability density function of the Circular Uniform distribution.</p> \\[ f(\\theta) = \\frac{1}{2\\pi} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Circular Uniform distribution.\n\n    $$\n    f(\\theta) = \\frac{1}{2\\pi}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n    return super().pdf(x, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.circularuniform_gen.cdf","title":"<code>cdf(x, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Circular Uniform distribution.</p> \\[ F(\\theta) = \\frac{\\theta}{2\\pi} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative distribution function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the Circular Uniform distribution.\n\n    $$\n    F(\\theta) = \\frac{\\theta}{2\\pi}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative distribution function evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.circularuniform_gen.ppf","title":"<code>ppf(q, *args, **kwargs)</code>","text":"<p>Percent-point function (inverse of the CDF) of the Circular Uniform distribution.</p> \\[ Q(q) = F^{-1}(q) = 2\\pi q, \\space 0 \\leq q \\leq 1 \\] <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles to evaluate.</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Values at the given quantiles.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (inverse of the CDF) of the Circular Uniform distribution.\n\n    $$\n    Q(q) = F^{-1}(q) = 2\\pi q, \\space 0 \\leq q \\leq 1\n    $$\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles to evaluate.\n\n    Returns\n    -------\n    ppf_values : array_like\n        Values at the given quantiles.\n    \"\"\"\n    return super().ppf(q, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.circularuniform_gen.rvs","title":"<code>rvs(size=None, random_state=None)</code>","text":"<p>Random variate generation for the circular uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int or tuple of ints</code> <p>Number of samples to draw. If <code>None</code> (default), return a single value.</p> <code>None</code> <code>random_state</code> <code>np.random.Generator, np.random.RandomState, or None</code> <p>Random number generator to use. If <code>None</code>, fall back to the distribution's internal generator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray or float</code> <p>Samples drawn uniformly from the interval <code>[0, 2\u03c0)</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, size=None, random_state=None):\n    \"\"\"\n    Random variate generation for the circular uniform distribution.\n\n    Parameters\n    ----------\n    size : int or tuple of ints, optional\n        Number of samples to draw. If ``None`` (default), return a single value.\n    random_state : np.random.Generator, np.random.RandomState, or None, optional\n        Random number generator to use. If ``None``, fall back to the\n        distribution's internal generator.\n\n    Returns\n    -------\n    samples : ndarray or float\n        Samples drawn uniformly from the interval ``[0, 2\u03c0)``.\n    \"\"\"\n    return self._rvs(size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.circularuniform_gen.fit","title":"<code>fit(data)</code>","text":"<p>The circular uniform distribution has no free parameters to estimate, so calling <code>fit</code> is undefined. A <code>NotImplementedError</code> is raised to signal that users should rely on descriptive helpers (e.g., <code>circ_mean_and_r</code>) instead of maximum-likelihood fitting.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(self, data):\n    \"\"\"\n    The circular uniform distribution has no free parameters to estimate,\n    so calling ``fit`` is undefined. A ``NotImplementedError`` is raised to\n    signal that users should rely on descriptive helpers (e.g.,\n    ``circ_mean_and_r``) instead of maximum-likelihood fitting.\n    \"\"\"\n    raise NotImplementedError(\n        \"circularuniform.fit() is undefined: the distribution has no parameters to estimate.\"\n    )\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.triangular_gen","title":"<code>triangular_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Triangular Distribution</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Closed-form quantile (inverse CDF).</p> <code>rvs</code> <p>Random variates via inverse-transform using the closed-form quantile.</p> <code>fit</code> <p>Fit the distribution to the data and return the parameter (rho).</p> Notes <p>Implementation based on Section 2.2.3 of Jammalamadaka &amp; SenGupta (2001)</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class triangular_gen(CircularContinuous):\n    \"\"\"Triangular Distribution\n\n    ![triangular](../images/circ-mod-triangular.png)\n\n    Methods\n    -------\n    pdf(x, rho)\n        Probability density function.\n\n    cdf(x, rho)\n        Cumulative distribution function.\n\n    ppf(q, rho)\n        Closed-form quantile (inverse CDF).\n\n    rvs(rho, size=None, random_state=None)\n        Random variates via inverse-transform using the closed-form quantile.\n\n    fit(data)\n        Fit the distribution to the data and return the parameter (rho).\n\n    Notes\n    -----\n    Implementation based on Section 2.2.3 of Jammalamadaka &amp; SenGupta (2001)\n    \"\"\"\n\n    def _argcheck(self, rho):\n        rho_arr = np.asarray(rho, dtype=float)\n        return (rho_arr &gt;= 0.0) &amp; (rho_arr &lt;= 4.0 / np.pi**2)\n\n    def _pdf(self, x, rho):\n        return (\n            (4 - np.pi**2.0 * rho + 2.0 * np.pi * rho * np.abs(np.pi - x)) / 8.0 / np.pi\n        )\n\n    def pdf(self, x, rho, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Triangular distribution.\n\n        $$\n        f(\\theta) = \\frac{4 - \\pi^2 \\rho + 2\\pi \\rho |\\pi - \\theta|}{8\\pi}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        rho : float\n            Concentratio parameter, 0 &lt;= rho &lt;= 4/pi^2.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n\n        return super().pdf(x, rho, *args, **kwargs)\n\n    def _cdf(self, x, rho):\n        x_arr = np.asarray(x, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n        x_b, rho_b = np.broadcast_arrays(x_arr, rho_arr)\n\n        result = np.zeros_like(x_b, dtype=float)\n\n        # lower branch: 0 &lt;= x &lt;= pi\n        mask_lower = (x_b &gt;= 0.0) &amp; (x_b &lt;= np.pi)\n        if np.any(mask_lower):\n            xl = x_b[mask_lower]\n            rl = rho_b[mask_lower]\n            result[mask_lower] = ((4 + np.pi**2 * rl) * xl - np.pi * rl * xl**2) / (\n                8 * np.pi\n            )\n\n        # upper branch: pi &lt; x &lt; 2pi\n        mask_upper = (x_b &gt; np.pi) &amp; (x_b &lt; 2 * np.pi)\n        if np.any(mask_upper):\n            xu = x_b[mask_upper]\n            ru = rho_b[mask_upper]\n            result[mask_upper] = 0.5 + (\n                (4 - 3 * np.pi**2 * ru) * (xu - np.pi)\n                + np.pi * ru * (xu**2 - np.pi**2)\n            ) / (8 * np.pi)\n\n        # upper tail: x &gt;= 2pi\n        result = np.where(x_b &gt;= 2 * np.pi, 1.0, result)\n\n        if np.ndim(result) == 0:\n            return float(result)\n        return result\n\n    def cdf(self, x, rho, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the circular triangular distribution on $[0, 2\\pi)$.\n\n        $$\n        F(\\theta;\\,\\rho)=\n        \\begin{cases}\n        \\dfrac{(4+\\pi^2\\rho)\\,\\theta - \\pi\\rho\\,\\theta^2}{8\\pi}, &amp; 0 \\le \\theta \\le \\pi,\\\\[6pt]\n        \\dfrac{1}{2} + \\dfrac{(4 - 3\\pi^2\\rho)\\,(\\theta-\\pi) + \\pi\\rho\\,(\\theta^2-\\pi^2)}{8\\pi},\n            &amp; \\pi &lt; \\theta &lt; 2\\pi.\n        \\end{cases}\n        $$\n\n        (With $F(\\theta)=0$ for $\\theta&lt;0$ and $F(\\theta)=1$ for $\\theta\\ge 2\\pi$.)\n\n        Parameters\n        ----------\n        x : array_like\n            Angles in radians on $[0, 2\\pi)$.\n        rho : float\n            Concentration parameter, $0 \\le \\rho \\le 4/\\pi^2$.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative distribution function evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, rho, *args, **kwargs)\n\n    def _ppf(self, q, rho):\n        q_arr = np.asarray(q, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n        q_b, rho_b = np.broadcast_arrays(q_arr, rho_arr)\n\n        result = np.empty_like(q_b, dtype=float)\n\n        mask_zero = np.isclose(rho_b, 0.0, atol=1e-12)\n        if np.any(mask_zero):\n            result[mask_zero] = q_b[mask_zero] * (2 * np.pi)\n\n        mask_general = ~mask_zero\n        if np.any(mask_general):\n            q_g = q_b[mask_general]\n            rho_g = rho_b[mask_general]\n\n            a_left = rho_g\n            b_left = -(4 + np.pi**2 * rho_g) / np.pi\n            a_right = rho_g\n            b_right = (4 - 3 * np.pi**2 * rho_g) / np.pi\n\n            res_general = np.empty_like(q_g, dtype=float)\n            mask_left = q_g &lt;= 0.5\n\n            if np.any(mask_left):\n                c_left = 8 * q_g[mask_left]\n                disc_left = np.clip(\n                    b_left[mask_left] ** 2 - 4 * a_left[mask_left] * c_left,\n                    0.0,\n                    None,\n                )\n                res_general[mask_left] = (\n                    -b_left[mask_left] - np.sqrt(disc_left)\n                ) / (2 * a_left[mask_left])\n\n            if np.any(~mask_left):\n                c_right = 2 * np.pi**2 * rho_g[~mask_left] - 8 * q_g[~mask_left]\n                disc_right = np.clip(\n                    b_right[~mask_left] ** 2 - 4 * a_right[~mask_left] * c_right,\n                    0.0,\n                    None,\n                )\n                res_general[~mask_left] = (\n                    -b_right[~mask_left] + np.sqrt(disc_right)\n                ) / (2 * a_right[~mask_left])\n\n            result[mask_general] = res_general\n\n        np.clip(result, 0.0, 2 * np.pi - np.finfo(float).eps, out=result)\n        if result.ndim == 0:\n            return float(result)\n        return result\n\n    def ppf(self, q, rho, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (quantile) of the circular triangular distribution on $[0, 2\\pi)$.\n\n        For $\\rho=0$ (circular uniform):\n\n        $$\n        \\operatorname{PPF}(q;0)=2\\pi q.\n        $$\n\n        For $\\rho&gt;0$:\n\n        $$\n        \\operatorname{PPF}(q;\\rho)=\n        \\begin{cases}\n        \\dfrac{1}{2\\rho}\\!\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\n        - \\sqrt{\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\\right)^{\\!2} - 32\\rho\\,q}\\right),\n        &amp; 0 \\le q \\le \\tfrac{1}{2}, \\\\[10pt]\n        \\pi + \\dfrac{-\\,(4-\\pi^2\\rho) + \\sqrt{(4-\\pi^2\\rho)^{2} + 32\\pi^{2}\\rho\\,(q-\\tfrac{1}{2})}}\n        {2\\pi\\rho},\n        &amp; \\tfrac{1}{2} &lt; q &lt; 1.\n        \\end{cases}\n        $$\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles in $[0, 1]$.\n        rho : float\n            Concentration parameter, $0 \\le \\rho \\le 4/\\pi^2$.\n\n        Returns\n        -------\n        ppf_values : array_like\n            Quantiles (angles in radians on $[0, 2\\pi)$).\n        \"\"\"\n        return super().ppf(q, rho, *args, **kwargs)\n\n    def _rvs(self, rho, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n        u = rng.uniform(0.0, 1.0, size=size)\n        samples = self._ppf(u, rho)\n        if np.isscalar(samples):\n            return float(samples)\n        return np.asarray(samples, dtype=float)\n\n    def rvs(self, rho=None, size=None, random_state=None):\n        r\"\"\"\n        Random variates from the circular triangular distribution on $[0, 2\\pi)$.\n\n        Sampling uses **inverse-transform** with the closed-form quantile:\n        let $U \\sim \\mathrm{Unif}(0,1)$ and set $\\theta = \\operatorname{PPF}(U;\\rho)$, where\n\n        - For $\\rho = 0$ (circular uniform):\n\n        $$\n        \\theta = 2\\pi U.\n        $$\n\n        - For $\\rho &gt; 0$ (piecewise quadratic inverse):\n\n        $$\n        \\theta =\n        \\begin{cases}\n            \\dfrac{1}{2\\rho}\\!\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\n            - \\sqrt{\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\\right)^{\\!2} - 32\\rho\\,U}\\right),\n            &amp; 0 \\le U \\le \\tfrac{1}{2}, \\\\[10pt]\n            \\pi + \\dfrac{-\\,(4-\\pi^2\\rho) + \\sqrt{(4-\\pi^2\\rho)^{2} + 32\\pi^{2}\\rho\\,(U-\\tfrac{1}{2})}}\n            {2\\pi\\rho},\n            &amp; \\tfrac{1}{2} &lt; U &lt; 1.\n        \\end{cases}\n        $$\n\n        Parameters\n        ----------\n        rho : float, optional\n            Concentration, $0 \\le \\rho \\le 4/\\pi^2$. Supply explicitly or by\n            freezing the distribution.\n        size : int or tuple of ints, optional\n            Output shape. If ``None`` (default), return a single scalar.\n        random_state : int, numpy.random.Generator, numpy.random.RandomState, optional\n            PRNG seed or generator. If ``None``, use the distribution's internal RNG.\n\n        Returns\n        -------\n        samples : ndarray or float\n            Angles in radians on $[0, 2\\pi)$, with shape ``size``.\n\n        Notes\n        -----\n        This is equivalent in law to R's **circular** `rtriangular` after\n        shifting its output by $+\\pi$ modulo $2\\pi$.\n        \"\"\"\n        rho_val = getattr(self, \"rho\", None) if rho is None else rho\n        if rho_val is None:\n            raise ValueError(\"'rho' must be provided.\")\n        return self._rvs(rho_val, size=size, random_state=random_state)\n\n    def fit(self, data, *, weights=None, method=\"mle\", return_info=False):\n        r\"\"\"\n        Estimate the concentration parameter $\\rho$ of the circular triangular law on $[0,2\\pi)$.\n\n        Methods\n        -------\n\n        mle (default): \n            maximize the log-likelihood. This solves the 1-D score equation\n            $\\sum_i \\frac{c_i}{4+\\rho\\,c_i}=0$ with $c_i = 2\\pi\\,|\\,\\pi-x_i\\,| - \\pi^2$.\n            Unique solution in $[0, 4/\\pi^2)$ or at a boundary.\n        moments :\n            closed-form $\\hat\\rho = \\max\\{0, \\min\\{4/\\pi^2,\\ \\overline{\\cos x}\\}\\}$.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample angles (radians). Values are wrapped to $[0, 2\\pi)$ internally.\n        weights : array_like, optional\n            Nonnegative sample weights. Broadcastable to `data`. Interpreted as frequencies.\n        method : {\"mle\",\"moments\"}, optional\n            Estimation method (see above).\n        return_info : bool, optional\n            If True, also return a dict with diagnostics (loglik, se, n_effective, method).\n\n        Returns\n        -------\n        rho_hat : float\n            Estimated concentration $\\hat\\rho \\in [0, 4/\\pi^2]$.\n        info : dict, optional\n            Returned only if `return_info=True`. Contains keys:\n            {\"loglik\", \"se\", \"n_effective\", \"method\", \"converged\"}.\n\n        Notes\n        -----\n        For this distribution $\\mathbb{E}[\\cos \\Theta]=\\rho$, so the method-of-moments\n        estimator is simply the (weighted) mean of $\\cos x$ clipped to $[0,4/\\pi^2]$.\n        The MLE solves a strictly monotone score equation, so bracketing root-finding\n        is robust and $O(n)$ per evaluation.\n        \"\"\"\n        x = np.asarray(data, dtype=float)\n        x = np.mod(x, 2*np.pi)\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"weights must be nonnegative\")\n            # broadcast\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n        # Effective sample size for diagnostics\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"sum of weights must be positive\")\n        w_norm = w / w_sum\n        n_eff = w_sum**2 / np.sum(w**2)  # Kish effective n\n\n        # Method-of-moments (always available; used as fallback/initial intuition)\n        r_bar = float(np.sum(w_norm * np.cos(x)))\n        rho_mom = float(np.clip(r_bar, 0.0, 4/np.pi**2))\n\n        if method == \"moments\":\n            rho_hat = rho_mom\n            # log-likelihood at MoM (for info only)\n            y = np.abs(np.pi - x)\n            ll = float(np.sum(w * np.log(4 - np.pi**2 * rho_hat + 2*np.pi * rho_hat * y)) - w_sum*np.log(8*np.pi))\n            # observed Fisher info for SE\n            c = 2*np.pi*y - np.pi**2\n            info_obs = float(np.sum(w * (c**2) / (4 + rho_hat*c)**2))\n            se = (1.0 / np.sqrt(info_obs)) if info_obs &gt; 0 else np.nan\n            if return_info:\n                return rho_hat, {\"loglik\": ll, \"se\": se, \"n_effective\": n_eff, \"method\": \"moments\", \"converged\": True}\n            return rho_hat\n\n        # --- MLE via monotone root of the score ---\n        y = np.abs(np.pi - x)                 # in [0, \u03c0]\n        c = 2*np.pi*y - np.pi**2              # in [-\u03c0^2, \u03c0^2]\n\n        def score(rho):\n            return float(np.sum(w * (c / (4.0 + rho * c))))\n\n        # Bracket: score(\u03c1) is strictly decreasing on [0, \u03c1_max)\n        rho_lo = 0.0\n        rho_hi = float(4/np.pi**2) - 1e-12\n\n        s_lo = score(rho_lo)  # = (1/4) * sum w*c\n        if s_lo &lt;= 0:         # likelihood decreasing at 0 \u2192 boundary optimum\n            rho_hat = 0.0\n            converged = True\n        else:\n            s_hi = score(rho_hi)  # tends negative if any y_i\u22480\n            if s_hi &gt;= 0:\n                # all mass far from \u03c0 (extreme case) \u2192 boundary at \u03c1_max\n                rho_hat = rho_hi\n                converged = True\n            else:\n                # Unique root inside (0, \u03c1_max)\n                rho_hat = float(brentq(score, rho_lo, rho_hi, xtol=1e-12, rtol=1e-12, maxiter=256))\n                converged = True\n\n        # Diagnostics\n        ll = float(np.sum(w * np.log(4 - np.pi**2 * rho_hat + 2*np.pi * rho_hat * y)) - w_sum*np.log(8*np.pi))\n        info_obs = float(np.sum(w * (c**2) / (4 + rho_hat*c)**2))\n        se = (1.0 / np.sqrt(info_obs)) if info_obs &gt; 0 else np.nan\n\n        if return_info:\n            return rho_hat, {\"loglik\": ll, \"se\": se, \"n_effective\": n_eff, \"method\": \"mle\", \"converged\": converged}\n        return rho_hat\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.triangular_gen.pdf","title":"<code>pdf(x, rho, *args, **kwargs)</code>","text":"<p>Probability density function of the Triangular distribution.</p> \\[ f(\\theta) = \\frac{4 - \\pi^2 \\rho + 2\\pi \\rho |\\pi - \\theta|}{8\\pi} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>rho</code> <code>float</code> <p>Concentratio parameter, 0 &lt;= rho &lt;= 4/pi^2.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, rho, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Triangular distribution.\n\n    $$\n    f(\\theta) = \\frac{4 - \\pi^2 \\rho + 2\\pi \\rho |\\pi - \\theta|}{8\\pi}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    rho : float\n        Concentratio parameter, 0 &lt;= rho &lt;= 4/pi^2.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n\n    return super().pdf(x, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.triangular_gen.cdf","title":"<code>cdf(x, rho, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the circular triangular distribution on \\([0, 2\\pi)\\).</p> \\[ F(\\theta;\\,\\rho)= \\begin{cases} \\dfrac{(4+\\pi^2\\rho)\\,\\theta - \\pi\\rho\\,\\theta^2}{8\\pi}, &amp; 0 \\le \\theta \\le \\pi,\\\\[6pt] \\dfrac{1}{2} + \\dfrac{(4 - 3\\pi^2\\rho)\\,(\\theta-\\pi) + \\pi\\rho\\,(\\theta^2-\\pi^2)}{8\\pi},     &amp; \\pi &lt; \\theta &lt; 2\\pi. \\end{cases} \\] <p>(With \\(F(\\theta)=0\\) for \\(\\theta&lt;0\\) and \\(F(\\theta)=1\\) for \\(\\theta\\ge 2\\pi\\).)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Angles in radians on \\([0, 2\\pi)\\).</p> required <code>rho</code> <code>float</code> <p>Concentration parameter, \\(0 \\le \\rho \\le 4/\\pi^2\\).</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative distribution function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, rho, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the circular triangular distribution on $[0, 2\\pi)$.\n\n    $$\n    F(\\theta;\\,\\rho)=\n    \\begin{cases}\n    \\dfrac{(4+\\pi^2\\rho)\\,\\theta - \\pi\\rho\\,\\theta^2}{8\\pi}, &amp; 0 \\le \\theta \\le \\pi,\\\\[6pt]\n    \\dfrac{1}{2} + \\dfrac{(4 - 3\\pi^2\\rho)\\,(\\theta-\\pi) + \\pi\\rho\\,(\\theta^2-\\pi^2)}{8\\pi},\n        &amp; \\pi &lt; \\theta &lt; 2\\pi.\n    \\end{cases}\n    $$\n\n    (With $F(\\theta)=0$ for $\\theta&lt;0$ and $F(\\theta)=1$ for $\\theta\\ge 2\\pi$.)\n\n    Parameters\n    ----------\n    x : array_like\n        Angles in radians on $[0, 2\\pi)$.\n    rho : float\n        Concentration parameter, $0 \\le \\rho \\le 4/\\pi^2$.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative distribution function evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.triangular_gen.ppf","title":"<code>ppf(q, rho, *args, **kwargs)</code>","text":"<p>Percent-point function (quantile) of the circular triangular distribution on \\([0, 2\\pi)\\).</p> <p>For \\(\\rho=0\\) (circular uniform):</p> \\[ \\operatorname{PPF}(q;0)=2\\pi q. \\] <p>For \\(\\rho&gt;0\\):</p> \\[ \\operatorname{PPF}(q;\\rho)= \\begin{cases} \\dfrac{1}{2\\rho}\\!\\left(\\dfrac{4+\\pi^2\\rho}{\\pi} - \\sqrt{\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\\right)^{\\!2} - 32\\rho\\,q}\\right), &amp; 0 \\le q \\le \\tfrac{1}{2}, \\\\[10pt] \\pi + \\dfrac{-\\,(4-\\pi^2\\rho) + \\sqrt{(4-\\pi^2\\rho)^{2} + 32\\pi^{2}\\rho\\,(q-\\tfrac{1}{2})}} {2\\pi\\rho}, &amp; \\tfrac{1}{2} &lt; q &lt; 1. \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles in \\([0, 1]\\).</p> required <code>rho</code> <code>float</code> <p>Concentration parameter, \\(0 \\le \\rho \\le 4/\\pi^2\\).</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Quantiles (angles in radians on \\([0, 2\\pi)\\)).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, rho, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (quantile) of the circular triangular distribution on $[0, 2\\pi)$.\n\n    For $\\rho=0$ (circular uniform):\n\n    $$\n    \\operatorname{PPF}(q;0)=2\\pi q.\n    $$\n\n    For $\\rho&gt;0$:\n\n    $$\n    \\operatorname{PPF}(q;\\rho)=\n    \\begin{cases}\n    \\dfrac{1}{2\\rho}\\!\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\n    - \\sqrt{\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\\right)^{\\!2} - 32\\rho\\,q}\\right),\n    &amp; 0 \\le q \\le \\tfrac{1}{2}, \\\\[10pt]\n    \\pi + \\dfrac{-\\,(4-\\pi^2\\rho) + \\sqrt{(4-\\pi^2\\rho)^{2} + 32\\pi^{2}\\rho\\,(q-\\tfrac{1}{2})}}\n    {2\\pi\\rho},\n    &amp; \\tfrac{1}{2} &lt; q &lt; 1.\n    \\end{cases}\n    $$\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles in $[0, 1]$.\n    rho : float\n        Concentration parameter, $0 \\le \\rho \\le 4/\\pi^2$.\n\n    Returns\n    -------\n    ppf_values : array_like\n        Quantiles (angles in radians on $[0, 2\\pi)$).\n    \"\"\"\n    return super().ppf(q, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.triangular_gen.rvs","title":"<code>rvs(rho=None, size=None, random_state=None)</code>","text":"<p>Random variates from the circular triangular distribution on \\([0, 2\\pi)\\).</p> <p>Sampling uses inverse-transform with the closed-form quantile: let \\(U \\sim \\mathrm{Unif}(0,1)\\) and set \\(\\theta = \\operatorname{PPF}(U;\\rho)\\), where</p> <ul> <li>For \\(\\rho = 0\\) (circular uniform):</li> </ul> \\[ \\theta = 2\\pi U. \\] <ul> <li>For \\(\\rho &gt; 0\\) (piecewise quadratic inverse):</li> </ul> \\[ \\theta = \\begin{cases}     \\dfrac{1}{2\\rho}\\!\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}     - \\sqrt{\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\\right)^{\\!2} - 32\\rho\\,U}\\right),     &amp; 0 \\le U \\le \\tfrac{1}{2}, \\\\[10pt]     \\pi + \\dfrac{-\\,(4-\\pi^2\\rho) + \\sqrt{(4-\\pi^2\\rho)^{2} + 32\\pi^{2}\\rho\\,(U-\\tfrac{1}{2})}}     {2\\pi\\rho},     &amp; \\tfrac{1}{2} &lt; U &lt; 1. \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>rho</code> <code>float</code> <p>Concentration, \\(0 \\le \\rho \\le 4/\\pi^2\\). Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>size</code> <code>int or tuple of ints</code> <p>Output shape. If <code>None</code> (default), return a single scalar.</p> <code>None</code> <code>random_state</code> <code>(int, Generator, RandomState)</code> <p>PRNG seed or generator. If <code>None</code>, use the distribution's internal RNG.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray or float</code> <p>Angles in radians on \\([0, 2\\pi)\\), with shape <code>size</code>.</p> Notes <p>This is equivalent in law to R's circular <code>rtriangular</code> after shifting its output by \\(+\\pi\\) modulo \\(2\\pi\\).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, rho=None, size=None, random_state=None):\n    r\"\"\"\n    Random variates from the circular triangular distribution on $[0, 2\\pi)$.\n\n    Sampling uses **inverse-transform** with the closed-form quantile:\n    let $U \\sim \\mathrm{Unif}(0,1)$ and set $\\theta = \\operatorname{PPF}(U;\\rho)$, where\n\n    - For $\\rho = 0$ (circular uniform):\n\n    $$\n    \\theta = 2\\pi U.\n    $$\n\n    - For $\\rho &gt; 0$ (piecewise quadratic inverse):\n\n    $$\n    \\theta =\n    \\begin{cases}\n        \\dfrac{1}{2\\rho}\\!\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\n        - \\sqrt{\\left(\\dfrac{4+\\pi^2\\rho}{\\pi}\\right)^{\\!2} - 32\\rho\\,U}\\right),\n        &amp; 0 \\le U \\le \\tfrac{1}{2}, \\\\[10pt]\n        \\pi + \\dfrac{-\\,(4-\\pi^2\\rho) + \\sqrt{(4-\\pi^2\\rho)^{2} + 32\\pi^{2}\\rho\\,(U-\\tfrac{1}{2})}}\n        {2\\pi\\rho},\n        &amp; \\tfrac{1}{2} &lt; U &lt; 1.\n    \\end{cases}\n    $$\n\n    Parameters\n    ----------\n    rho : float, optional\n        Concentration, $0 \\le \\rho \\le 4/\\pi^2$. Supply explicitly or by\n        freezing the distribution.\n    size : int or tuple of ints, optional\n        Output shape. If ``None`` (default), return a single scalar.\n    random_state : int, numpy.random.Generator, numpy.random.RandomState, optional\n        PRNG seed or generator. If ``None``, use the distribution's internal RNG.\n\n    Returns\n    -------\n    samples : ndarray or float\n        Angles in radians on $[0, 2\\pi)$, with shape ``size``.\n\n    Notes\n    -----\n    This is equivalent in law to R's **circular** `rtriangular` after\n    shifting its output by $+\\pi$ modulo $2\\pi$.\n    \"\"\"\n    rho_val = getattr(self, \"rho\", None) if rho is None else rho\n    if rho_val is None:\n        raise ValueError(\"'rho' must be provided.\")\n    return self._rvs(rho_val, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.triangular_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', return_info=False)</code>","text":"<p>Estimate the concentration parameter \\(\\rho\\) of the circular triangular law on \\([0,2\\pi)\\).</p> <p>Functions:</p> Name Description <code>mle</code> <p>maximize the log-likelihood. This solves the 1-D score equation \\(\\sum_i \\frac{c_i}{4+\\rho\\,c_i}=0\\) with \\(c_i = 2\\pi\\,|\\,\\pi-x_i\\,| - \\pi^2\\). Unique solution in \\([0, 4/\\pi^2)\\) or at a boundary.</p> <code>moments :</code> <p>closed-form \\(\\hat\\rho = \\max\\{0, \\min\\{4/\\pi^2,\\ \\overline{\\cos x}\\}\\}\\).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample angles (radians). Values are wrapped to \\([0, 2\\pi)\\) internally.</p> required <code>weights</code> <code>array_like</code> <p>Nonnegative sample weights. Broadcastable to <code>data</code>. Interpreted as frequencies.</p> <code>None</code> <code>method</code> <code>(mle, moments)</code> <p>Estimation method (see above).</p> <code>\"mle\",\"moments\"</code> <code>return_info</code> <code>bool</code> <p>If True, also return a dict with diagnostics (loglik, se, n_effective, method).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>rho_hat</code> <code>float</code> <p>Estimated concentration \\(\\hat\\rho \\in [0, 4/\\pi^2]\\).</p> <code>info</code> <code>(dict, optional)</code> <p>Returned only if <code>return_info=True</code>. Contains keys: {\"loglik\", \"se\", \"n_effective\", \"method\", \"converged\"}.</p> Notes <p>For this distribution \\(\\mathbb{E}[\\cos \\Theta]=\\rho\\), so the method-of-moments estimator is simply the (weighted) mean of \\(\\cos x\\) clipped to \\([0,4/\\pi^2]\\). The MLE solves a strictly monotone score equation, so bracketing root-finding is robust and \\(O(n)\\) per evaluation.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(self, data, *, weights=None, method=\"mle\", return_info=False):\n    r\"\"\"\n    Estimate the concentration parameter $\\rho$ of the circular triangular law on $[0,2\\pi)$.\n\n    Methods\n    -------\n\n    mle (default): \n        maximize the log-likelihood. This solves the 1-D score equation\n        $\\sum_i \\frac{c_i}{4+\\rho\\,c_i}=0$ with $c_i = 2\\pi\\,|\\,\\pi-x_i\\,| - \\pi^2$.\n        Unique solution in $[0, 4/\\pi^2)$ or at a boundary.\n    moments :\n        closed-form $\\hat\\rho = \\max\\{0, \\min\\{4/\\pi^2,\\ \\overline{\\cos x}\\}\\}$.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample angles (radians). Values are wrapped to $[0, 2\\pi)$ internally.\n    weights : array_like, optional\n        Nonnegative sample weights. Broadcastable to `data`. Interpreted as frequencies.\n    method : {\"mle\",\"moments\"}, optional\n        Estimation method (see above).\n    return_info : bool, optional\n        If True, also return a dict with diagnostics (loglik, se, n_effective, method).\n\n    Returns\n    -------\n    rho_hat : float\n        Estimated concentration $\\hat\\rho \\in [0, 4/\\pi^2]$.\n    info : dict, optional\n        Returned only if `return_info=True`. Contains keys:\n        {\"loglik\", \"se\", \"n_effective\", \"method\", \"converged\"}.\n\n    Notes\n    -----\n    For this distribution $\\mathbb{E}[\\cos \\Theta]=\\rho$, so the method-of-moments\n    estimator is simply the (weighted) mean of $\\cos x$ clipped to $[0,4/\\pi^2]$.\n    The MLE solves a strictly monotone score equation, so bracketing root-finding\n    is robust and $O(n)$ per evaluation.\n    \"\"\"\n    x = np.asarray(data, dtype=float)\n    x = np.mod(x, 2*np.pi)\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"weights must be nonnegative\")\n        # broadcast\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n    # Effective sample size for diagnostics\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"sum of weights must be positive\")\n    w_norm = w / w_sum\n    n_eff = w_sum**2 / np.sum(w**2)  # Kish effective n\n\n    # Method-of-moments (always available; used as fallback/initial intuition)\n    r_bar = float(np.sum(w_norm * np.cos(x)))\n    rho_mom = float(np.clip(r_bar, 0.0, 4/np.pi**2))\n\n    if method == \"moments\":\n        rho_hat = rho_mom\n        # log-likelihood at MoM (for info only)\n        y = np.abs(np.pi - x)\n        ll = float(np.sum(w * np.log(4 - np.pi**2 * rho_hat + 2*np.pi * rho_hat * y)) - w_sum*np.log(8*np.pi))\n        # observed Fisher info for SE\n        c = 2*np.pi*y - np.pi**2\n        info_obs = float(np.sum(w * (c**2) / (4 + rho_hat*c)**2))\n        se = (1.0 / np.sqrt(info_obs)) if info_obs &gt; 0 else np.nan\n        if return_info:\n            return rho_hat, {\"loglik\": ll, \"se\": se, \"n_effective\": n_eff, \"method\": \"moments\", \"converged\": True}\n        return rho_hat\n\n    # --- MLE via monotone root of the score ---\n    y = np.abs(np.pi - x)                 # in [0, \u03c0]\n    c = 2*np.pi*y - np.pi**2              # in [-\u03c0^2, \u03c0^2]\n\n    def score(rho):\n        return float(np.sum(w * (c / (4.0 + rho * c))))\n\n    # Bracket: score(\u03c1) is strictly decreasing on [0, \u03c1_max)\n    rho_lo = 0.0\n    rho_hi = float(4/np.pi**2) - 1e-12\n\n    s_lo = score(rho_lo)  # = (1/4) * sum w*c\n    if s_lo &lt;= 0:         # likelihood decreasing at 0 \u2192 boundary optimum\n        rho_hat = 0.0\n        converged = True\n    else:\n        s_hi = score(rho_hi)  # tends negative if any y_i\u22480\n        if s_hi &gt;= 0:\n            # all mass far from \u03c0 (extreme case) \u2192 boundary at \u03c1_max\n            rho_hat = rho_hi\n            converged = True\n        else:\n            # Unique root inside (0, \u03c1_max)\n            rho_hat = float(brentq(score, rho_lo, rho_hi, xtol=1e-12, rtol=1e-12, maxiter=256))\n            converged = True\n\n    # Diagnostics\n    ll = float(np.sum(w * np.log(4 - np.pi**2 * rho_hat + 2*np.pi * rho_hat * y)) - w_sum*np.log(8*np.pi))\n    info_obs = float(np.sum(w * (c**2) / (4 + rho_hat*c)**2))\n    se = (1.0 / np.sqrt(info_obs)) if info_obs &gt; 0 else np.nan\n\n    if return_info:\n        return rho_hat, {\"loglik\": ll, \"se\": se, \"n_effective\": n_eff, \"method\": \"mle\", \"converged\": converged}\n    return rho_hat\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cardioid_gen","title":"<code>cardioid_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Cardioid (cosine) Distribution</p> <p></p> <p>A cosine-modulated perturbation of the circular uniform law with support on <code>[0, 2\u03c0)</code>. The mean direction <code>mu</code> controls location, while the mean resultant length <code>rho</code> (bounded by 0.5) governs concentration. Closed-form expressions are used for the PDF and CDF, and quantiles are obtained by solving <code>F(theta; mu, rho) = q</code> with a safeguarded Halley--Newton iteration shared by <code>ppf</code> and <code>rvs</code>.</p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Percent-point function (inverse CDF).</p> <code>rvs</code> <p>Random variates via inverse transform using the quantile solver.</p> <code>fit</code> <p>Estimate <code>(mu, rho)</code> via method-of-moments or maximum likelihood.</p> Notes <p>Implementation based on Section 4.3.4 of Pewsey et al. (2014).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class cardioid_gen(CircularContinuous):\n    r\"\"\"Cardioid (cosine) Distribution\n\n    ![cardioid](../images/circ-mod-cardioid.png)\n\n    A cosine-modulated perturbation of the circular uniform law with support on\n    ``[0, 2\u03c0)``. The mean direction ``mu`` controls location, while the mean\n    resultant length ``rho`` (bounded by 0.5) governs concentration. Closed-form\n    expressions are used for the PDF and CDF, and quantiles are obtained by\n    solving ``F(theta; mu, rho) = q`` with a safeguarded Halley--Newton iteration\n    shared by ``ppf`` and ``rvs``.\n\n    Methods\n    -------\n    pdf(x, mu, rho)\n        Probability density function.\n    cdf(x, mu, rho)\n        Cumulative distribution function.\n    ppf(q, mu, rho)\n        Percent-point function (inverse CDF).\n    rvs(mu, rho, size=None, random_state=None)\n        Random variates via inverse transform using the quantile solver.\n    fit(data, *args, **kwargs)\n        Estimate ``(mu, rho)`` via method-of-moments or maximum likelihood.\n\n    Notes\n    -----\n    Implementation based on Section 4.3.4 of Pewsey et al. (2014).\n    \"\"\"\n\n    def _argcheck(self, mu, rho):\n        try:\n            mu_arr, rho_arr = np.broadcast_arrays(mu, rho)\n        except ValueError:\n            return False\n        return (\n            (mu_arr &gt;= 0.0)\n            &amp; (mu_arr &lt;= 2.0 * np.pi)\n            &amp; (rho_arr &gt;= 0.0)\n            &amp; (rho_arr &lt;= 0.5)\n        )\n\n    def _pdf(self, x, mu, rho):\n        return (1 + 2 * rho * np.cos(x - mu)) / 2.0 / np.pi\n\n    def pdf(self, x, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Cardioid distribution.\n\n        $$\n        f(\\theta) = \\frac{1}{2\\pi} \\left(1 + 2\\rho \\cos(\\theta - \\mu)\\right), \\space \\rho \\in [0, 1/2]\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Mean resultant length, 0 &lt;= rho &lt;= 0.5.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n        return super().pdf(x, mu, rho, *args, **kwargs)\n\n    def _cdf(self, x, mu, rho):\n        return (x + 2 * rho * (np.sin(x - mu) + np.sin(mu))) / (2 * np.pi)\n\n    def cdf(self, x, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the Cardioid distribution.\n\n        $$\n        F(\\theta) = \\frac{\\theta + 2\\rho (\\sin(\\mu) + \\sin(\\theta - \\mu))}{2\\pi}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Mean resultant length, 0 &lt;= rho &lt;= 0.5.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative distribution function evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, mu, rho, *args, **kwargs)\n\n    def _solve_inverse_cdf(self, probabilities, mu_val, rho_val):\n        two_pi = 2.0 * np.pi\n        probs = np.asarray(probabilities, dtype=float)\n\n        if probs.size == 0:\n            return probs.astype(float)\n\n        sin_mu = np.sin(mu_val)\n\n        if np.isclose(rho_val, 0.0, atol=1e-15):\n            result = two_pi * probs\n            if result.ndim == 0:\n                value = float(result)\n                return two_pi if np.isclose(float(probs), 1.0, rtol=0.0, atol=1e-12) else value\n            mask_one = np.isclose(probs, 1.0, rtol=0.0, atol=1e-12)\n            if np.any(mask_one):\n                result = result.copy()\n                result[mask_one] = two_pi\n            return result\n\n        theta = mu_val + two_pi * (probs - 0.5)\n        theta = np.mod(theta, two_pi)\n        theta = np.asarray(theta, dtype=float)\n\n        tol = 1e-12\n        tiny = 1e-14\n        use_halley = rho_val &gt; 0.25\n        max_iter = 6 if use_halley else 3\n\n        for iteration in range(max_iter):\n            delta = (\n                theta + 2.0 * rho_val * (np.sin(theta - mu_val) + sin_mu)\n            ) / two_pi - probs\n\n            converged = np.abs(delta) &lt;= tol\n            if np.all(converged):\n                break\n\n            d1 = (1.0 + 2.0 * rho_val * np.cos(theta - mu_val)) / two_pi\n            d2 = (-2.0 * rho_val * np.sin(theta - mu_val)) / two_pi\n\n            step_newton = np.divide(\n                delta,\n                d1,\n                out=np.zeros_like(delta, dtype=float),\n                where=np.abs(d1) &gt; tiny,\n            )\n\n            if iteration == 0 and use_halley:\n                denom = 2.0 * d1**2 - delta * d2\n                halley_valid = np.abs(denom) &gt; tiny\n                step_halley = np.divide(\n                    2.0 * delta * d1,\n                    denom,\n                    out=np.zeros_like(delta, dtype=float),\n                    where=halley_valid,\n                )\n                step = np.where(halley_valid, step_halley, step_newton)\n            else:\n                step = step_newton\n\n            step = np.clip(step, -np.pi, np.pi)\n            theta = np.where(converged, theta, theta - step)\n            theta = np.mod(theta, two_pi)\n\n        delta = (\n            theta + 2.0 * rho_val * (np.sin(theta - mu_val) + sin_mu)\n        ) / two_pi - probs\n        remaining = np.abs(delta) &gt; 10.0 * tol\n        if np.any(remaining):\n            theta_shape = theta.shape\n            theta_flat = theta.reshape(-1)\n            probs_flat = probs.reshape(-1)\n            remaining_flat = remaining.reshape(-1)\n            target = probs_flat[remaining_flat]\n            low = np.zeros_like(target)\n            high = np.full_like(target, two_pi)\n            for _ in range(32):\n                mid = 0.5 * (low + high)\n                f_mid = (\n                    mid + 2.0 * rho_val * (np.sin(mid - mu_val) + sin_mu)\n                ) / two_pi\n                mask_low = f_mid &lt;= target\n                low = np.where(mask_low, mid, low)\n                high = np.where(mask_low, high, mid)\n            theta_flat[remaining_flat] = 0.5 * (low + high)\n            theta = theta_flat.reshape(theta_shape)\n\n        result = np.mod(theta, two_pi)\n        if result.ndim == 0:\n            value = float(result)\n            return two_pi if np.isclose(float(probs), 1.0, rtol=0.0, atol=1e-12) else value\n\n        mask_one = np.isclose(probs, 1.0, rtol=0.0, atol=1e-12)\n        if np.any(mask_one):\n            result = result.copy()\n            result[mask_one] = two_pi\n        return result\n\n    def _ppf(self, q, mu, rho):\n        mu_arr = np.asarray(mu, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        rho_val = float(rho_arr.reshape(-1)[0])\n        if not (0.0 &lt;= rho_val &lt;= 0.5):\n            raise ValueError(\"`rho` must lie in [0, 0.5].\")\n\n        q_arr = np.asarray(q, dtype=float)\n        if q_arr.size == 0:\n            return q_arr.astype(float)\n\n        flat = q_arr.reshape(-1)\n        result = np.full_like(flat, np.nan, dtype=float)\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if np.any(valid):\n            solved = np.asarray(\n                self._solve_inverse_cdf(flat[valid], mu_val, rho_val),\n                dtype=float,\n            ).reshape(-1)\n            result[valid] = solved\n\n        result = result.reshape(q_arr.shape)\n        if q_arr.ndim == 0:\n            return float(result)\n        return result\n\n    def ppf(self, q, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (inverse CDF) of the Cardioid distribution.\n\n        The quantile $\\theta$ solves\n\n        $$\n        F(\\theta) = \\frac{\\theta + 2\\rho\\bigl(\\sin\\mu + \\sin(\\theta - \\mu)\\bigr)}{2\\pi} = q,\n        $$\n\n        on the support $[0, 2\\pi]$.  The implementation applies a\n        Halley--Newton iteration with adaptive clipping and a final bisection\n        safeguard, ensuring robustness for large $\\rho$ and quantiles\n        close to the boundary.  The same solver powers ``rvs``, so sampled\n        variates and tabulated quantiles are numerically consistent.\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles to evaluate; finite values in ``[0, 1]`` are supported.\n        mu : float\n            Mean direction, ``0 &lt;= mu &lt;= 2*pi``.\n        rho : float\n            Mean resultant length, ``0 &lt;= rho &lt;= 0.5``.\n\n        Returns\n        -------\n        ppf_values : array_like\n            Angles satisfying $F(\\theta)=q$. Inputs outside ``[0, 1]`` are\n            returned as ``nan``.\n        \"\"\"\n        return super().ppf(q, mu, rho, *args, **kwargs)\n\n    def _rvs(self, mu, rho, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n        if mu_arr.size != 1 or rho_arr.size != 1:\n            raise ValueError(\"cardioid parameters must be scalar-valued.\")\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        rho_val = float(rho_arr.reshape(-1)[0])\n        if not (0.0 &lt;= rho_val &lt;= 0.5):\n            raise ValueError(\"`rho` must lie in [0, 0.5].\")\n\n        two_pi = 2.0 * np.pi\n\n        if np.isclose(rho_val, 0.0, atol=1e-15):\n            samples = rng.uniform(0.0, two_pi, size=size)\n            return float(samples) if np.isscalar(samples) else samples\n\n        u = rng.uniform(0.0, 1.0, size=size)\n        samples = self._solve_inverse_cdf(u, mu_val, rho_val)\n        return float(samples) if np.isscalar(samples) else np.asarray(samples, dtype=float)\n\n\n    def rvs(self, mu=None, rho=None, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the Cardioid distribution.\n\n        Each sample is obtained by inverse-transform sampling.  For a uniform\n        draw $U \\sim \\mathcal{U}(0, 1)$, the angle $\\Theta$\n        satisfies\n\n        $$\n        \\frac{\\Theta + 2\\rho\\bigl(\\sin\\mu + \\sin(\\Theta - \\mu)\\bigr)}{2\\pi} = U,\n        $$\n\n        and is computed with the safeguarded Halley--Newton solver described in\n        ``ppf``.  When $\\rho = 0$, the distribution degenerates to the\n        circular uniform law and samples are drawn directly from ``[0, 2\u03c0)``.\n\n        Parameters\n        ----------\n        mu : float, optional\n            Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n            freezing the distribution.\n        rho : float, optional\n            Mean resultant length, ``0 &lt;= rho &lt;= 0.5``. Supply explicitly or by\n            freezing the distribution.\n        size : int or tuple of ints, optional\n            Number of samples to draw. ``None`` (default) returns a scalar.\n        random_state : np.random.Generator, np.random.RandomState, or None, optional\n            Random number generator to use.\n\n        Returns\n        -------\n        samples : ndarray or float\n            Random variates on ``[0, 2\u03c0)``.\n        \"\"\"\n        mu_val = getattr(self, \"mu\", None) if mu is None else mu\n        rho_val = getattr(self, \"rho\", None) if rho is None else rho\n\n        if mu_val is None or rho_val is None:\n            raise ValueError(\"Both 'mu' and 'rho' must be provided.\")\n\n        return self._rvs(mu_val, rho_val, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        return_info=False,\n        optimizer=\"L-BFGS-B\",\n        **kwargs,\n    ):\n        \"\"\"\n        Estimate ``mu`` and ``rho`` for the cardioid distribution.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n        weights : array_like, optional\n            Non-negative weights/frequencies broadcastable to ``data``.\n        method : {\\\"mle\\\", \\\"moments\\\"}, optional\n            Estimation strategy. ``\"moments\"`` uses the first trigonometric\n            moment, ``\"mle\"`` (default) maximises the weighted log-likelihood.\n        return_info : bool, optional\n            If True, also return a diagnostic dictionary.\n        optimizer : str, optional\n            Optimiser passed to ``scipy.optimize.minimize`` when\n            ``method=\"mle\"``.\n        **kwargs :\n            Additional keyword arguments forwarded to the optimiser.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float))\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        mu_mom, r_mom = circ_mean_and_r(alpha=x, w=w)\n        if not np.isfinite(mu_mom):\n            mu_mom = float(0.0)\n        mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n        rho_mom = float(np.clip(r_mom, 0.0, 0.5))\n\n        def _nll(params):\n            mu_param, rho_param = params\n            if not (0.0 &lt;= rho_param &lt;= 0.5):\n                return np.inf\n            cos_term = np.cos(x - mu_param)\n            denom = 1.0 + 2.0 * rho_param * cos_term\n            if np.any(denom &lt;= 0.0):\n                return np.inf\n            log_terms = np.log(denom)\n            value = -np.sum(w * log_terms) + w_sum * np.log(2.0 * np.pi)\n            return float(value)\n\n        def _grad(params):\n            mu_param, rho_param = params\n            cos_term = np.cos(x - mu_param)\n            denom = 1.0 + 2.0 * rho_param * cos_term\n            mask_bad = denom &lt;= 0.0\n            if np.any(mask_bad):\n                return np.array([0.0, 0.0], dtype=float)\n            sin_term = np.sin(x - mu_param)\n            inv = w / denom\n            g_mu = -2.0 * rho_param * np.sum(inv * sin_term)\n            g_rho = -2.0 * np.sum(inv * cos_term)\n            return np.array([g_mu, g_rho], dtype=float)\n\n        method = method.lower()\n        if method not in {\"mle\", \"moments\"}:\n            raise ValueError(\"`method` must be either 'mle' or 'moments'.\")\n\n        if method == \"moments\":\n            mu_hat = self._wrap_direction(mu_mom)\n            rho_hat = rho_mom\n            info = {\n                \"method\": \"moments\",\n                \"loglik\": float(-_nll((mu_hat, rho_hat))),\n                \"n_effective\": float(n_eff),\n                \"converged\": True,\n            }\n        else:\n            if rho_mom &lt;= 1e-12:\n                mu_hat = self._wrap_direction(mu_mom)\n                rho_hat = 0.0\n                info = {\n                    \"method\": \"mle\",\n                    \"loglik\": float(-_nll((mu_hat, rho_hat))),\n                    \"n_effective\": float(n_eff),\n                    \"converged\": True,\n                    \"nit\": 0,\n                    \"message\": \"Degenerate start (rho\u22480); returning boundary solution.\",\n                }\n            else:\n                init = np.array([mu_mom, rho_mom], dtype=float)\n                bounds = [(0.0, 2.0 * np.pi), (0.0, 0.5)]\n                result = minimize(\n                    _nll,\n                    init,\n                    method=optimizer,\n                    jac=_grad,\n                    bounds=bounds,\n                    **kwargs,\n                )\n                if not result.success:\n                    raise RuntimeError(\n                        f\"cardioid.fit(method='mle') failed: {result.message}\"\n                    )\n                mu_hat = self._wrap_direction(float(result.x[0]))\n                rho_hat = float(np.clip(result.x[1], 0.0, 0.5))\n                info = {\n                    \"method\": \"mle\",\n                    \"loglik\": float(-result.fun),\n                    \"n_effective\": float(n_eff),\n                    \"converged\": bool(result.success),\n                    \"nit\": result.nit,\n                    \"grad_norm\": float(np.linalg.norm(result.jac))\n                    if getattr(result, \"jac\", None) is not None\n                    else np.nan,\n                    \"optimizer\": optimizer,\n                }\n\n        estimates = (mu_hat, rho_hat)\n        if return_info:\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cardioid_gen.pdf","title":"<code>pdf(x, mu, rho, *args, **kwargs)</code>","text":"<p>Probability density function of the Cardioid distribution.</p> \\[ f(\\theta) = \\frac{1}{2\\pi} \\left(1 + 2\\rho \\cos(\\theta - \\mu)\\right), \\space \\rho \\in [0, 1/2] \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Mean resultant length, 0 &lt;= rho &lt;= 0.5.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Cardioid distribution.\n\n    $$\n    f(\\theta) = \\frac{1}{2\\pi} \\left(1 + 2\\rho \\cos(\\theta - \\mu)\\right), \\space \\rho \\in [0, 1/2]\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Mean resultant length, 0 &lt;= rho &lt;= 0.5.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n    return super().pdf(x, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cardioid_gen.cdf","title":"<code>cdf(x, mu, rho, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Cardioid distribution.</p> \\[ F(\\theta) = \\frac{\\theta + 2\\rho (\\sin(\\mu) + \\sin(\\theta - \\mu))}{2\\pi} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Mean resultant length, 0 &lt;= rho &lt;= 0.5.</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative distribution function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the Cardioid distribution.\n\n    $$\n    F(\\theta) = \\frac{\\theta + 2\\rho (\\sin(\\mu) + \\sin(\\theta - \\mu))}{2\\pi}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Mean resultant length, 0 &lt;= rho &lt;= 0.5.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative distribution function evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cardioid_gen.ppf","title":"<code>ppf(q, mu, rho, *args, **kwargs)</code>","text":"<p>Percent-point function (inverse CDF) of the Cardioid distribution.</p> <p>The quantile \\(\\theta\\) solves</p> \\[ F(\\theta) = \\frac{\\theta + 2\\rho\\bigl(\\sin\\mu + \\sin(\\theta - \\mu)\\bigr)}{2\\pi} = q, \\] <p>on the support \\([0, 2\\pi]\\).  The implementation applies a Halley--Newton iteration with adaptive clipping and a final bisection safeguard, ensuring robustness for large \\(\\rho\\) and quantiles close to the boundary.  The same solver powers <code>rvs</code>, so sampled variates and tabulated quantiles are numerically consistent.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles to evaluate; finite values in <code>[0, 1]</code> are supported.</p> required <code>mu</code> <code>float</code> <p>Mean direction, <code>0 &lt;= mu &lt;= 2*pi</code>.</p> required <code>rho</code> <code>float</code> <p>Mean resultant length, <code>0 &lt;= rho &lt;= 0.5</code>.</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Angles satisfying \\(F(\\theta)=q\\). Inputs outside <code>[0, 1]</code> are returned as <code>nan</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (inverse CDF) of the Cardioid distribution.\n\n    The quantile $\\theta$ solves\n\n    $$\n    F(\\theta) = \\frac{\\theta + 2\\rho\\bigl(\\sin\\mu + \\sin(\\theta - \\mu)\\bigr)}{2\\pi} = q,\n    $$\n\n    on the support $[0, 2\\pi]$.  The implementation applies a\n    Halley--Newton iteration with adaptive clipping and a final bisection\n    safeguard, ensuring robustness for large $\\rho$ and quantiles\n    close to the boundary.  The same solver powers ``rvs``, so sampled\n    variates and tabulated quantiles are numerically consistent.\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles to evaluate; finite values in ``[0, 1]`` are supported.\n    mu : float\n        Mean direction, ``0 &lt;= mu &lt;= 2*pi``.\n    rho : float\n        Mean resultant length, ``0 &lt;= rho &lt;= 0.5``.\n\n    Returns\n    -------\n    ppf_values : array_like\n        Angles satisfying $F(\\theta)=q$. Inputs outside ``[0, 1]`` are\n        returned as ``nan``.\n    \"\"\"\n    return super().ppf(q, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cardioid_gen.rvs","title":"<code>rvs(mu=None, rho=None, size=None, random_state=None)</code>","text":"<p>Draw random variates from the Cardioid distribution.</p> <p>Each sample is obtained by inverse-transform sampling.  For a uniform draw \\(U \\sim \\mathcal{U}(0, 1)\\), the angle \\(\\Theta\\) satisfies</p> \\[ \\frac{\\Theta + 2\\rho\\bigl(\\sin\\mu + \\sin(\\Theta - \\mu)\\bigr)}{2\\pi} = U, \\] <p>and is computed with the safeguarded Halley--Newton solver described in <code>ppf</code>.  When \\(\\rho = 0\\), the distribution degenerates to the circular uniform law and samples are drawn directly from <code>[0, 2\u03c0)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Mean direction, <code>0 &lt;= mu &lt;= 2*pi</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>rho</code> <code>float</code> <p>Mean resultant length, <code>0 &lt;= rho &lt;= 0.5</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>size</code> <code>int or tuple of ints</code> <p>Number of samples to draw. <code>None</code> (default) returns a scalar.</p> <code>None</code> <code>random_state</code> <code>np.random.Generator, np.random.RandomState, or None</code> <p>Random number generator to use.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray or float</code> <p>Random variates on <code>[0, 2\u03c0)</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, mu=None, rho=None, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the Cardioid distribution.\n\n    Each sample is obtained by inverse-transform sampling.  For a uniform\n    draw $U \\sim \\mathcal{U}(0, 1)$, the angle $\\Theta$\n    satisfies\n\n    $$\n    \\frac{\\Theta + 2\\rho\\bigl(\\sin\\mu + \\sin(\\Theta - \\mu)\\bigr)}{2\\pi} = U,\n    $$\n\n    and is computed with the safeguarded Halley--Newton solver described in\n    ``ppf``.  When $\\rho = 0$, the distribution degenerates to the\n    circular uniform law and samples are drawn directly from ``[0, 2\u03c0)``.\n\n    Parameters\n    ----------\n    mu : float, optional\n        Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n        freezing the distribution.\n    rho : float, optional\n        Mean resultant length, ``0 &lt;= rho &lt;= 0.5``. Supply explicitly or by\n        freezing the distribution.\n    size : int or tuple of ints, optional\n        Number of samples to draw. ``None`` (default) returns a scalar.\n    random_state : np.random.Generator, np.random.RandomState, or None, optional\n        Random number generator to use.\n\n    Returns\n    -------\n    samples : ndarray or float\n        Random variates on ``[0, 2\u03c0)``.\n    \"\"\"\n    mu_val = getattr(self, \"mu\", None) if mu is None else mu\n    rho_val = getattr(self, \"rho\", None) if rho is None else rho\n\n    if mu_val is None or rho_val is None:\n        raise ValueError(\"Both 'mu' and 'rho' must be provided.\")\n\n    return self._rvs(mu_val, rho_val, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cardioid_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', return_info=False, optimizer='L-BFGS-B', **kwargs)</code>","text":"<p>Estimate <code>mu</code> and <code>rho</code> for the cardioid distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample angles (radians). Values are wrapped to <code>[0, 2\u03c0)</code> internally.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights/frequencies broadcastable to <code>data</code>.</p> <code>None</code> <code>method</code> <code>(mle, moments)</code> <p>Estimation strategy. <code>\"moments\"</code> uses the first trigonometric moment, <code>\"mle\"</code> (default) maximises the weighted log-likelihood.</p> <code>\"mle\"</code> <code>return_info</code> <code>bool</code> <p>If True, also return a diagnostic dictionary.</p> <code>False</code> <code>optimizer</code> <code>str</code> <p>Optimiser passed to <code>scipy.optimize.minimize</code> when <code>method=\"mle\"</code>.</p> <code>'L-BFGS-B'</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to the optimiser.</p> <code>{}</code> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    return_info=False,\n    optimizer=\"L-BFGS-B\",\n    **kwargs,\n):\n    \"\"\"\n    Estimate ``mu`` and ``rho`` for the cardioid distribution.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n    weights : array_like, optional\n        Non-negative weights/frequencies broadcastable to ``data``.\n    method : {\\\"mle\\\", \\\"moments\\\"}, optional\n        Estimation strategy. ``\"moments\"`` uses the first trigonometric\n        moment, ``\"mle\"`` (default) maximises the weighted log-likelihood.\n    return_info : bool, optional\n        If True, also return a diagnostic dictionary.\n    optimizer : str, optional\n        Optimiser passed to ``scipy.optimize.minimize`` when\n        ``method=\"mle\"``.\n    **kwargs :\n        Additional keyword arguments forwarded to the optimiser.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float))\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    mu_mom, r_mom = circ_mean_and_r(alpha=x, w=w)\n    if not np.isfinite(mu_mom):\n        mu_mom = float(0.0)\n    mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n    rho_mom = float(np.clip(r_mom, 0.0, 0.5))\n\n    def _nll(params):\n        mu_param, rho_param = params\n        if not (0.0 &lt;= rho_param &lt;= 0.5):\n            return np.inf\n        cos_term = np.cos(x - mu_param)\n        denom = 1.0 + 2.0 * rho_param * cos_term\n        if np.any(denom &lt;= 0.0):\n            return np.inf\n        log_terms = np.log(denom)\n        value = -np.sum(w * log_terms) + w_sum * np.log(2.0 * np.pi)\n        return float(value)\n\n    def _grad(params):\n        mu_param, rho_param = params\n        cos_term = np.cos(x - mu_param)\n        denom = 1.0 + 2.0 * rho_param * cos_term\n        mask_bad = denom &lt;= 0.0\n        if np.any(mask_bad):\n            return np.array([0.0, 0.0], dtype=float)\n        sin_term = np.sin(x - mu_param)\n        inv = w / denom\n        g_mu = -2.0 * rho_param * np.sum(inv * sin_term)\n        g_rho = -2.0 * np.sum(inv * cos_term)\n        return np.array([g_mu, g_rho], dtype=float)\n\n    method = method.lower()\n    if method not in {\"mle\", \"moments\"}:\n        raise ValueError(\"`method` must be either 'mle' or 'moments'.\")\n\n    if method == \"moments\":\n        mu_hat = self._wrap_direction(mu_mom)\n        rho_hat = rho_mom\n        info = {\n            \"method\": \"moments\",\n            \"loglik\": float(-_nll((mu_hat, rho_hat))),\n            \"n_effective\": float(n_eff),\n            \"converged\": True,\n        }\n    else:\n        if rho_mom &lt;= 1e-12:\n            mu_hat = self._wrap_direction(mu_mom)\n            rho_hat = 0.0\n            info = {\n                \"method\": \"mle\",\n                \"loglik\": float(-_nll((mu_hat, rho_hat))),\n                \"n_effective\": float(n_eff),\n                \"converged\": True,\n                \"nit\": 0,\n                \"message\": \"Degenerate start (rho\u22480); returning boundary solution.\",\n            }\n        else:\n            init = np.array([mu_mom, rho_mom], dtype=float)\n            bounds = [(0.0, 2.0 * np.pi), (0.0, 0.5)]\n            result = minimize(\n                _nll,\n                init,\n                method=optimizer,\n                jac=_grad,\n                bounds=bounds,\n                **kwargs,\n            )\n            if not result.success:\n                raise RuntimeError(\n                    f\"cardioid.fit(method='mle') failed: {result.message}\"\n                )\n            mu_hat = self._wrap_direction(float(result.x[0]))\n            rho_hat = float(np.clip(result.x[1], 0.0, 0.5))\n            info = {\n                \"method\": \"mle\",\n                \"loglik\": float(-result.fun),\n                \"n_effective\": float(n_eff),\n                \"converged\": bool(result.success),\n                \"nit\": result.nit,\n                \"grad_norm\": float(np.linalg.norm(result.jac))\n                if getattr(result, \"jac\", None) is not None\n                else np.nan,\n                \"optimizer\": optimizer,\n            }\n\n    estimates = (mu_hat, rho_hat)\n    if return_info:\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cartwright_gen","title":"<code>cartwright_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Cartwright's Power-of-Cosine Distribution</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Percent-point function obtained by inverting the regularised incomplete beta.</p> <code>rvs</code> <p>Random variates via a Beta-to-angle transform consistent with the quantile.</p> <code>fit</code> <p>Estimate <code>(mu, zeta)</code> using moments or maximum likelihood.</p> Note <p>Implementation based on Section 4.3.5 of Pewsey et al. (2014)</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class cartwright_gen(CircularContinuous):\n    \"\"\"Cartwright's Power-of-Cosine Distribution\n\n    ![cartwright](../images/circ-mod-cartwright.png)\n\n\n    Methods\n    -------\n    pdf(x, mu, zeta)\n        Probability density function.\n\n    cdf(x, mu, zeta)\n        Cumulative distribution function.\n\n    ppf(q, mu, zeta)\n        Percent-point function obtained by inverting the regularised incomplete beta.\n\n    rvs(mu, zeta, size=None, random_state=None)\n        Random variates via a Beta-to-angle transform consistent with the quantile.\n\n    fit(data, *args, **kwargs)\n        Estimate ``(mu, zeta)`` using moments or maximum likelihood.\n\n    Note\n    ----\n    Implementation based on Section 4.3.5 of Pewsey et al. (2014)\n    \"\"\"\n\n    def _argcheck(self, mu, zeta):\n        try:\n            mu_arr, zeta_arr = np.broadcast_arrays(mu, zeta)\n        except ValueError:\n            return False\n        return (mu_arr &gt;= 0.0) &amp; (mu_arr &lt;= 2.0 * np.pi) &amp; (zeta_arr &gt; 0.0)\n\n    @staticmethod\n    def _moment_r(zeta):\n        z = np.asarray(zeta, dtype=float)\n        if np.any(z &lt;= 0):\n            raise ValueError(\"`zeta` must be positive.\")\n        inv = 1.0 / z\n        log_term = (-1.0 + 2.0 * inv) * np.log(2.0)\n        log_term += np.log(2.0)\n        log_term += np.log(inv**2 / (inv + 1.0))\n        log_term += gammaln(inv)\n        log_term += gammaln(inv + 0.5)\n        log_term -= 0.5 * np.log(np.pi)\n        log_term -= gammaln(1.0 + 2.0 * inv)\n        result = np.exp(log_term)\n        return float(result) if np.isscalar(zeta) else result\n\n    def _pdf(self, x, mu, zeta):\n        return (\n            (2 ** (-1 + 1 / zeta) * (gamma(1 + 1 / zeta)) ** 2)\n            * (1 + np.cos(x - mu)) ** (1 / zeta)\n            / (np.pi * gamma(1 + 2 / zeta))\n        )\n\n    def pdf(self, x, mu, zeta, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Cartwright distribution.\n\n        $$\n        f(\\theta) = \\frac{2^{- 1+1/\\zeta} \\Gamma^2(1 + 1/\\zeta)}{\\pi \\Gamma(1 + 2/\\zeta)} (1 + \\cos(\\theta - \\mu))^{1/\\zeta}\n        $$\n\n        , where $\\Gamma$ is the gamma function.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        zeta : float\n            Shape parameter, zeta &gt; 0.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n\n        return super().pdf(x, mu, zeta, *args, **kwargs)\n\n    @staticmethod\n    def _cartwright_cumulative(phi, a, b, half_norm):\n        phi_arr = np.asarray(phi, dtype=float)\n        scalar_input = np.isscalar(phi_arr)\n        phi_vec = np.atleast_1d(phi_arr)\n        two_pi = 2.0 * np.pi\n        result = np.empty_like(phi_vec, dtype=float)\n\n        mask_lower = phi_vec &lt;= np.pi\n        if np.any(mask_lower):\n            s_small = np.sin(0.5 * phi_vec[mask_lower]) ** 2\n            val = betainc(a, b, np.clip(s_small, 0.0, 1.0))\n            result[mask_lower] = half_norm * val\n\n        if np.any(~mask_lower):\n            phi_ref = two_pi - phi_vec[~mask_lower]\n            s_large = np.sin(0.5 * phi_ref) ** 2\n            val = betainc(a, b, np.clip(s_large, 0.0, 1.0))\n            result[~mask_lower] = 1.0 - half_norm * val\n\n        if scalar_input:\n            return float(result[0])\n        return result.reshape(phi_arr.shape)\n\n    def _cdf(self, x, mu, zeta):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n\n        if flat.size == 0:\n            return arr.astype(float)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        zeta_arr = np.asarray(zeta, dtype=float)\n        if mu_arr.size != 1 or zeta_arr.size != 1:\n            raise ValueError(\"cartwright parameters must be scalar-valued.\")\n\n        mu_val = float(mu_arr.reshape(-1)[0])\n        zeta_val = float(zeta_arr.reshape(-1)[0])\n        if zeta_val &lt;= 0.0:\n            raise ValueError(\"`zeta` must be positive.\")\n\n        two_pi = 2.0 * np.pi\n        a = 0.5\n        b = 1.0 / zeta_val + 0.5\n        const = (\n            2.0 ** (-1.0 + 1.0 / zeta_val)\n            * gamma(1.0 + 1.0 / zeta_val) ** 2\n            / (np.pi * gamma(1.0 + 2.0 / zeta_val))\n        )\n        beta_term = beta_fn(a, b)\n        half_norm = const * (2.0 ** (1.0 / zeta_val)) * beta_term  # equals 0.5\n        half_norm = float(np.clip(half_norm, np.finfo(float).tiny, None))\n\n        phi_start = (-mu_val) % two_pi\n        phi_end = (flat - mu_val) % two_pi\n\n        H_start = self._cartwright_cumulative(np.array([phi_start]), a, b, half_norm)[0]\n        H_end = self._cartwright_cumulative(phi_end, a, b, half_norm)\n\n        cdf = np.where(\n            phi_end &gt;= phi_start,\n            np.clip(H_end - H_start, 0.0, 1.0),\n            1.0 - np.clip(H_start - H_end, 0.0, 1.0),\n        )\n        negative = cdf &lt; 0.0\n        if np.any(negative):\n            cdf = np.where(negative, cdf + 1.0, cdf)\n        cdf = np.clip(cdf, 0.0, 1.0)\n\n        if arr.ndim == 0:\n            value = float(cdf[0])\n            return 1.0 if np.isclose(float(wrapped), 2.0 * np.pi) else value\n\n        result = cdf.reshape(arr.shape)\n        result[np.isclose(arr, 2.0 * np.pi)] = 1.0\n        return result\n\n    def cdf(self, x, mu, zeta, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the Cartwright distribution.\n\n        The CDF is evaluated analytically via a beta-function series,\n        exploiting the symmetry around the mean direction.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        zeta : float\n            Shape parameter, zeta &gt; 0.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative distribution function evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, mu, zeta, *args, **kwargs)\n\n    def _ppf(self, q, mu, zeta):\n        mu_arr = np.asarray(mu, dtype=float)\n        zeta_arr = np.asarray(zeta, dtype=float)\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        zeta_val = float(zeta_arr.reshape(-1)[0])\n        if zeta_val &lt;= 0.0:\n            raise ValueError(\"`zeta` must be positive.\")\n\n        q_arr = np.asarray(q, dtype=float)\n        if q_arr.size == 0:\n            return q_arr.astype(float)\n\n        two_pi = 2.0 * np.pi\n        a = 0.5\n        b = 1.0 / zeta_val + 0.5\n        const = (\n            2.0 ** (-1.0 + 1.0 / zeta_val)\n            * gamma(1.0 + 1.0 / zeta_val) ** 2\n            / (np.pi * gamma(1.0 + 2.0 / zeta_val))\n        )\n        half_norm = const * (2.0 ** (1.0 / zeta_val)) * beta_fn(a, b)\n        half_norm = float(np.clip(half_norm, np.finfo(float).tiny, None))\n\n        phi_start = (-mu_val) % two_pi\n        H_start = self._cartwright_cumulative(np.array([phi_start]), a, b, half_norm)[0]\n\n        flat = q_arr.reshape(-1)\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if np.any(valid):\n            q_valid = flat[valid]\n\n            # Handle exact boundary quantiles explicitly\n            close_zero = np.isclose(q_valid, 0.0, rtol=0.0, atol=1e-12)\n            close_one = np.isclose(q_valid, 1.0, rtol=0.0, atol=1e-12)\n\n            s = (H_start + q_valid) % 1.0\n\n            phi = np.empty_like(q_valid)\n            mask_lower = s &lt;= 0.5\n\n            if np.any(mask_lower):\n                u = np.clip(s[mask_lower] / half_norm, 0.0, 1.0)\n                t = betaincinv(a, b, np.clip(u, 0.0, 1.0))\n                t = np.clip(t, 0.0, 1.0)\n                phi[mask_lower] = 2.0 * np.arcsin(np.sqrt(t))\n\n            if np.any(~mask_lower):\n                s_upper = s[~mask_lower]\n                u = np.clip((1.0 - s_upper) / half_norm, 0.0, 1.0)\n                t = betaincinv(a, b, np.clip(u, 0.0, 1.0))\n                t = np.clip(t, 0.0, 1.0)\n                phi[~mask_lower] = two_pi - 2.0 * np.arcsin(np.sqrt(t))\n\n            theta = (mu_val + phi) % two_pi\n\n            if np.any(close_zero):\n                theta[close_zero] = float(np.mod(mu_val + phi_start, two_pi))\n            if np.any(close_one):\n                theta[close_one] = two_pi\n\n            result[valid] = theta\n\n        result = result.reshape(q_arr.shape)\n        if q_arr.ndim == 0:\n            return float(result)\n        return result\n\n    def ppf(self, q, mu, zeta, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (inverse CDF) of the Cartwright distribution.\n\n        The quantile inversion exploits the beta integral governing the CDF.\n        With\n        $$\n        t = \\sin^2\\!\\left(\\tfrac{1}{2}\\phi\\right), \\qquad\n        a = \\tfrac{1}{2}, \\qquad b = \\tfrac{1}{\\zeta} + \\tfrac{1}{2},\n        $$\n        the cumulative distribution reduces to\n        $$\n        H(\\phi) =\n        \\begin{cases}\n        \\tfrac{1}{2} I_t(a, b), &amp; 0 \\le \\phi \\le \\pi, \\\\[6pt]\n        1 - \\tfrac{1}{2} I_t(a, b), &amp; \\pi &lt; \\phi &lt; 2\\pi,\n        \\end{cases}\n        $$\n        where $I_t$ is the regularised incomplete beta function. The inverse\n        quantile solves $H(\\phi) = s$ via the inverse regularised incomplete\n        beta, ``betaincinv``, yielding the exact $O(1)$ mapping used here and in\n        ``rvs``.\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles to evaluate (0 &lt;= q &lt;= 1).\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        zeta : float\n            Shape parameter, zeta &gt; 0.\n\n        Returns\n        -------\n        ppf_values : array_like\n            Angles corresponding to the given quantiles.\n        \"\"\"\n        return super().ppf(q, mu, zeta, *args, **kwargs)\n\n    def _rvs(self, mu, zeta, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        zeta_arr = np.asarray(zeta, dtype=float)\n        if mu_arr.size != 1 or zeta_arr.size != 1:\n            raise ValueError(\"cartwright parameters must be scalar-valued.\")\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        zeta_val = float(zeta_arr.reshape(-1)[0])\n        if zeta_val &lt;= 0.0:\n            raise ValueError(\"`zeta` must be positive.\")\n\n        shape = ()\n        if size is not None:\n            if np.isscalar(size):\n                shape = (int(size),)\n            else:\n                shape = tuple(int(dim) for dim in np.atleast_1d(size))\n\n        beta_b = 1.0 / zeta_val + 0.5\n        t = rng.beta(0.5, beta_b, size=shape)\n        sqrt_t = np.sqrt(t)\n        angles = 2.0 * np.arcsin(np.clip(sqrt_t, 0.0, 1.0))\n\n        signs = np.where(rng.random(size=shape) &lt; 0.5, -1.0, 1.0)\n        theta = mu_val + signs * angles\n        theta = np.mod(theta, 2.0 * np.pi)\n\n        if theta.ndim == 0:\n            return float(theta)\n        return theta.reshape(shape)\n\n    def rvs(self, mu=None, zeta=None, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the Cartwright distribution.\n\n        Sampling follows the same Beta-to-angle transform as the quantile\n        function: draw $T \\sim \\mathrm{Beta}\\!\\left(\\tfrac{1}{2},\n        \\tfrac{1}{\\zeta} + \\tfrac{1}{2}\\right)$, map it via\n        $\\phi = 2\\arcsin(\\sqrt{T})$, then reflect $\\phi$ with equal probability\n        around $\\mu$. This construction keeps ``rvs`` numerically consistent\n        with ``ppf``.\n\n        Parameters\n        ----------\n        mu : float, optional\n            Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n            freezing the distribution.\n        zeta : float, optional\n            Shape parameter, ``zeta &gt; 0``. Supply explicitly or by freezing the\n            distribution.\n        size : int or tuple of ints, optional\n            Number of samples to draw. ``None`` (default) returns a scalar.\n        random_state : np.random.Generator, np.random.RandomState, or None, optional\n            Random number generator to use.\n\n        Returns\n        -------\n        samples : ndarray or float\n            Random variates on ``[0, 2\u03c0)``.\n        \"\"\"\n        mu_val = getattr(self, \"mu\", None) if mu is None else mu\n        zeta_val = getattr(self, \"zeta\", None) if zeta is None else zeta\n\n        if mu_val is None or zeta_val is None:\n            raise ValueError(\"Both 'mu' and 'zeta' must be provided.\")\n\n        return self._rvs(mu_val, zeta_val, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        return_info=False,\n        optimizer=\"L-BFGS-B\",\n        **kwargs,\n    ):\n        \"\"\"\n        Estimate ``mu`` and ``zeta`` for the Cartwright distribution.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n        weights : array_like, optional\n            Non-negative weights/frequencies broadcastable to ``data``.\n        method : {\"mle\", \"moments\"}, optional\n            Estimation strategy. \"moments\" matches the first trigonometric\n            moment, \"mle\" (default) maximises the weighted log-likelihood.\n        return_info : bool, optional\n            If True, also return a diagnostic dictionary.\n        optimizer : str, optional\n            Optimiser passed to ``scipy.optimize.minimize`` when\n            ``method=\"mle\"``.\n        **kwargs :\n            Additional keyword arguments forwarded to the optimiser.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float))\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        mu_mom, _ = circ_mean_and_r(alpha=x, w=w)\n        if not np.isfinite(mu_mom):\n            mu_mom = float(0.0)\n        mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n        delta = (x - mu_mom + np.pi) % (2.0 * np.pi) - np.pi\n        sin_half = np.sin(0.5 * delta)\n        m_t = float(np.sum(w * sin_half**2) / w_sum)\n        m_t = float(np.clip(m_t, 0.0, 0.5 - 1e-12))\n        if m_t &lt;= 1e-12:\n            zeta_mom = 1e-6\n        else:\n            denom = max(1e-12, 0.5 - m_t)\n            zeta_mom = float(np.clip(m_t / denom, 1e-6, 1e6))\n\n        def log_c(z):\n            inv = 1.0 / z\n            return (\n                (-1.0 + inv) * np.log(2.0)\n                + 2.0 * gammaln(1.0 + inv)\n                - np.log(np.pi)\n                - gammaln(1.0 + 2.0 * inv)\n            )\n\n        def nll(params):\n            mu_param, zeta_param = params\n            if zeta_param &lt;= 0.0:\n                return np.inf\n            cos_term = np.cos(x - mu_param)\n            denom = np.clip(1.0 + cos_term, 1e-15, None)\n            sum_log = np.sum(w * np.log(denom))\n            ll = w_sum * log_c(zeta_param) + (1.0 / zeta_param) * sum_log\n            return float(-ll)\n\n        def grad(params):\n            mu_param, zeta_param = params\n            cos_term = np.cos(x - mu_param)\n            denom = np.clip(1.0 + cos_term, 1e-15, None)\n            sin_term = np.sin(x - mu_param)\n            sum_log = np.sum(w * np.log(denom))\n            grad_mu = -(1.0 / zeta_param) * np.sum(w * sin_term / denom)\n            inv = 1.0 / zeta_param\n            term = 2.0 * digamma(1.0 + 2.0 * inv) - (\n                np.log(2.0) + 2.0 * digamma(1.0 + inv)\n            )\n            grad_zeta = (sum_log - w_sum * term) / (zeta_param**2)\n            return np.array([grad_mu, grad_zeta], dtype=float)\n\n        method = method.lower()\n        if method not in {\"mle\", \"moments\"}:\n            raise ValueError(\"`method` must be either 'mle' or 'moments'.\")\n\n        if method == \"moments\":\n            mu_hat = self._wrap_direction(mu_mom)\n            zeta_hat = zeta_mom\n            info = {\n                \"method\": \"moments\",\n                \"loglik\": float(-nll((mu_hat, zeta_hat))),\n                \"n_effective\": float(n_eff),\n                \"converged\": True,\n            }\n        else:\n            mu_init = mu_mom\n            zeta_init = zeta_mom if np.isfinite(zeta_mom) else 10.0\n            zeta_init = float(np.clip(zeta_init, 1e-3, 1e4))\n            bounds = [(0.0, 2.0 * np.pi), (1e-6, 1e6)]\n            result = minimize(\n                nll,\n                np.array([mu_init, zeta_init], dtype=float),\n                method=optimizer,\n                jac=grad,\n                bounds=bounds,\n                **kwargs,\n            )\n            if not result.success:\n                raise RuntimeError(\n                    f\"cartwright.fit(method='mle') failed: {result.message}\"\n                )\n            mu_hat = self._wrap_direction(float(result.x[0]))\n            zeta_hat = float(np.clip(result.x[1], 1e-6, 1e6))\n            info = {\n                \"method\": \"mle\",\n                \"loglik\": float(-result.fun),\n                \"n_effective\": float(n_eff),\n                \"converged\": bool(result.success),\n                \"nit\": result.nit,\n                \"grad_norm\": float(np.linalg.norm(result.jac))\n                if getattr(result, \"jac\", None) is not None\n                else np.nan,\n                \"optimizer\": optimizer,\n            }\n\n        estimates = (mu_hat, zeta_hat)\n        if return_info:\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cartwright_gen.pdf","title":"<code>pdf(x, mu, zeta, *args, **kwargs)</code>","text":"<p>Probability density function of the Cartwright distribution.</p> \\[ f(\\theta) = \\frac{2^{- 1+1/\\zeta} \\Gamma^2(1 + 1/\\zeta)}{\\pi \\Gamma(1 + 2/\\zeta)} (1 + \\cos(\\theta - \\mu))^{1/\\zeta} \\] <p>, where \\(\\Gamma\\) is the gamma function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>zeta</code> <code>float</code> <p>Shape parameter, zeta &gt; 0.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, zeta, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Cartwright distribution.\n\n    $$\n    f(\\theta) = \\frac{2^{- 1+1/\\zeta} \\Gamma^2(1 + 1/\\zeta)}{\\pi \\Gamma(1 + 2/\\zeta)} (1 + \\cos(\\theta - \\mu))^{1/\\zeta}\n    $$\n\n    , where $\\Gamma$ is the gamma function.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    zeta : float\n        Shape parameter, zeta &gt; 0.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n\n    return super().pdf(x, mu, zeta, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cartwright_gen.cdf","title":"<code>cdf(x, mu, zeta, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Cartwright distribution.</p> <p>The CDF is evaluated analytically via a beta-function series, exploiting the symmetry around the mean direction.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>zeta</code> <code>float</code> <p>Shape parameter, zeta &gt; 0.</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative distribution function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, zeta, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the Cartwright distribution.\n\n    The CDF is evaluated analytically via a beta-function series,\n    exploiting the symmetry around the mean direction.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    zeta : float\n        Shape parameter, zeta &gt; 0.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative distribution function evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, mu, zeta, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cartwright_gen.ppf","title":"<code>ppf(q, mu, zeta, *args, **kwargs)</code>","text":"<p>Percent-point function (inverse CDF) of the Cartwright distribution.</p> <p>The quantile inversion exploits the beta integral governing the CDF. With $$ t = \\sin^2!\\left(\\tfrac{1}{2}\\phi\\right), \\qquad a = \\tfrac{1}{2}, \\qquad b = \\tfrac{1}{\\zeta} + \\tfrac{1}{2}, $$ the cumulative distribution reduces to $$ H(\\phi) = \\begin{cases} \\tfrac{1}{2} I_t(a, b), &amp; 0 \\le \\phi \\le \\pi, \\[6pt] 1 - \\tfrac{1}{2} I_t(a, b), &amp; \\pi &lt; \\phi &lt; 2\\pi, \\end{cases} $$ where \\(I_t\\) is the regularised incomplete beta function. The inverse quantile solves \\(H(\\phi) = s\\) via the inverse regularised incomplete beta, <code>betaincinv</code>, yielding the exact \\(O(1)\\) mapping used here and in <code>rvs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles to evaluate (0 &lt;= q &lt;= 1).</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>zeta</code> <code>float</code> <p>Shape parameter, zeta &gt; 0.</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Angles corresponding to the given quantiles.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, mu, zeta, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (inverse CDF) of the Cartwright distribution.\n\n    The quantile inversion exploits the beta integral governing the CDF.\n    With\n    $$\n    t = \\sin^2\\!\\left(\\tfrac{1}{2}\\phi\\right), \\qquad\n    a = \\tfrac{1}{2}, \\qquad b = \\tfrac{1}{\\zeta} + \\tfrac{1}{2},\n    $$\n    the cumulative distribution reduces to\n    $$\n    H(\\phi) =\n    \\begin{cases}\n    \\tfrac{1}{2} I_t(a, b), &amp; 0 \\le \\phi \\le \\pi, \\\\[6pt]\n    1 - \\tfrac{1}{2} I_t(a, b), &amp; \\pi &lt; \\phi &lt; 2\\pi,\n    \\end{cases}\n    $$\n    where $I_t$ is the regularised incomplete beta function. The inverse\n    quantile solves $H(\\phi) = s$ via the inverse regularised incomplete\n    beta, ``betaincinv``, yielding the exact $O(1)$ mapping used here and in\n    ``rvs``.\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles to evaluate (0 &lt;= q &lt;= 1).\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    zeta : float\n        Shape parameter, zeta &gt; 0.\n\n    Returns\n    -------\n    ppf_values : array_like\n        Angles corresponding to the given quantiles.\n    \"\"\"\n    return super().ppf(q, mu, zeta, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cartwright_gen.rvs","title":"<code>rvs(mu=None, zeta=None, size=None, random_state=None)</code>","text":"<p>Draw random variates from the Cartwright distribution.</p> <p>Sampling follows the same Beta-to-angle transform as the quantile function: draw \\(T \\sim \\mathrm{Beta}\\!\\left(\\tfrac{1}{2}, \\tfrac{1}{\\zeta} + \\tfrac{1}{2}\\right)\\), map it via \\(\\phi = 2\\arcsin(\\sqrt{T})\\), then reflect \\(\\phi\\) with equal probability around \\(\\mu\\). This construction keeps <code>rvs</code> numerically consistent with <code>ppf</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Mean direction, <code>0 &lt;= mu &lt;= 2*pi</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>zeta</code> <code>float</code> <p>Shape parameter, <code>zeta &gt; 0</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>size</code> <code>int or tuple of ints</code> <p>Number of samples to draw. <code>None</code> (default) returns a scalar.</p> <code>None</code> <code>random_state</code> <code>np.random.Generator, np.random.RandomState, or None</code> <p>Random number generator to use.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray or float</code> <p>Random variates on <code>[0, 2\u03c0)</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, mu=None, zeta=None, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the Cartwright distribution.\n\n    Sampling follows the same Beta-to-angle transform as the quantile\n    function: draw $T \\sim \\mathrm{Beta}\\!\\left(\\tfrac{1}{2},\n    \\tfrac{1}{\\zeta} + \\tfrac{1}{2}\\right)$, map it via\n    $\\phi = 2\\arcsin(\\sqrt{T})$, then reflect $\\phi$ with equal probability\n    around $\\mu$. This construction keeps ``rvs`` numerically consistent\n    with ``ppf``.\n\n    Parameters\n    ----------\n    mu : float, optional\n        Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n        freezing the distribution.\n    zeta : float, optional\n        Shape parameter, ``zeta &gt; 0``. Supply explicitly or by freezing the\n        distribution.\n    size : int or tuple of ints, optional\n        Number of samples to draw. ``None`` (default) returns a scalar.\n    random_state : np.random.Generator, np.random.RandomState, or None, optional\n        Random number generator to use.\n\n    Returns\n    -------\n    samples : ndarray or float\n        Random variates on ``[0, 2\u03c0)``.\n    \"\"\"\n    mu_val = getattr(self, \"mu\", None) if mu is None else mu\n    zeta_val = getattr(self, \"zeta\", None) if zeta is None else zeta\n\n    if mu_val is None or zeta_val is None:\n        raise ValueError(\"Both 'mu' and 'zeta' must be provided.\")\n\n    return self._rvs(mu_val, zeta_val, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.cartwright_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', return_info=False, optimizer='L-BFGS-B', **kwargs)</code>","text":"<p>Estimate <code>mu</code> and <code>zeta</code> for the Cartwright distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample angles (radians). Values are wrapped to <code>[0, 2\u03c0)</code> internally.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights/frequencies broadcastable to <code>data</code>.</p> <code>None</code> <code>method</code> <code>(mle, moments)</code> <p>Estimation strategy. \"moments\" matches the first trigonometric moment, \"mle\" (default) maximises the weighted log-likelihood.</p> <code>\"mle\"</code> <code>return_info</code> <code>bool</code> <p>If True, also return a diagnostic dictionary.</p> <code>False</code> <code>optimizer</code> <code>str</code> <p>Optimiser passed to <code>scipy.optimize.minimize</code> when <code>method=\"mle\"</code>.</p> <code>'L-BFGS-B'</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to the optimiser.</p> <code>{}</code> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    return_info=False,\n    optimizer=\"L-BFGS-B\",\n    **kwargs,\n):\n    \"\"\"\n    Estimate ``mu`` and ``zeta`` for the Cartwright distribution.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n    weights : array_like, optional\n        Non-negative weights/frequencies broadcastable to ``data``.\n    method : {\"mle\", \"moments\"}, optional\n        Estimation strategy. \"moments\" matches the first trigonometric\n        moment, \"mle\" (default) maximises the weighted log-likelihood.\n    return_info : bool, optional\n        If True, also return a diagnostic dictionary.\n    optimizer : str, optional\n        Optimiser passed to ``scipy.optimize.minimize`` when\n        ``method=\"mle\"``.\n    **kwargs :\n        Additional keyword arguments forwarded to the optimiser.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float))\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    mu_mom, _ = circ_mean_and_r(alpha=x, w=w)\n    if not np.isfinite(mu_mom):\n        mu_mom = float(0.0)\n    mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n    delta = (x - mu_mom + np.pi) % (2.0 * np.pi) - np.pi\n    sin_half = np.sin(0.5 * delta)\n    m_t = float(np.sum(w * sin_half**2) / w_sum)\n    m_t = float(np.clip(m_t, 0.0, 0.5 - 1e-12))\n    if m_t &lt;= 1e-12:\n        zeta_mom = 1e-6\n    else:\n        denom = max(1e-12, 0.5 - m_t)\n        zeta_mom = float(np.clip(m_t / denom, 1e-6, 1e6))\n\n    def log_c(z):\n        inv = 1.0 / z\n        return (\n            (-1.0 + inv) * np.log(2.0)\n            + 2.0 * gammaln(1.0 + inv)\n            - np.log(np.pi)\n            - gammaln(1.0 + 2.0 * inv)\n        )\n\n    def nll(params):\n        mu_param, zeta_param = params\n        if zeta_param &lt;= 0.0:\n            return np.inf\n        cos_term = np.cos(x - mu_param)\n        denom = np.clip(1.0 + cos_term, 1e-15, None)\n        sum_log = np.sum(w * np.log(denom))\n        ll = w_sum * log_c(zeta_param) + (1.0 / zeta_param) * sum_log\n        return float(-ll)\n\n    def grad(params):\n        mu_param, zeta_param = params\n        cos_term = np.cos(x - mu_param)\n        denom = np.clip(1.0 + cos_term, 1e-15, None)\n        sin_term = np.sin(x - mu_param)\n        sum_log = np.sum(w * np.log(denom))\n        grad_mu = -(1.0 / zeta_param) * np.sum(w * sin_term / denom)\n        inv = 1.0 / zeta_param\n        term = 2.0 * digamma(1.0 + 2.0 * inv) - (\n            np.log(2.0) + 2.0 * digamma(1.0 + inv)\n        )\n        grad_zeta = (sum_log - w_sum * term) / (zeta_param**2)\n        return np.array([grad_mu, grad_zeta], dtype=float)\n\n    method = method.lower()\n    if method not in {\"mle\", \"moments\"}:\n        raise ValueError(\"`method` must be either 'mle' or 'moments'.\")\n\n    if method == \"moments\":\n        mu_hat = self._wrap_direction(mu_mom)\n        zeta_hat = zeta_mom\n        info = {\n            \"method\": \"moments\",\n            \"loglik\": float(-nll((mu_hat, zeta_hat))),\n            \"n_effective\": float(n_eff),\n            \"converged\": True,\n        }\n    else:\n        mu_init = mu_mom\n        zeta_init = zeta_mom if np.isfinite(zeta_mom) else 10.0\n        zeta_init = float(np.clip(zeta_init, 1e-3, 1e4))\n        bounds = [(0.0, 2.0 * np.pi), (1e-6, 1e6)]\n        result = minimize(\n            nll,\n            np.array([mu_init, zeta_init], dtype=float),\n            method=optimizer,\n            jac=grad,\n            bounds=bounds,\n            **kwargs,\n        )\n        if not result.success:\n            raise RuntimeError(\n                f\"cartwright.fit(method='mle') failed: {result.message}\"\n            )\n        mu_hat = self._wrap_direction(float(result.x[0]))\n        zeta_hat = float(np.clip(result.x[1], 1e-6, 1e6))\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-result.fun),\n            \"n_effective\": float(n_eff),\n            \"converged\": bool(result.success),\n            \"nit\": result.nit,\n            \"grad_norm\": float(np.linalg.norm(result.jac))\n            if getattr(result, \"jac\", None) is not None\n            else np.nan,\n            \"optimizer\": optimizer,\n        }\n\n    estimates = (mu_hat, zeta_hat)\n    if return_info:\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapnorm_gen","title":"<code>wrapnorm_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Wrapped Normal Distribution</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Percent-point function (inverse CDF).</p> <code>rvs</code> <p>Random variates.</p> <code>fit</code> <p>Estimate <code>(mu, rho)</code> via method-of-moments or maximum likelihood.</p> <p>Examples:</p> <pre><code>from pycircstat2.distributions import wrapnorm\n</code></pre> Notes <p>Implementation based on Section 4.3.7 of Pewsey et al. (2014)</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class wrapnorm_gen(CircularContinuous):\n    \"\"\"Wrapped Normal Distribution\n\n    ![wrapnorm](../images/circ-mod-wrapnorm.png)\n\n    Methods\n    -------\n    pdf(x, mu, rho)\n        Probability density function.\n\n    cdf(x, mu, rho)\n        Cumulative distribution function.\n\n    ppf(q, mu, rho)\n        Percent-point function (inverse CDF).\n\n    rvs(mu, rho, size=None, random_state=None)\n        Random variates.\n\n    fit(data, *args, **kwargs)\n        Estimate ``(mu, rho)`` via method-of-moments or maximum likelihood.\n\n    Examples\n    --------\n    ```\n    from pycircstat2.distributions import wrapnorm\n    ```\n\n    Notes\n    -----\n    Implementation based on Section 4.3.7 of Pewsey et al. (2014)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._series_window_cache = {}\n\n    def _argcheck(self, mu, rho):\n        try:\n            mu_arr, rho_arr = np.broadcast_arrays(mu, rho)\n        except ValueError:\n            return False\n        return (\n            (mu_arr &gt;= 0.0)\n            &amp; (mu_arr &lt;= 2.0 * np.pi)\n            &amp; (rho_arr &gt; 0.0)\n            &amp; (rho_arr &lt; 1.0)\n        )\n\n    def _pdf(self, x, mu, rho):\n        return (\n            1\n            + 2\n            * np.sum([rho ** (p**2) * np.cos(p * (x - mu)) for p in range(1, 30)], 0)\n        ) / (2 * np.pi)\n\n    def pdf(self, x, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Wrapped Normal distribution.\n\n        $$\n        f(\\theta) = \\frac{1}{2\\pi} \\left(1 + 2\\sum_{p=1}^{\\infty} \\rho^{p^2} \\cos(p(\\theta - \\mu))\\right)\n        $$\n\n        , here we approximate the infinite sum by summing the first 30 terms.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Shape parameter, 0 &lt; rho &lt;= 1.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n        return super().pdf(x, mu, rho, *args, **kwargs)\n\n    @staticmethod\n    def _wrapnorm_cdf_pdf(theta, mu_val, sigma, *, tol=1e-13, max_iter=500):\n        theta_arr = np.asarray(theta, dtype=float)\n        flat = theta_arr.reshape(-1)\n        if flat.size == 0:\n            return theta_arr.astype(float), theta_arr.astype(float)\n\n        inv_sigma = 1.0 / sigma\n        two_pi = 2.0 * np.pi\n\n        diff = flat - mu_val\n        z0 = diff * inv_sigma\n        z_ref0 = (-mu_val) * inv_sigma\n\n        cdf = ndtr(z0) - ndtr(z_ref0)\n        pdf = INV_SQRT_2PI * inv_sigma * np.exp(-0.5 * z0**2)\n\n        k = 1\n        max_contrib = np.inf\n        while k &lt;= max_iter and max_contrib &gt; tol:\n            shift = two_pi * k\n\n            z_pos = (diff + shift) * inv_sigma\n            z_pos_ref = (-mu_val + shift) * inv_sigma\n            delta_pos = ndtr(z_pos) - ndtr(z_pos_ref)\n            pdf += INV_SQRT_2PI * inv_sigma * np.exp(-0.5 * z_pos**2)\n\n            z_neg = (diff - shift) * inv_sigma\n            z_neg_ref = (-mu_val - shift) * inv_sigma\n            delta_neg = ndtr(z_neg) - ndtr(z_neg_ref)\n            pdf += INV_SQRT_2PI * inv_sigma * np.exp(-0.5 * z_neg**2)\n\n            cdf += delta_pos + delta_neg\n            max_contrib = max(\n                float(np.max(np.abs(delta_pos))),\n                float(np.max(np.abs(delta_neg))),\n            )\n            if not np.isfinite(max_contrib):\n                break\n            k += 1\n\n        cdf = np.clip(cdf, 0.0, 1.0)\n        pdf = np.clip(pdf, 0.0, None)\n\n        cdf = cdf.reshape(theta_arr.shape)\n        pdf = pdf.reshape(theta_arr.shape)\n        return cdf, pdf\n\n    def _cdf(self, x, mu, rho):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n\n        if flat.size == 0:\n            return arr.astype(float)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n        if mu_arr.size != 1 or rho_arr.size != 1:\n            raise ValueError(\"wrapnorm parameters must be scalar-valued.\")\n\n        mu_val = float(mu_arr.reshape(-1)[0])\n        rho_val = float(rho_arr.reshape(-1)[0])\n        two_pi = 2.0 * np.pi\n\n        if rho_val &lt;= 1e-12:\n            uniform = flat / two_pi\n            if arr.ndim == 0:\n                value = float(uniform[0])\n                return 1.0 if np.isclose(float(wrapped), two_pi) else value\n            result = uniform.reshape(arr.shape)\n            result[np.isclose(arr, two_pi)] = 1.0\n            return result\n\n        rho_clipped = np.clip(rho_val, np.finfo(float).tiny, 1.0 - 1e-15)\n        sigma = float(np.sqrt(-2.0 * np.log(rho_clipped)))\n\n        cdf_flat, _ = self._wrapnorm_cdf_pdf(flat, mu_val, sigma)\n        if arr.ndim == 0:\n            value = float(cdf_flat.reshape(-1)[0])\n            return 1.0 if np.isclose(float(wrapped), two_pi) else value\n\n        result = cdf_flat.reshape(arr.shape)\n        result[np.isclose(arr, two_pi)] = 1.0\n        return result\n\n    def cdf(self, x, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the Wrapped Normal distribution.\n\n        The CDF is evaluated via the wrapped normal series involving the\n        standard normal distribution function.\n\n        $$\n        F(\\theta) = \\sum_{k=-\\infty}^{\\infty} \\left[\n            \\Phi\\left(\\frac{\\theta - \\mu + 2\\pi k}{\\sigma}\\right)\n            - \\Phi\\left(\\frac{-\\mu + 2\\pi k}{\\sigma}\\right)\n        \\right], \\quad \\sigma = \\sqrt{-2\\log \\rho}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Shape parameter, 0 &lt; rho &lt;= 1.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative distribution function evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, mu, rho, *args, **kwargs)\n\n    def _ppf(self, q, mu, rho):\n        mu_arr = np.asarray(mu, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        rho_val = float(rho_arr.reshape(-1)[0])\n        two_pi = 2.0 * np.pi\n\n        q_arr = np.asarray(q, dtype=float)\n        flat = q_arr.reshape(-1)\n        if flat.size == 0:\n            return q_arr.astype(float)\n\n        def _finish(arr):\n            reshaped = arr.reshape(q_arr.shape)\n            if q_arr.ndim == 0:\n                return float(reshaped)\n            return reshaped\n\n        result = np.full_like(flat, np.nan, dtype=float)\n        valid = np.isfinite(flat)\n\n        if not np.any(valid):\n            return _finish(result)\n\n        close_zero = valid &amp; (flat &lt;= 0.0)\n        close_one = valid &amp; (flat &gt;= 1.0)\n        result[close_zero] = 0.0\n        result[close_one] = two_pi\n\n        interior = valid &amp; ~(close_zero | close_one)\n        if not np.any(interior):\n            return _finish(result)\n\n        flat_interior = flat[interior]\n\n        if rho_val &lt;= 1e-12:\n            result[interior] = two_pi * flat_interior\n            return _finish(result)\n\n        rho_clipped = np.clip(rho_val, np.finfo(float).tiny, 1.0 - 1e-15)\n        sigma = float(np.sqrt(-2.0 * np.log(rho_clipped)))\n\n        if sigma &lt;= 1e-12:\n            result[interior] = np.mod(mu_val, two_pi)\n            return _finish(result)\n\n        q_sub = flat_interior\n        theta = np.clip(two_pi * q_sub, 1e-12, two_pi - 1e-12)\n        if sigma &lt; 1.0:\n            normal_guess = mu_val + sigma * ndtri(np.clip(q_sub, 1e-12, 1.0 - 1e-12))\n            theta = 0.5 * theta + 0.5 * np.mod(normal_guess, two_pi)\n\n        lower = np.zeros_like(theta)\n        upper = np.full_like(theta, two_pi)\n        tol = 1e-12\n        max_iter = 6\n\n        theta_curr = theta\n        cdf_vals, pdf_vals = self._wrapnorm_cdf_pdf(theta_curr, mu_val, sigma)\n        delta = cdf_vals - q_sub\n\n        for _ in range(max_iter):\n            lower = np.where(delta &lt;= 0.0, theta_curr, lower)\n            upper = np.where(delta &gt; 0.0, theta_curr, upper)\n            if np.max(np.abs(delta)) &lt;= tol:\n                break\n            denom = np.clip(pdf_vals, 1e-15, None)\n            step = np.clip(delta / denom, -np.pi, np.pi)\n            theta_next = theta_curr - step\n            theta_next = np.where(\n                (theta_next &lt;= lower) | (theta_next &gt;= upper),\n                0.5 * (lower + upper),\n                theta_next,\n            )\n            theta_next = np.clip(theta_next, 0.0, two_pi)\n            theta_curr = theta_next\n            cdf_vals, pdf_vals = self._wrapnorm_cdf_pdf(theta_curr, mu_val, sigma)\n            delta = cdf_vals - q_sub\n\n        lower = np.where(delta &lt;= 0.0, theta_curr, lower)\n        upper = np.where(delta &gt; 0.0, theta_curr, upper)\n\n        mask = np.abs(delta) &gt; tol\n        if np.any(mask):\n            lower_b = lower.copy()\n            upper_b = upper.copy()\n            theta_b = theta_curr.copy()\n            for _ in range(40):\n                if not np.any(mask):\n                    break\n                mid = 0.5 * (lower_b + upper_b)\n                mid_cdf, _ = self._wrapnorm_cdf_pdf(mid, mu_val, sigma)\n                delta_mid = mid_cdf - q_sub\n                take_upper = (delta_mid &gt; 0.0) &amp; mask\n                take_lower = (~take_upper) &amp; mask\n                upper_b = np.where(take_upper, mid, upper_b)\n                lower_b = np.where(take_lower, mid, lower_b)\n                theta_b = np.where(mask, mid, theta_b)\n                mask = mask &amp; (np.abs(delta_mid) &gt; tol)\n            theta_curr = np.where(mask, 0.5 * (lower_b + upper_b), theta_b)\n\n        theta_curr = np.clip(theta_curr, 0.0, two_pi)\n        endpoint_mask = theta_curr &gt;= (two_pi - 1e-12)\n        if np.any(endpoint_mask):\n            endpoint_value = np.nextafter(two_pi, 0.0)\n            theta_curr = np.where(endpoint_mask, endpoint_value, theta_curr)\n\n        result[interior] = theta_curr\n        return _finish(result)\n\n    def ppf(self, q, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (inverse CDF) of the Wrapped Normal distribution.\n\n        The quantile is found by inverting the wrapped normal CDF using a\n        safeguarded Newton iteration on $[0, 2\\pi]$. At each step the algorithm\n        evaluates the truncated unwrapped Gaussian series\n        $$\n        F(\\theta)=\\sum_{k=-\\infty}^{\\infty}\n        \\Bigl[\\Phi\\!\\Bigl(\\tfrac{\\theta-\\mu+2\\pi k}{\\sigma}\\Bigr)\n        - \\Phi\\!\\Bigl(\\tfrac{-\\mu+2\\pi k}{\\sigma}\\Bigr)\\Bigr],\n        \\qquad\n        f(\\theta)=\\sum_{k=-\\infty}^{\\infty}\n        \\frac{1}{\\sigma}\\,\\varphi\\!\\Bigl(\\tfrac{\\theta-\\mu+2\\pi k}{\\sigma}\\Bigr),\n        $$\n        with $\\sigma = \\sqrt{-2\\log\\rho}$, using the CDF residual to update the\n        bracket and the PDF as the local slope. A final bisection polish ensures\n        robust convergence and keeps the quantile consistent with ``cdf`` and\n        ``rvs``.\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles to evaluate (0 &lt;= q &lt;= 1).\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Shape parameter, 0 &lt; rho &lt; 1.\n\n        Returns\n        -------\n        ppf_values : array_like\n            Angles corresponding to the given quantiles.\n        \"\"\"\n        return super().ppf(q, mu, rho, *args, **kwargs)\n\n    def _rvs(self, mu, rho, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n        if mu_arr.size != 1 or rho_arr.size != 1:\n            raise ValueError(\"wrapnorm parameters must be scalar-valued.\")\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        rho_val = float(np.clip(rho_arr.reshape(-1)[0], np.finfo(float).tiny, 1.0 - 1e-15))\n\n        if rho_val &lt;= 1e-12:\n            samples = rng.uniform(0.0, 2.0 * np.pi, size=size)\n            return float(samples) if np.isscalar(samples) else samples\n\n        sigma = float(np.sqrt(-2.0 * np.log(rho_val)))\n        if sigma &lt; 1e-12:\n            if size is None:\n                return mu_val\n            if np.isscalar(size):\n                return np.full((int(size),), mu_val, dtype=float)\n            shape = tuple(int(dim) for dim in np.atleast_1d(size))\n            return np.full(shape, mu_val, dtype=float)\n\n        samples = rng.normal(loc=mu_val, scale=sigma, size=size)\n        wrapped = np.mod(samples, 2.0 * np.pi)\n        if np.isscalar(wrapped):\n            return float(wrapped)\n        return wrapped\n\n    def rvs(self, mu=None, rho=None, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the Wrapped Normal distribution.\n\n        Samples are obtained by drawing from $N(\\mu, \\sigma^2)$ with\n        $\\sigma = \\sqrt{-2\\log\\rho}$ and wrapping the result modulo $2\\pi$.\n        This matches the analytic mixture used in ``cdf`` and ``ppf``, keeping\n        all three methods numerically consistent.\n\n        Parameters\n        ----------\n        mu : float, optional\n            Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n            freezing the distribution.\n        rho : float, optional\n            Shape parameter, ``0 &lt; rho &lt; 1``. Supply explicitly or by freezing\n            the distribution.\n        size : int or tuple of ints, optional\n            Number of samples to draw. ``None`` (default) returns a scalar.\n        random_state : np.random.Generator, np.random.RandomState, or None, optional\n            Random number generator to use.\n\n        Returns\n        -------\n        samples : ndarray or float\n            Random variates on ``[0, 2\u03c0)``.\n        \"\"\"\n        mu_val = getattr(self, \"mu\", None) if mu is None else mu\n        rho_val = getattr(self, \"rho\", None) if rho is None else rho\n\n        if mu_val is None or rho_val is None:\n            raise ValueError(\"Both 'mu' and 'rho' must be provided.\")\n\n        return self._rvs(mu_val, rho_val, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        return_info=False,\n        optimizer=\"L-BFGS-B\",\n        **kwargs,\n    ):\n        \"\"\"\n        Estimate ``mu`` and ``rho`` for the wrapped normal distribution.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n        weights : array_like, optional\n            Non-negative weights broadcastable to ``data``.\n        method : {\"moments\", \"mle\"}, optional\n            Estimation strategy. ``\"moments\"`` (aliases: \"analytical\") returns\n            the circular mean and resultant length. ``\"mle\"`` (alias:\n            \"numerical\") maximises the weighted log-likelihood via numerical\n            optimisation.\n        return_info : bool, optional\n            If True, return a diagnostics dictionary alongside the estimates.\n        optimizer : str, optional\n            Optimiser passed to ``scipy.optimize.minimize`` when\n            ``method=\"mle\"``.\n        **kwargs :\n            Additional keyword arguments forwarded to the optimiser.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        mu_mom, rho_mom = circ_mean_and_r(alpha=x, w=w)\n        if not np.isfinite(mu_mom):\n            mu_mom = float(0.0)\n        mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n        rho_mom = float(np.clip(rho_mom, 1e-9, 1.0 - 1e-9))\n\n        def logpdf_series(mu_param, rho_param):\n            rho_val = float(np.clip(rho_param, 1e-12, 1.0 - 1e-12))\n            if rho_val &lt;= 1e-8:\n                return np.full_like(x, -np.log(2.0 * np.pi), dtype=float)\n\n            sigma = float(np.sqrt(-2.0 * np.log(rho_val)))\n            if sigma &gt; 10.0:\n                return np.full_like(x, -np.log(2.0 * np.pi), dtype=float)\n\n            two_pi = 2.0 * np.pi\n            cache = getattr(self, \"_series_window_cache\", None)\n            if cache is None:\n                cache = {}\n                self._series_window_cache = cache\n\n            mu_norm = float(np.mod(mu_param, two_pi))\n            mu_bucket = int(round(mu_norm / two_pi * 512)) % 512\n            rho_bucket = int(round(min(4095.0, -np.log1p(-rho_val) * 64.0)))\n            key = (mu_bucket, rho_bucket)\n\n            max_cap = 256\n            max_k = cache.get(\n                key,\n                max(5, int(np.ceil(3.0 * sigma / two_pi)) + 5),\n            )\n\n            tail_tol = 1e-10\n            while True:\n                ks = np.arange(-max_k, max_k + 1, dtype=float)\n                diff = x[:, None] - mu_param + two_pi * ks[None, :]\n                exponents = -0.5 * (diff / sigma) ** 2\n                max_exp = np.max(exponents, axis=1, keepdims=True)\n                shifted = np.exp(exponents - max_exp)\n                sum_exp = np.sum(shifted, axis=1)\n                log_pdf = max_exp.squeeze(1) + np.log(sum_exp)\n                log_pdf -= 0.5 * np.log(2.0 * np.pi) + np.log(sigma)\n\n                tail_contrib = float(\n                    np.max(shifted[:, (0, -1)] / np.maximum(sum_exp[:, None], 1e-300))\n                )\n                if tail_contrib &lt;= tail_tol or max_k &gt;= max_cap:\n                    cache[key] = max_k\n                    return log_pdf.astype(float, copy=False)\n\n                max_k = min(max_cap, max_k + 2)\n            return log_pdf\n\n        def nll(params):\n            mu_param, rho_param = params\n            if not (0.0 &lt;= rho_param &lt; 1.0):\n                return np.inf\n            log_pdf = logpdf_series(mu_param, rho_param)\n            return float(-np.sum(w * log_pdf))\n\n        method_key = method.lower()\n        alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n        method_key = alias.get(method_key, method_key)\n\n        if method_key not in {\"moments\", \"mle\"}:\n            raise ValueError(\"`method` must be one of {'moments', 'mle', 'analytical', 'numerical'}.\")\n\n        if \"algorithm\" in kwargs:\n            optimizer = kwargs.pop(\"algorithm\")\n\n        if method_key == \"moments\":\n            mu_hat = self._wrap_direction(mu_mom)\n            rho_hat = rho_mom\n            info = {\n                \"method\": \"moments\",\n                \"loglik\": float(-nll((mu_hat, rho_hat))),\n                \"n_effective\": float(n_eff),\n                \"converged\": True,\n            }\n        else:\n            bounds = [(0.0, 2.0 * np.pi), (1e-9, 1.0 - 1e-9)]\n            init = np.array([mu_mom, rho_mom], dtype=float)\n            result = minimize(\n                nll,\n                init,\n                method=optimizer,\n                bounds=bounds,\n                **kwargs,\n            )\n            if not result.success:\n                raise RuntimeError(\n                    f\"wrapnorm.fit(method='mle') failed: {result.message}\"\n                )\n            mu_hat = self._wrap_direction(float(result.x[0]))\n            rho_hat = float(np.clip(result.x[1], 1e-9, 1.0 - 1e-9))\n            info = {\n                \"method\": \"mle\",\n                \"loglik\": float(-result.fun),\n                \"n_effective\": float(n_eff),\n                \"converged\": bool(result.success),\n                \"nit\": result.nit,\n                \"grad_norm\": np.nan,\n                \"optimizer\": optimizer,\n            }\n\n        estimates = (mu_hat, rho_hat)\n        if return_info:\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapnorm_gen.pdf","title":"<code>pdf(x, mu, rho, *args, **kwargs)</code>","text":"<p>Probability density function of the Wrapped Normal distribution.</p> \\[ f(\\theta) = \\frac{1}{2\\pi} \\left(1 + 2\\sum_{p=1}^{\\infty} \\rho^{p^2} \\cos(p(\\theta - \\mu))\\right) \\] <p>, here we approximate the infinite sum by summing the first 30 terms.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Shape parameter, 0 &lt; rho &lt;= 1.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Wrapped Normal distribution.\n\n    $$\n    f(\\theta) = \\frac{1}{2\\pi} \\left(1 + 2\\sum_{p=1}^{\\infty} \\rho^{p^2} \\cos(p(\\theta - \\mu))\\right)\n    $$\n\n    , here we approximate the infinite sum by summing the first 30 terms.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Shape parameter, 0 &lt; rho &lt;= 1.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n    return super().pdf(x, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapnorm_gen.cdf","title":"<code>cdf(x, mu, rho, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Wrapped Normal distribution.</p> <p>The CDF is evaluated via the wrapped normal series involving the standard normal distribution function.</p> \\[ F(\\theta) = \\sum_{k=-\\infty}^{\\infty} \\left[     \\Phi\\left(\\frac{\\theta - \\mu + 2\\pi k}{\\sigma}\\right)     - \\Phi\\left(\\frac{-\\mu + 2\\pi k}{\\sigma}\\right) \\right], \\quad \\sigma = \\sqrt{-2\\log \\rho} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Shape parameter, 0 &lt; rho &lt;= 1.</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative distribution function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the Wrapped Normal distribution.\n\n    The CDF is evaluated via the wrapped normal series involving the\n    standard normal distribution function.\n\n    $$\n    F(\\theta) = \\sum_{k=-\\infty}^{\\infty} \\left[\n        \\Phi\\left(\\frac{\\theta - \\mu + 2\\pi k}{\\sigma}\\right)\n        - \\Phi\\left(\\frac{-\\mu + 2\\pi k}{\\sigma}\\right)\n    \\right], \\quad \\sigma = \\sqrt{-2\\log \\rho}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Shape parameter, 0 &lt; rho &lt;= 1.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative distribution function evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapnorm_gen.ppf","title":"<code>ppf(q, mu, rho, *args, **kwargs)</code>","text":"<p>Percent-point function (inverse CDF) of the Wrapped Normal distribution.</p> <p>The quantile is found by inverting the wrapped normal CDF using a safeguarded Newton iteration on \\([0, 2\\pi]\\). At each step the algorithm evaluates the truncated unwrapped Gaussian series $$ F(\\theta)=\\sum_{k=-\\infty}^{\\infty} \\Bigl[\\Phi!\\Bigl(\\tfrac{\\theta-\\mu+2\\pi k}{\\sigma}\\Bigr) - \\Phi!\\Bigl(\\tfrac{-\\mu+2\\pi k}{\\sigma}\\Bigr)\\Bigr], \\qquad f(\\theta)=\\sum_{k=-\\infty}^{\\infty} \\frac{1}{\\sigma}\\,\\varphi!\\Bigl(\\tfrac{\\theta-\\mu+2\\pi k}{\\sigma}\\Bigr), $$ with \\(\\sigma = \\sqrt{-2\\log\\rho}\\), using the CDF residual to update the bracket and the PDF as the local slope. A final bisection polish ensures robust convergence and keeps the quantile consistent with <code>cdf</code> and <code>rvs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles to evaluate (0 &lt;= q &lt;= 1).</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Shape parameter, 0 &lt; rho &lt; 1.</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Angles corresponding to the given quantiles.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (inverse CDF) of the Wrapped Normal distribution.\n\n    The quantile is found by inverting the wrapped normal CDF using a\n    safeguarded Newton iteration on $[0, 2\\pi]$. At each step the algorithm\n    evaluates the truncated unwrapped Gaussian series\n    $$\n    F(\\theta)=\\sum_{k=-\\infty}^{\\infty}\n    \\Bigl[\\Phi\\!\\Bigl(\\tfrac{\\theta-\\mu+2\\pi k}{\\sigma}\\Bigr)\n    - \\Phi\\!\\Bigl(\\tfrac{-\\mu+2\\pi k}{\\sigma}\\Bigr)\\Bigr],\n    \\qquad\n    f(\\theta)=\\sum_{k=-\\infty}^{\\infty}\n    \\frac{1}{\\sigma}\\,\\varphi\\!\\Bigl(\\tfrac{\\theta-\\mu+2\\pi k}{\\sigma}\\Bigr),\n    $$\n    with $\\sigma = \\sqrt{-2\\log\\rho}$, using the CDF residual to update the\n    bracket and the PDF as the local slope. A final bisection polish ensures\n    robust convergence and keeps the quantile consistent with ``cdf`` and\n    ``rvs``.\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles to evaluate (0 &lt;= q &lt;= 1).\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Shape parameter, 0 &lt; rho &lt; 1.\n\n    Returns\n    -------\n    ppf_values : array_like\n        Angles corresponding to the given quantiles.\n    \"\"\"\n    return super().ppf(q, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapnorm_gen.rvs","title":"<code>rvs(mu=None, rho=None, size=None, random_state=None)</code>","text":"<p>Draw random variates from the Wrapped Normal distribution.</p> <p>Samples are obtained by drawing from \\(N(\\mu, \\sigma^2)\\) with \\(\\sigma = \\sqrt{-2\\log\\rho}\\) and wrapping the result modulo \\(2\\pi\\). This matches the analytic mixture used in <code>cdf</code> and <code>ppf</code>, keeping all three methods numerically consistent.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Mean direction, <code>0 &lt;= mu &lt;= 2*pi</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>rho</code> <code>float</code> <p>Shape parameter, <code>0 &lt; rho &lt; 1</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>size</code> <code>int or tuple of ints</code> <p>Number of samples to draw. <code>None</code> (default) returns a scalar.</p> <code>None</code> <code>random_state</code> <code>np.random.Generator, np.random.RandomState, or None</code> <p>Random number generator to use.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray or float</code> <p>Random variates on <code>[0, 2\u03c0)</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, mu=None, rho=None, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the Wrapped Normal distribution.\n\n    Samples are obtained by drawing from $N(\\mu, \\sigma^2)$ with\n    $\\sigma = \\sqrt{-2\\log\\rho}$ and wrapping the result modulo $2\\pi$.\n    This matches the analytic mixture used in ``cdf`` and ``ppf``, keeping\n    all three methods numerically consistent.\n\n    Parameters\n    ----------\n    mu : float, optional\n        Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n        freezing the distribution.\n    rho : float, optional\n        Shape parameter, ``0 &lt; rho &lt; 1``. Supply explicitly or by freezing\n        the distribution.\n    size : int or tuple of ints, optional\n        Number of samples to draw. ``None`` (default) returns a scalar.\n    random_state : np.random.Generator, np.random.RandomState, or None, optional\n        Random number generator to use.\n\n    Returns\n    -------\n    samples : ndarray or float\n        Random variates on ``[0, 2\u03c0)``.\n    \"\"\"\n    mu_val = getattr(self, \"mu\", None) if mu is None else mu\n    rho_val = getattr(self, \"rho\", None) if rho is None else rho\n\n    if mu_val is None or rho_val is None:\n        raise ValueError(\"Both 'mu' and 'rho' must be provided.\")\n\n    return self._rvs(mu_val, rho_val, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapnorm_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', return_info=False, optimizer='L-BFGS-B', **kwargs)</code>","text":"<p>Estimate <code>mu</code> and <code>rho</code> for the wrapped normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample angles (radians). Values are wrapped to <code>[0, 2\u03c0)</code> internally.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights broadcastable to <code>data</code>.</p> <code>None</code> <code>method</code> <code>(moments, mle)</code> <p>Estimation strategy. <code>\"moments\"</code> (aliases: \"analytical\") returns the circular mean and resultant length. <code>\"mle\"</code> (alias: \"numerical\") maximises the weighted log-likelihood via numerical optimisation.</p> <code>\"moments\"</code> <code>return_info</code> <code>bool</code> <p>If True, return a diagnostics dictionary alongside the estimates.</p> <code>False</code> <code>optimizer</code> <code>str</code> <p>Optimiser passed to <code>scipy.optimize.minimize</code> when <code>method=\"mle\"</code>.</p> <code>'L-BFGS-B'</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to the optimiser.</p> <code>{}</code> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    return_info=False,\n    optimizer=\"L-BFGS-B\",\n    **kwargs,\n):\n    \"\"\"\n    Estimate ``mu`` and ``rho`` for the wrapped normal distribution.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n    weights : array_like, optional\n        Non-negative weights broadcastable to ``data``.\n    method : {\"moments\", \"mle\"}, optional\n        Estimation strategy. ``\"moments\"`` (aliases: \"analytical\") returns\n        the circular mean and resultant length. ``\"mle\"`` (alias:\n        \"numerical\") maximises the weighted log-likelihood via numerical\n        optimisation.\n    return_info : bool, optional\n        If True, return a diagnostics dictionary alongside the estimates.\n    optimizer : str, optional\n        Optimiser passed to ``scipy.optimize.minimize`` when\n        ``method=\"mle\"``.\n    **kwargs :\n        Additional keyword arguments forwarded to the optimiser.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    mu_mom, rho_mom = circ_mean_and_r(alpha=x, w=w)\n    if not np.isfinite(mu_mom):\n        mu_mom = float(0.0)\n    mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n    rho_mom = float(np.clip(rho_mom, 1e-9, 1.0 - 1e-9))\n\n    def logpdf_series(mu_param, rho_param):\n        rho_val = float(np.clip(rho_param, 1e-12, 1.0 - 1e-12))\n        if rho_val &lt;= 1e-8:\n            return np.full_like(x, -np.log(2.0 * np.pi), dtype=float)\n\n        sigma = float(np.sqrt(-2.0 * np.log(rho_val)))\n        if sigma &gt; 10.0:\n            return np.full_like(x, -np.log(2.0 * np.pi), dtype=float)\n\n        two_pi = 2.0 * np.pi\n        cache = getattr(self, \"_series_window_cache\", None)\n        if cache is None:\n            cache = {}\n            self._series_window_cache = cache\n\n        mu_norm = float(np.mod(mu_param, two_pi))\n        mu_bucket = int(round(mu_norm / two_pi * 512)) % 512\n        rho_bucket = int(round(min(4095.0, -np.log1p(-rho_val) * 64.0)))\n        key = (mu_bucket, rho_bucket)\n\n        max_cap = 256\n        max_k = cache.get(\n            key,\n            max(5, int(np.ceil(3.0 * sigma / two_pi)) + 5),\n        )\n\n        tail_tol = 1e-10\n        while True:\n            ks = np.arange(-max_k, max_k + 1, dtype=float)\n            diff = x[:, None] - mu_param + two_pi * ks[None, :]\n            exponents = -0.5 * (diff / sigma) ** 2\n            max_exp = np.max(exponents, axis=1, keepdims=True)\n            shifted = np.exp(exponents - max_exp)\n            sum_exp = np.sum(shifted, axis=1)\n            log_pdf = max_exp.squeeze(1) + np.log(sum_exp)\n            log_pdf -= 0.5 * np.log(2.0 * np.pi) + np.log(sigma)\n\n            tail_contrib = float(\n                np.max(shifted[:, (0, -1)] / np.maximum(sum_exp[:, None], 1e-300))\n            )\n            if tail_contrib &lt;= tail_tol or max_k &gt;= max_cap:\n                cache[key] = max_k\n                return log_pdf.astype(float, copy=False)\n\n            max_k = min(max_cap, max_k + 2)\n        return log_pdf\n\n    def nll(params):\n        mu_param, rho_param = params\n        if not (0.0 &lt;= rho_param &lt; 1.0):\n            return np.inf\n        log_pdf = logpdf_series(mu_param, rho_param)\n        return float(-np.sum(w * log_pdf))\n\n    method_key = method.lower()\n    alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n    method_key = alias.get(method_key, method_key)\n\n    if method_key not in {\"moments\", \"mle\"}:\n        raise ValueError(\"`method` must be one of {'moments', 'mle', 'analytical', 'numerical'}.\")\n\n    if \"algorithm\" in kwargs:\n        optimizer = kwargs.pop(\"algorithm\")\n\n    if method_key == \"moments\":\n        mu_hat = self._wrap_direction(mu_mom)\n        rho_hat = rho_mom\n        info = {\n            \"method\": \"moments\",\n            \"loglik\": float(-nll((mu_hat, rho_hat))),\n            \"n_effective\": float(n_eff),\n            \"converged\": True,\n        }\n    else:\n        bounds = [(0.0, 2.0 * np.pi), (1e-9, 1.0 - 1e-9)]\n        init = np.array([mu_mom, rho_mom], dtype=float)\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            bounds=bounds,\n            **kwargs,\n        )\n        if not result.success:\n            raise RuntimeError(\n                f\"wrapnorm.fit(method='mle') failed: {result.message}\"\n            )\n        mu_hat = self._wrap_direction(float(result.x[0]))\n        rho_hat = float(np.clip(result.x[1], 1e-9, 1.0 - 1e-9))\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-result.fun),\n            \"n_effective\": float(n_eff),\n            \"converged\": bool(result.success),\n            \"nit\": result.nit,\n            \"grad_norm\": np.nan,\n            \"optimizer\": optimizer,\n        }\n\n    estimates = (mu_hat, rho_hat)\n    if return_info:\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapcauchy_gen","title":"<code>wrapcauchy_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Wrapped Cauchy Distribution.</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Percent-point function (inverse CDF) via the M\u00f6bius mapping.</p> <code>rvs</code> <p>Random variates.</p> <code>fit</code> <p>Fit the distribution to the data and return the parameters (mu, rho).</p> Notes <p>Implementation based on Section 4.3.6 of Pewsey et al. (2014).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class wrapcauchy_gen(CircularContinuous):\n    \"\"\"Wrapped Cauchy Distribution.\n\n    ![wrapcauchy](../images/circ-mod-wrapcauchy.png)\n\n    Methods\n    -------\n    pdf(x, mu, rho)\n        Probability density function.\n\n    cdf(x, mu, rho)\n        Cumulative distribution function.\n\n    ppf(q, mu, rho)\n        Percent-point function (inverse CDF) via the M\u00f6bius mapping.\n\n    rvs(mu, rho, size=None, random_state=None)\n        Random variates.\n\n    fit(data, method=\"analytical\", *args, **kwargs)\n        Fit the distribution to the data and return the parameters (mu, rho).\n\n    Notes\n    -----\n    Implementation based on Section 4.3.6 of Pewsey et al. (2014).\n    \"\"\"\n\n    def _argcheck(self, mu, rho):\n        try:\n            mu_arr, rho_arr = np.broadcast_arrays(mu, rho)\n        except ValueError:\n            return False\n        return (\n            (mu_arr &gt;= 0.0)\n            &amp; (mu_arr &lt;= 2.0 * np.pi)\n            &amp; (rho_arr &gt;= 0.0)\n            &amp; (rho_arr &lt; 1.0)\n        )\n\n    def _pdf(self, x, mu, rho):\n        return (1 - rho**2) / (2 * np.pi * (1 + rho**2 - 2 * rho * np.cos(x - mu)))\n\n    def pdf(self, x, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Wrapped Cauchy distribution.\n\n        $$\n        f(\\theta) = \\frac{1 - \\rho^2}{2\\pi(1 + \\rho^2 - 2\\rho \\cos(\\theta - \\mu))}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Shape parameter, 0 &lt; rho &lt;= 1.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n        return super().pdf(x, mu, rho, *args, **kwargs)\n\n    def _logpdf(self, x, mu, rho):\n        return np.log(np.clip(self._pdf(x, mu, rho), 1e-16, None))\n\n    def logpdf(self, x, mu, rho, *args, **kwargs):\n        \"\"\"\n        Logarithm of the probability density function.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the log-PDF.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Mean resultant length, 0 &lt; rho &lt;= 1.\n\n        Returns\n        -------\n        logpdf_values : array_like\n            Logarithm of the probability density function evaluated at `x`.\n        \"\"\"\n        return super().logpdf(x, mu, rho, *args, **kwargs)\n\n    def _cdf(self, x, mu, rho):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        if mu_arr.size != 1:\n            raise ValueError(\"wrapcauchy parameters must be scalar-valued.\")\n        mu_val = float(mu_arr.reshape(-1)[0])\n        rho_arr = np.asarray(rho, dtype=float)\n        if rho_arr.size != 1:\n            raise ValueError(\"wrapcauchy parameters must be scalar-valued.\")\n        rho_val = float(rho_arr.reshape(-1)[0])\n        rho_val = np.clip(rho_val, np.finfo(float).tiny, 1.0 - 1e-15)\n\n        if flat.size == 0:\n            return arr.astype(float)\n\n        two_pi = 2.0 * np.pi\n        A = (1.0 + rho_val) / (1.0 - rho_val)\n\n        phi = (flat - mu_val + np.pi) % two_pi - np.pi\n        base_phi = (-mu_val + np.pi) % two_pi - np.pi\n\n        angle = np.arctan2(A * np.sin(0.5 * phi), np.cos(0.5 * phi))\n        base_angle = np.arctan2(A * np.sin(0.5 * base_phi), np.cos(0.5 * base_phi))\n\n        cdf = 0.5 + angle / np.pi\n        base_val = 0.5 + base_angle / np.pi\n\n        diff = cdf - base_val\n        diff = np.where(diff &lt; -1e-12, diff + 1.0, diff)\n        diff = np.where(diff &gt; 1.0, diff - 1.0, diff)\n        cdf = np.clip(diff, 0.0, 1.0)\n\n        if arr.ndim == 0:\n            value = float(cdf[0])\n            return 1.0 if np.isclose(float(wrapped), 2.0 * np.pi) else value\n        reshaped = cdf.reshape(arr.shape)\n        reshaped[np.isclose(arr, 2.0 * np.pi)] = 1.0\n        return reshaped\n\n    def cdf(self, x, mu, rho, *args, **kwargs):\n        \"\"\"\n        Cumulative distribution function of the Wrapped Cauchy distribution.\n\n        The CDF is evaluated analytically via the wrapped Cauchy series.\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the CDF.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        rho : float\n            Shape parameter, 0 &lt; rho &lt;= 1.\n\n        Returns\n        -------\n        cdf_values : array_like\n            CDF evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, mu, rho, *args, **kwargs)\n\n    @staticmethod\n    def _wrapcauchy_H(phi, A):\n        phi_arr = np.asarray(phi, dtype=float)\n        angle = np.arctan2(A * np.sin(0.5 * phi_arr), np.cos(0.5 * phi_arr))\n        H = 0.5 + angle / np.pi\n        return float(H) if np.isscalar(phi) else H\n\n    def _ppf(self, q, mu, rho):\n        mu_arr = np.asarray(mu, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        rho_val = float(rho_arr.reshape(-1)[0])\n        if not (0.0 &lt;= rho_val &lt; 1.0):\n            raise ValueError(\"`rho` must lie in [0, 1).\")\n\n        q_arr = np.asarray(q, dtype=float)\n        flat = q_arr.reshape(-1)\n        if flat.size == 0:\n            return q_arr.astype(float)\n\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        lower_mask = flat &lt;= 0.0\n        upper_mask = flat &gt;= 1.0\n        result[lower_mask] = 0.0\n        result[upper_mask] = 2.0 * np.pi\n\n        interior = ~(lower_mask | upper_mask)\n        if not np.any(interior):\n            return result.reshape(q_arr.shape)\n\n        q_int = flat[interior]\n        two_pi = 2.0 * np.pi\n\n        if rho_val &lt;= 1e-15:\n            result[interior] = (two_pi * q_int) % two_pi\n            return result.reshape(q_arr.shape)\n\n        A = (1.0 + rho_val) / (1.0 - rho_val)\n        phi0 = (-mu_val + np.pi) % two_pi - np.pi\n        H_start = float(self._wrapcauchy_H(phi0, A))\n\n        s = (H_start + q_int) % 1.0\n        eps = 1e-15\n        alpha = np.pi * (np.clip(s, eps, 1.0 - eps) - 0.5)\n        tan_alpha = np.tan(alpha)\n        phi = 2.0 * np.arctan(tan_alpha / A)\n        theta = (mu_val + phi) % two_pi\n        result[interior] = theta\n\n        return result.reshape(q_arr.shape)\n\n    def ppf(self, q, mu, rho, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (inverse CDF) of the Wrapped Cauchy distribution.\n\n        The quantile is obtained by inverting the M\u00f6bius form of the CDF:\n        $$\n        \\phi = 2 \\arctan\\!\\left(\\frac{\\tan\\left(\\pi (s-\\tfrac12)\\right)}{A}\\right),\n        \\qquad A=\\frac{1+\\rho}{1-\\rho},\n        $$\n        where $s = (H(\\phi_0) + q) \\bmod 1$ and $\\phi_0$ is the anchored angle\n        at $x=0$. This matches the direct normalised CDF and keeps ``ppf`` in\n        sync with ``cdf`` and the M\u00f6bius sampler used by ``rvs``.\n        \"\"\"\n        return super().ppf(q, mu, rho, *args, **kwargs)\n\n    def _rvs(self, mu, rho, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        rho_arr = np.asarray(rho, dtype=float)\n        if mu_arr.size != 1 or rho_arr.size != 1:\n            raise ValueError(\"wrapcauchy parameters must be scalar-valued.\")\n\n        mu_val = float(mu_arr.reshape(-1)[0])\n        rho_val = float(rho_arr.reshape(-1)[0])\n        two_pi = 2.0 * np.pi\n\n        if np.isclose(rho_val, 0.0, atol=1e-15):\n            return rng.uniform(0.0, two_pi, size=size)\n\n        if np.isclose(rho_val, 1.0, atol=1e-15):\n            angle = float(np.mod(mu_val, two_pi))\n            if size is None:\n                return angle\n            return np.full(size, angle, dtype=float)\n\n        if size is None:\n            target_shape = ()\n        elif np.isscalar(size):\n            target_shape = (int(size),)\n        else:\n            target_shape = tuple(int(dim) for dim in np.atleast_1d(size))\n\n        # M\u00f6bius transform sampler: exact and numerically stable for rho&lt;1.\n        u = rng.uniform(-np.pi, np.pi, size=target_shape)\n        z = np.exp(1j * u)\n        alpha = rho_val * np.exp(1j * mu_val)\n        denom = 1.0 + rho_val * np.exp(-1j * mu_val) * z\n        tiny = 1e-15\n        mask = np.abs(denom) &lt; tiny\n        denom = np.where(mask, tiny, denom)\n        w = (z + alpha) / denom\n        angles = np.angle(w)\n        original_shape = angles.shape\n\n        if np.any(mask):\n            # Fallback to tangent sampler for rare near-pole cases.\n            count = int(np.count_nonzero(mask))\n            fallback_u = rng.uniform(0.0, 1.0, size=count)\n            factor = (1.0 + rho_val) / (1.0 - rho_val)\n            tan_term = np.tan(np.pi * (fallback_u - 0.5))\n            fallback = mu_val + 2.0 * np.arctan(factor * tan_term)\n            fallback = np.mod(fallback, two_pi)\n            angles_flat = angles.reshape(-1)\n            mask_flat = mask.reshape(-1)\n            angles_flat[mask_flat] = fallback\n            angles = angles_flat.reshape(original_shape)\n\n        theta = np.mod(angles, two_pi)\n        if target_shape == ():\n            return float(theta)\n        return theta.reshape(target_shape)\n\n    def rvs(self, mu=None, rho=None, size=None, random_state=None):\n        \"\"\"\n        Draw random variates from the Wrapped Cauchy distribution.\n\n        Parameters\n        ----------\n        mu : float, optional\n            Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n            freezing the distribution.\n        rho : float, optional\n            Shape parameter, ``0 &lt;= rho &lt; 1``. Supply explicitly or by freezing\n            the distribution.\n        size : int or tuple of ints, optional\n            Number of samples to draw. ``None`` (default) returns a scalar.\n        random_state : np.random.Generator, np.random.RandomState, or None, optional\n            Random number generator to use.\n\n        Returns\n        -------\n        samples : ndarray or float\n            Random variates on ``[0, 2\u03c0)``.\n        \"\"\"\n        mu_val = getattr(self, \"mu\", None) if mu is None else mu\n        rho_val = getattr(self, \"rho\", None) if rho is None else rho\n\n        if mu_val is None or rho_val is None:\n            raise ValueError(\"Both 'mu' and 'rho' must be provided.\")\n\n        return self._rvs(mu_val, rho_val, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        return_info=False,\n        optimizer=\"L-BFGS-B\",\n        **kwargs,\n    ):\n        \"\"\"\n        Estimate ``mu`` and ``rho`` for the wrapped Cauchy distribution.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n        weights : array_like, optional\n            Non-negative weights broadcastable to ``data``.\n        method : {\"moments\", \"mle\"}, optional\n            Estimation strategy. ``\"moments\"`` (alias: \"analytical\") returns the\n            closed-form estimates based on the first trigonometric moment.\n            ``\"mle\"`` (alias: \"numerical\") maximises the weighted log-likelihood.\n        return_info : bool, optional\n            If True, also return a diagnostic dictionary.\n        optimizer : str, optional\n            Optimiser passed to ``scipy.optimize.minimize`` when\n            ``method=\"mle\"``.\n        **kwargs :\n            Additional keyword arguments forwarded to the optimiser.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float))\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        mu_mom, rho_mom = circ_mean_and_r(alpha=x, w=w)\n        if not np.isfinite(mu_mom):\n            mu_mom = float(0.0)\n        mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n        rho_mom = float(np.clip(rho_mom, 0.0, 1.0 - 1e-12))\n\n        def nll(params):\n            mu_param, rho_param = params\n            if not (0.0 &lt;= rho_param &lt; 1.0):\n                return np.inf\n            denom = np.clip(1.0 + rho_param**2 - 2.0 * rho_param * np.cos(x - mu_param), 1e-15, None)\n            log_pdf = np.log1p(-rho_param**2) - np.log(2.0 * np.pi) - np.log(denom)\n            value = -np.sum(w * log_pdf)\n            return float(value)\n\n        def grad(params):\n            mu_param, rho_param = params\n            denom = np.clip(1.0 + rho_param**2 - 2.0 * rho_param * np.cos(x - mu_param), 1e-15, None)\n            cos_term = np.cos(x - mu_param)\n            sin_term = np.sin(x - mu_param)\n\n            inv_denom = w / denom\n            g_mu = -2.0 * rho_param * np.sum(inv_denom * sin_term)\n            g_rho = (\n                w_sum * (2.0 * rho_param / np.clip(1.0 - rho_param**2, 1e-15, None))\n                + np.sum(inv_denom * (2.0 * rho_param - 2.0 * cos_term))\n            )\n            return np.array([g_mu, g_rho], dtype=float)\n\n        method_key = method.lower()\n        alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n        method_key = alias.get(method_key, method_key)\n\n        if \"algorithm\" in kwargs:\n            optimizer = kwargs.pop(\"algorithm\")\n\n        if method_key not in {\"moments\", \"mle\"}:\n            raise ValueError(\"`method` must be one of {'moments', 'mle', 'analytical', 'numerical'}.\")\n\n        if method_key == \"moments\":\n            mu_hat = self._wrap_direction(mu_mom)\n            rho_hat = rho_mom\n            info = {\n                \"method\": \"moments\",\n                \"loglik\": float(-nll((mu_hat, rho_hat))),\n                \"n_effective\": float(n_eff),\n                \"converged\": True,\n            }\n        else:\n            bounds = [(0.0, 2.0 * np.pi), (1e-9, 1.0 - 1e-9)]\n            init = np.array([mu_mom, max(1e-3, min(rho_mom, 1.0 - 1e-3))], dtype=float)\n            result = minimize(\n                nll,\n                init,\n                method=optimizer,\n                jac=grad,\n                bounds=bounds,\n                **kwargs,\n            )\n            if not result.success:\n                raise RuntimeError(f\"wrapcauchy.fit(method='mle') failed: {result.message}\")\n            mu_hat = self._wrap_direction(float(result.x[0]))\n            rho_hat = float(np.clip(result.x[1], 1e-9, 1.0 - 1e-9))\n            info = {\n                \"method\": \"mle\",\n                \"loglik\": float(-result.fun),\n                \"n_effective\": float(n_eff),\n                \"converged\": bool(result.success),\n                \"nit\": result.nit,\n                \"grad_norm\": float(np.linalg.norm(result.jac))\n                if getattr(result, \"jac\", None) is not None\n                else np.nan,\n                \"optimizer\": optimizer,\n            }\n\n        estimates = (mu_hat, rho_hat)\n        if return_info:\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapcauchy_gen.pdf","title":"<code>pdf(x, mu, rho, *args, **kwargs)</code>","text":"<p>Probability density function of the Wrapped Cauchy distribution.</p> \\[ f(\\theta) = \\frac{1 - \\rho^2}{2\\pi(1 + \\rho^2 - 2\\rho \\cos(\\theta - \\mu))} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Shape parameter, 0 &lt; rho &lt;= 1.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Wrapped Cauchy distribution.\n\n    $$\n    f(\\theta) = \\frac{1 - \\rho^2}{2\\pi(1 + \\rho^2 - 2\\rho \\cos(\\theta - \\mu))}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Shape parameter, 0 &lt; rho &lt;= 1.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n    return super().pdf(x, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapcauchy_gen.logpdf","title":"<code>logpdf(x, mu, rho, *args, **kwargs)</code>","text":"<p>Logarithm of the probability density function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the log-PDF.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Mean resultant length, 0 &lt; rho &lt;= 1.</p> required <p>Returns:</p> Name Type Description <code>logpdf_values</code> <code>array_like</code> <p>Logarithm of the probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def logpdf(self, x, mu, rho, *args, **kwargs):\n    \"\"\"\n    Logarithm of the probability density function.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the log-PDF.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Mean resultant length, 0 &lt; rho &lt;= 1.\n\n    Returns\n    -------\n    logpdf_values : array_like\n        Logarithm of the probability density function evaluated at `x`.\n    \"\"\"\n    return super().logpdf(x, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapcauchy_gen.cdf","title":"<code>cdf(x, mu, rho, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Wrapped Cauchy distribution.</p> <p>The CDF is evaluated analytically via the wrapped Cauchy series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the CDF.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>rho</code> <code>float</code> <p>Shape parameter, 0 &lt; rho &lt;= 1.</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>CDF evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, rho, *args, **kwargs):\n    \"\"\"\n    Cumulative distribution function of the Wrapped Cauchy distribution.\n\n    The CDF is evaluated analytically via the wrapped Cauchy series.\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the CDF.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    rho : float\n        Shape parameter, 0 &lt; rho &lt;= 1.\n\n    Returns\n    -------\n    cdf_values : array_like\n        CDF evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapcauchy_gen.ppf","title":"<code>ppf(q, mu, rho, *args, **kwargs)</code>","text":"<p>Percent-point function (inverse CDF) of the Wrapped Cauchy distribution.</p> <p>The quantile is obtained by inverting the M\u00f6bius form of the CDF: $$ \\phi = 2 \\arctan!\\left(\\frac{\\tan\\left(\\pi (s-\\tfrac12)\\right)}{A}\\right), \\qquad A=\\frac{1+\\rho}{1-\\rho}, $$ where \\(s = (H(\\phi_0) + q) \\bmod 1\\) and \\(\\phi_0\\) is the anchored angle at \\(x=0\\). This matches the direct normalised CDF and keeps <code>ppf</code> in sync with <code>cdf</code> and the M\u00f6bius sampler used by <code>rvs</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, mu, rho, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (inverse CDF) of the Wrapped Cauchy distribution.\n\n    The quantile is obtained by inverting the M\u00f6bius form of the CDF:\n    $$\n    \\phi = 2 \\arctan\\!\\left(\\frac{\\tan\\left(\\pi (s-\\tfrac12)\\right)}{A}\\right),\n    \\qquad A=\\frac{1+\\rho}{1-\\rho},\n    $$\n    where $s = (H(\\phi_0) + q) \\bmod 1$ and $\\phi_0$ is the anchored angle\n    at $x=0$. This matches the direct normalised CDF and keeps ``ppf`` in\n    sync with ``cdf`` and the M\u00f6bius sampler used by ``rvs``.\n    \"\"\"\n    return super().ppf(q, mu, rho, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapcauchy_gen.rvs","title":"<code>rvs(mu=None, rho=None, size=None, random_state=None)</code>","text":"<p>Draw random variates from the Wrapped Cauchy distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Mean direction, <code>0 &lt;= mu &lt;= 2*pi</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>rho</code> <code>float</code> <p>Shape parameter, <code>0 &lt;= rho &lt; 1</code>. Supply explicitly or by freezing the distribution.</p> <code>None</code> <code>size</code> <code>int or tuple of ints</code> <p>Number of samples to draw. <code>None</code> (default) returns a scalar.</p> <code>None</code> <code>random_state</code> <code>np.random.Generator, np.random.RandomState, or None</code> <p>Random number generator to use.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray or float</code> <p>Random variates on <code>[0, 2\u03c0)</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, mu=None, rho=None, size=None, random_state=None):\n    \"\"\"\n    Draw random variates from the Wrapped Cauchy distribution.\n\n    Parameters\n    ----------\n    mu : float, optional\n        Mean direction, ``0 &lt;= mu &lt;= 2*pi``. Supply explicitly or by\n        freezing the distribution.\n    rho : float, optional\n        Shape parameter, ``0 &lt;= rho &lt; 1``. Supply explicitly or by freezing\n        the distribution.\n    size : int or tuple of ints, optional\n        Number of samples to draw. ``None`` (default) returns a scalar.\n    random_state : np.random.Generator, np.random.RandomState, or None, optional\n        Random number generator to use.\n\n    Returns\n    -------\n    samples : ndarray or float\n        Random variates on ``[0, 2\u03c0)``.\n    \"\"\"\n    mu_val = getattr(self, \"mu\", None) if mu is None else mu\n    rho_val = getattr(self, \"rho\", None) if rho is None else rho\n\n    if mu_val is None or rho_val is None:\n        raise ValueError(\"Both 'mu' and 'rho' must be provided.\")\n\n    return self._rvs(mu_val, rho_val, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapcauchy_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', return_info=False, optimizer='L-BFGS-B', **kwargs)</code>","text":"<p>Estimate <code>mu</code> and <code>rho</code> for the wrapped Cauchy distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample angles (radians). Values are wrapped to <code>[0, 2\u03c0)</code> internally.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights broadcastable to <code>data</code>.</p> <code>None</code> <code>method</code> <code>(moments, mle)</code> <p>Estimation strategy. <code>\"moments\"</code> (alias: \"analytical\") returns the closed-form estimates based on the first trigonometric moment. <code>\"mle\"</code> (alias: \"numerical\") maximises the weighted log-likelihood.</p> <code>\"moments\"</code> <code>return_info</code> <code>bool</code> <p>If True, also return a diagnostic dictionary.</p> <code>False</code> <code>optimizer</code> <code>str</code> <p>Optimiser passed to <code>scipy.optimize.minimize</code> when <code>method=\"mle\"</code>.</p> <code>'L-BFGS-B'</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to the optimiser.</p> <code>{}</code> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    return_info=False,\n    optimizer=\"L-BFGS-B\",\n    **kwargs,\n):\n    \"\"\"\n    Estimate ``mu`` and ``rho`` for the wrapped Cauchy distribution.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n    weights : array_like, optional\n        Non-negative weights broadcastable to ``data``.\n    method : {\"moments\", \"mle\"}, optional\n        Estimation strategy. ``\"moments\"`` (alias: \"analytical\") returns the\n        closed-form estimates based on the first trigonometric moment.\n        ``\"mle\"`` (alias: \"numerical\") maximises the weighted log-likelihood.\n    return_info : bool, optional\n        If True, also return a diagnostic dictionary.\n    optimizer : str, optional\n        Optimiser passed to ``scipy.optimize.minimize`` when\n        ``method=\"mle\"``.\n    **kwargs :\n        Additional keyword arguments forwarded to the optimiser.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float))\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False)\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    mu_mom, rho_mom = circ_mean_and_r(alpha=x, w=w)\n    if not np.isfinite(mu_mom):\n        mu_mom = float(0.0)\n    mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n    rho_mom = float(np.clip(rho_mom, 0.0, 1.0 - 1e-12))\n\n    def nll(params):\n        mu_param, rho_param = params\n        if not (0.0 &lt;= rho_param &lt; 1.0):\n            return np.inf\n        denom = np.clip(1.0 + rho_param**2 - 2.0 * rho_param * np.cos(x - mu_param), 1e-15, None)\n        log_pdf = np.log1p(-rho_param**2) - np.log(2.0 * np.pi) - np.log(denom)\n        value = -np.sum(w * log_pdf)\n        return float(value)\n\n    def grad(params):\n        mu_param, rho_param = params\n        denom = np.clip(1.0 + rho_param**2 - 2.0 * rho_param * np.cos(x - mu_param), 1e-15, None)\n        cos_term = np.cos(x - mu_param)\n        sin_term = np.sin(x - mu_param)\n\n        inv_denom = w / denom\n        g_mu = -2.0 * rho_param * np.sum(inv_denom * sin_term)\n        g_rho = (\n            w_sum * (2.0 * rho_param / np.clip(1.0 - rho_param**2, 1e-15, None))\n            + np.sum(inv_denom * (2.0 * rho_param - 2.0 * cos_term))\n        )\n        return np.array([g_mu, g_rho], dtype=float)\n\n    method_key = method.lower()\n    alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n    method_key = alias.get(method_key, method_key)\n\n    if \"algorithm\" in kwargs:\n        optimizer = kwargs.pop(\"algorithm\")\n\n    if method_key not in {\"moments\", \"mle\"}:\n        raise ValueError(\"`method` must be one of {'moments', 'mle', 'analytical', 'numerical'}.\")\n\n    if method_key == \"moments\":\n        mu_hat = self._wrap_direction(mu_mom)\n        rho_hat = rho_mom\n        info = {\n            \"method\": \"moments\",\n            \"loglik\": float(-nll((mu_hat, rho_hat))),\n            \"n_effective\": float(n_eff),\n            \"converged\": True,\n        }\n    else:\n        bounds = [(0.0, 2.0 * np.pi), (1e-9, 1.0 - 1e-9)]\n        init = np.array([mu_mom, max(1e-3, min(rho_mom, 1.0 - 1e-3))], dtype=float)\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            jac=grad,\n            bounds=bounds,\n            **kwargs,\n        )\n        if not result.success:\n            raise RuntimeError(f\"wrapcauchy.fit(method='mle') failed: {result.message}\")\n        mu_hat = self._wrap_direction(float(result.x[0]))\n        rho_hat = float(np.clip(result.x[1], 1e-9, 1.0 - 1e-9))\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-result.fun),\n            \"n_effective\": float(n_eff),\n            \"converged\": bool(result.success),\n            \"nit\": result.nit,\n            \"grad_norm\": float(np.linalg.norm(result.jac))\n            if getattr(result, \"jac\", None) is not None\n            else np.nan,\n            \"optimizer\": optimizer,\n        }\n\n    estimates = (mu_hat, rho_hat)\n    if return_info:\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen","title":"<code>vonmises_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Von Mises Distribution</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Percent-point function (inverse of CDF).</p> <code>rvs</code> <p>Random variates.</p> <code>fit</code> <p>Fit the distribution to the data and return the parameters (mu, kappa).</p> <p>Examples:</p> <pre><code>from pycircstat2.distributions import vonmises\n</code></pre> References <ul> <li>Section 4.3.8 of Pewsey et al. (2014)</li> </ul> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class vonmises_gen(CircularContinuous):\n    \"\"\"Von Mises Distribution\n\n    ![vonmises](../images/circ-mod-vonmises.png)\n\n    Methods\n    -------\n    pdf(x, mu, kappa)\n        Probability density function.\n\n    cdf(x, mu, kappa)\n        Cumulative distribution function.\n\n    ppf(q, mu, kappa)\n        Percent-point function (inverse of CDF).\n\n    rvs(mu, kappa, size=None, random_state=None)\n        Random variates.\n\n    fit(data, *args, **kwargs)\n        Fit the distribution to the data and return the parameters (mu, kappa).\n\n    Examples\n    --------\n    ```\n    from pycircstat2.distributions import vonmises\n    ```\n\n    References\n    ----------\n    - Section 4.3.8 of Pewsey et al. (2014)\n\n    \"\"\"\n\n    _freeze_doc = \"\"\"\n    Freeze the distribution with specific parameters.\n\n    Parameters\n    ----------\n    mu : float\n        The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n    kappa : float\n        The concentration parameter of the distribution (kappa &gt; 0).\n\n    Returns\n    -------\n    rv_frozen : rv_frozen instance\n        The frozen distribution instance with fixed parameters.\n    \"\"\"\n\n    def __call__(self, *args, **kwds):\n        return self.freeze(*args, **kwds)\n\n    __call__.__doc__ = _freeze_doc\n\n    def _argcheck(self, mu, kappa):\n        try:\n            mu_arr, kappa_arr = np.broadcast_arrays(mu, kappa)\n        except ValueError:\n            return False\n        return (\n            (mu_arr &gt;= 0.0)\n            &amp; (mu_arr &lt;= 2.0 * np.pi)\n            &amp; (kappa_arr &gt; 0.0)\n        )\n\n    def _pdf(self, x, mu, kappa):\n        return np.exp(kappa * np.cos(x - mu)) / (2 * np.pi * i0(kappa))\n\n    def pdf(self, x, mu, kappa, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Von Mises distribution.\n\n        $$\n        f(\\theta) = \\frac{e^{\\kappa \\cos(\\theta - \\mu)}}{2\\pi I_0(\\kappa)}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        mu : float\n            The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n        kappa : float\n            The concentration parameter of the distribution (kappa &gt; 0).\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n        return super().pdf(x, mu, kappa, *args, **kwargs)\n\n    def _logpdf(self, x, mu, kappa):\n        return kappa * np.cos(x - mu) - np.log(2 * np.pi * i0(kappa))\n\n    def logpdf(self, x, mu, kappa, *args, **kwargs):\n        \"\"\"\n        Logarithm of the probability density function of the Von Mises\n        distribution.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the logarithm of the probability density function.\n        mu : float\n            The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n        kappa : float\n            The concentration parameter of the distribution (kappa &gt; 0).\n\n        Returns\n        -------\n        logpdf_values : array_like\n            Logarithm of the probability density function evaluated at `x`.\n        \"\"\"\n        return super().logpdf(x, mu, kappa, *args, **kwargs)\n\n    def _cdf(self, x, mu, kappa):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n\n        if flat.size == 0:\n            return arr.astype(float)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        kappa_arr = np.asarray(kappa, dtype=float)\n\n        mu_val = float(mu_arr.reshape(-1)[0])\n        if mu_arr.size &gt; 1 and not np.allclose(mu_arr, mu_val, atol=0.0, rtol=0.0):\n            raise ValueError(\"vonmises parameters must be broadcastable scalars.\")\n\n        kappa_val = float(kappa_arr.reshape(-1)[0])\n        if kappa_arr.size &gt; 1 and not np.allclose(kappa_arr, kappa_val, atol=0.0, rtol=0.0):\n            raise ValueError(\"vonmises parameters must be broadcastable scalars.\")\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt; 1e-9:\n            uniform = flat / two_pi\n            if arr.ndim == 0:\n                value = float(uniform[0])\n                return 1.0 if np.isclose(float(wrapped), two_pi) else value\n            result = uniform.reshape(arr.shape)\n            result[np.isclose(arr, two_pi)] = 1.0\n            return result\n\n        denom = i0(kappa_val)\n        if not np.isfinite(denom) or denom == 0.0:\n            return self._cdf_from_pdf(x, mu, kappa)\n\n        phi = (flat - mu_val + np.pi) % two_pi - np.pi\n        base_phi = (-mu_val + np.pi) % two_pi - np.pi\n\n        term_sum = np.zeros_like(phi)\n        term_base = 0.0\n        tol = 1e-12\n        max_terms = 500\n        converged = False\n\n        for n in range(1, max_terms + 1):\n            coeff = iv(n, kappa_val) / (denom * n)\n            if not np.isfinite(coeff):\n                continue\n\n            term = coeff * np.sin(n * phi)\n            term_sum += term\n            term_base += coeff * np.sin(n * base_phi)\n\n            max_term = np.max(np.abs(term))\n            if max_term &lt; tol and abs(coeff) &lt; tol:\n                converged = True\n                break\n\n        if not converged:\n            return self._cdf_from_pdf(x, mu, kappa)\n\n        cdf_raw = 0.5 + phi / two_pi + (1.0 / np.pi) * term_sum\n        base_val = 0.5 + base_phi / two_pi + (1.0 / np.pi) * term_base\n\n        forward = np.clip(cdf_raw - base_val, 0.0, 1.0)\n        backward = np.clip(base_val - cdf_raw, 0.0, 1.0)\n        cdf = np.where(phi &gt;= base_phi, forward, 1.0 - backward)\n        cdf = np.clip(cdf, 0.0, 1.0)\n\n        if arr.ndim == 0:\n            value = float(cdf[0])\n            return 1.0 if np.isclose(float(wrapped), two_pi) else value\n\n        result = cdf.reshape(arr.shape)\n        result[np.isclose(arr, two_pi)] = 1.0\n        return result\n\n    def cdf(self, x, mu, kappa, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the Von Mises distribution.\n\n        $$\n        F(\\theta) = \\frac{1}{2 \\pi I_0(\\kappa)}\\int_{0}^{\\theta} e^{\\kappa \\cos(\\theta - \\mu)} dx\n        $$\n\n        The CDF is evaluated via its Fourier-Bessel series expansion,\n        $$\n        F(\\theta) = \\frac{1}{2} + \\frac{\\theta - \\mu}{2\\pi}\n        + \\frac{1}{\\pi}\\sum_{n=1}^{\\infty} \\frac{I_n(\\kappa)}{I_0(\\kappa)\\,n}\n        \\sin\\bigl(n(\\theta - \\mu)\\bigr),\n        $$\n        truncated adaptively for numerical stability and re-normalised to the\n        $[0, 2\\pi)$ support.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n        mu : float\n            The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n        kappa : float\n            The concentration parameter of the distribution (kappa &gt; 0).\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative distribution function evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, mu, kappa, *args, **kwargs)\n\n    def _ppf(self, q, mu, kappa):\n        mu_arr = np.asarray(mu, dtype=float)\n        kappa_arr = np.asarray(kappa, dtype=float)\n\n        mu_val = float(np.mod(mu_arr.reshape(-1)[0], 2.0 * np.pi))\n        kappa_val = float(kappa_arr.reshape(-1)[0])\n        if kappa_val &lt; 0.0:\n            raise ValueError(\"`kappa` must be non-negative.\")\n\n        q_arr = np.asarray(q, dtype=float)\n        flat = q_arr.reshape(-1)\n        if flat.size == 0:\n            return q_arr.astype(float)\n\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        lower_mask = flat &lt;= 0.0\n        upper_mask = flat &gt;= 1.0\n        result[lower_mask] = 0.0\n        result[upper_mask] = 2.0 * np.pi\n\n        interior = ~(lower_mask | upper_mask)\n        if not np.any(interior):\n            return result.reshape(q_arr.shape)\n\n        q_int = flat[interior]\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt;= 1e-9:\n            result[interior] = (two_pi * q_int) % two_pi\n            return result.reshape(q_arr.shape)\n\n        eps = 1e-15\n        q_clipped = np.clip(q_int, eps, 1.0 - eps)\n\n        theta = (mu_val + two_pi * (q_clipped - 0.5)) % two_pi\n        if kappa_val &lt; 0.3:\n            theta = (two_pi * q_clipped) % two_pi\n        elif kappa_val &gt; 5.0:\n            normal_guess = mu_val + ndtri(q_clipped) / np.sqrt(kappa_val)\n            normal_guess = np.mod(normal_guess, two_pi)\n            blend = 0.5 if kappa_val &lt; 20.0 else 0.8\n            theta = np.mod(blend * normal_guess + (1.0 - blend) * (two_pi * q_clipped), two_pi)\n\n        L = np.zeros_like(theta)\n        H = np.full_like(theta, two_pi)\n\n        tol_cdf = 1e-12\n        tol_theta = 1e-10\n        max_iter = 6\n\n        theta_curr = theta.copy()\n        for _ in range(max_iter):\n            cdf_vals = np.asarray(self.cdf(theta_curr, mu_val, kappa_val), dtype=float)\n            pdf_vals = np.exp(kappa_val * np.cos(theta_curr - mu_val)) / (2.0 * np.pi * i0(kappa_val))\n            delta = cdf_vals - q_clipped\n\n            L = np.where(delta &lt;= 0.0, theta_curr, L)\n            H = np.where(delta &gt; 0.0, theta_curr, H)\n\n            converged = (np.abs(delta) &lt;= tol_cdf) &amp; ((H - L) &lt;= tol_theta)\n            if np.all(converged):\n                break\n\n            denom = np.where(pdf_vals &gt; 1e-15, pdf_vals, 1e-15)\n            step = np.clip(delta / denom, -np.pi, np.pi)\n            theta_next = theta_curr - step\n            midpoint = 0.5 * (L + H)\n            theta_next = np.where((theta_next &lt;= L) | (theta_next &gt;= H), midpoint, theta_next)\n            theta_next = np.mod(theta_next, two_pi)\n            theta_curr = theta_next\n\n        delta = np.asarray(self.cdf(theta_curr, mu_val, kappa_val), dtype=float) - q_clipped\n        mask = (np.abs(delta) &gt; tol_cdf) | ((H - L) &gt; tol_theta)\n        if np.any(mask):\n            theta_b = theta_curr.copy()\n            L_b = L.copy()\n            H_b = H.copy()\n            for _ in range(30):\n                if not np.any(mask):\n                    break\n                mid = 0.5 * (L_b + H_b)\n                mid_vals = np.asarray(self.cdf(mid, mu_val, kappa_val), dtype=float)\n                delta_mid = mid_vals - q_clipped\n                take_upper = (delta_mid &gt; 0.0) &amp; mask\n                take_lower = (~take_upper) &amp; mask\n                H_b = np.where(take_upper, mid, H_b)\n                L_b = np.where(take_lower, mid, L_b)\n                theta_b = np.where(mask, mid, theta_b)\n                mask = mask &amp; (np.abs(delta_mid) &gt; tol_cdf)\n            theta_curr = np.where(mask, 0.5 * (L_b + H_b), theta_b)\n\n        result[interior] = np.mod(theta_curr, two_pi)\n        return result.reshape(q_arr.shape)\n\n\n    def ppf(self, q, mu, kappa, *args, **kwargs):\n        \"\"\"\n        Percent-point function (inverse of the CDF) of the Von Mises distribution.\n\n        The quantile is obtained by inverting the analytic Fourier\u2013Bessel series\n        using a safeguarded Newton iteration with the exact von Mises PDF as the\n        slope, followed by a bisection polish.\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles to evaluate.\n        mu : float\n            The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n        kappa : float\n            The concentration parameter of the distribution (kappa &gt; 0).\n\n        Returns\n        -------\n        ppf_values : array_like\n            Values at the given quantiles.\n        \"\"\"\n        return super().ppf(q, mu, kappa, *args, **kwargs)\n\n    def _rvs(self, mu, kappa, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        mu_arr = np.asarray(mu, dtype=float)\n        kappa_arr = np.asarray(kappa, dtype=float)\n\n        mu_val = float(mu_arr.reshape(-1)[0])\n        if mu_arr.size &gt; 1 and not np.allclose(mu_arr, mu_val, atol=0.0, rtol=0.0):\n            raise ValueError(\"vonmises parameters must be broadcastable scalars.\")\n        mu_val = float(np.mod(mu_val, 2.0 * np.pi))\n\n        kappa_val = float(kappa_arr.reshape(-1)[0])\n        if kappa_arr.size &gt; 1 and not np.allclose(kappa_arr, kappa_val, atol=0.0, rtol=0.0):\n            raise ValueError(\"vonmises parameters must be broadcastable scalars.\")\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt;= 1e-9:\n            return rng.uniform(0.0, two_pi, size=size)\n\n        a = 1.0 + np.sqrt(1.0 + 4.0 * kappa_val**2)\n        b = (a - np.sqrt(2.0 * a)) / (2.0 * kappa_val)\n        r = (1.0 + b**2) / (2.0 * b)\n\n        if size is None:\n            samples = np.empty(1, dtype=float)\n            target_shape = ()\n        elif np.isscalar(size):\n            samples = np.empty(int(size), dtype=float)\n            target_shape = (int(size),)\n        else:\n            target_shape = tuple(int(s) for s in np.atleast_1d(size))\n            samples = np.empty(int(np.prod(target_shape)), dtype=float)\n\n        total = samples.size\n        for idx in range(total):\n            while True:\n                u1 = rng.uniform()\n                z = np.cos(np.pi * u1)\n                f = (1.0 + r * z) / (r + z)\n                c = kappa_val * (r - f)\n                u2 = rng.uniform()\n                if u2 &lt; c * (2.0 - c) or u2 &lt;= c * np.exp(1.0 - c):\n                    break\n            u3 = rng.uniform()\n            theta = mu_val + np.sign(u3 - 0.5) * np.arccos(f)\n            samples[idx] = np.mod(theta, two_pi)\n\n        if target_shape == ():\n            return float(samples[0])\n        return samples.reshape(target_shape)\n\n    def rvs(self, size=None, random_state=None, *args, **kwargs):\n        \"\"\"\n        Draw random variates.\n\n        Parameters\n        ----------\n        size : int or tuple, optional\n            Number of samples to generate.\n        random_state : RandomState, optional\n            Random number generator instance.\n\n        Returns\n        -------\n        samples : ndarray\n            Random variates.\n        \"\"\"\n        # Check if instance-level parameters are set\n        mu = getattr(self, \"mu\", None)\n        kappa = getattr(self, \"kappa\", None)\n\n        # Override instance parameters if provided in args/kwargs\n        mu = kwargs.pop(\"mu\", mu)\n        kappa = kwargs.pop(\"kappa\", kappa)\n\n        # Ensure required parameters are provided\n        if mu is None or kappa is None:\n            raise ValueError(\"Both 'mu' and 'kappa' must be provided.\")\n\n        # Call the private _rvs method\n        return self._rvs(mu, kappa, size=size, random_state=random_state)\n\n    def support(self, *args, **kwargs):\n        return (0, 2 * np.pi)\n\n    def mean(self, *args, **kwargs):\n        \"\"\"\n        Circular mean of the Von Mises distribution.\n\n        Returns\n        -------\n        mean : float\n            The circular mean direction (in radians), equal to `mu`.\n        \"\"\"\n        (mu, _) = self._parse_args(*args, **kwargs)[0]\n        return mu\n\n    def median(self, *args, **kwargs):\n        \"\"\"\n        Circular median of the Von Mises distribution.\n\n        Returns\n        -------\n        median : float\n            The circular median direction (in radians), equal to `mu`.\n        \"\"\"\n        return self.mean(*args, **kwargs)\n\n    def var(self, *args, **kwargs):\n        \"\"\"\n        Circular variance of the Von Mises distribution.\n\n        Returns\n        -------\n        variance : float\n            The circular variance, derived from `kappa`.\n        \"\"\"\n        (_, kappa) = self._parse_args(*args, **kwargs)[0]\n        return 1 - i1(kappa) / i0(kappa)\n\n    def std(self, *args, **kwargs):\n        \"\"\"\n        Circular standard deviation of the Von Mises distribution.\n\n        Returns\n        -------\n        std : float\n            The circular standard deviation, derived from `kappa`.\n        \"\"\"\n        (_, kappa) = self._parse_args(*args, **kwargs)[0]\n        r = i1(kappa) / i0(kappa)\n\n        return np.sqrt(-2 * np.log(r))\n\n    def entropy(self, *args, **kwargs):\n        \"\"\"\n        Entropy of the Von Mises distribution.\n\n        Returns\n        -------\n        entropy : float\n            The entropy of the distribution.\n        \"\"\"\n        (_, kappa) = self._parse_args(*args, **kwargs)[0]\n        return -np.log(i0(kappa)) + (kappa * i1(kappa)) / i0(kappa)\n\n    def _nnlf(self, theta, data):\n        \"\"\"\n        Custom negative log-likelihood function for the Von Mises distribution.\n        \"\"\"\n        mu, kappa = theta\n\n        if not self._argcheck(mu, kappa):  # Validate parameter range\n            return np.inf\n\n        # Compute log-likelihood robustly\n        log_likelihood = self._logpdf(data, mu, kappa)\n\n        # Negative log-likelihood\n        return -np.sum(log_likelihood)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        return_info=False,\n        optimizer=\"L-BFGS-B\",\n        **kwargs,\n    ):\n        \"\"\"\n        Estimate ``mu`` and ``kappa`` for the von Mises distribution.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n        weights : array_like, optional\n            Non-negative weights broadcastable to ``data``.\n        method : {\"moments\", \"mle\"}, optional\n            Estimation strategy. ``\"moments\"`` (alias ``\"analytical\"``) returns\n            the circular mean together with the standard approximation for\n            ``kappa``. ``\"mle\"`` (alias ``\"numerical\"``) maximises the weighted\n            log-likelihood using a bounded optimiser.\n        return_info : bool, optional\n            If True, return a diagnostics dictionary alongside the estimates.\n        optimizer : str, optional\n            Optimiser passed to ``scipy.optimize.minimize`` when\n            ``method=\"mle\"``.\n        **kwargs :\n            Additional keyword arguments forwarded to the optimiser.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        mu_mom, r_mom = circ_mean_and_r(alpha=x, w=w)\n        if not np.isfinite(mu_mom):\n            mu_mom = float(0.0)\n        mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n        r_mom = float(np.clip(r_mom, 1e-12, 1.0 - 1e-12))\n        n_adjust = int(max(1, round(w_sum)))\n        kappa_mom = float(np.clip(circ_kappa(r=r_mom, n=n_adjust), 1e-9, 1e6))\n\n        method_key = method.lower()\n        alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n        method_key = alias.get(method_key, method_key)\n\n        if \"algorithm\" in kwargs:\n            optimizer = kwargs.pop(\"algorithm\")\n\n        if method_key not in {\"moments\", \"mle\"}:\n            raise ValueError(\"`method` must be one of {'moments', 'mle', 'analytical', 'numerical'}.\")\n\n        def nll(params):\n            mu_param, kappa_param = params\n            if not (kappa_param &gt; 0.0):\n                return np.inf\n            cos_term = np.cos(x - mu_param)\n            sum_cos = np.sum(w * cos_term)\n            log_i0_val = np.log(i0(kappa_param))\n            return float(\n                -kappa_param * sum_cos + w_sum * (np.log(2.0 * np.pi) + log_i0_val)\n            )\n\n        def grad(params):\n            mu_param, kappa_param = params\n            cos_term = np.cos(x - mu_param)\n            sin_term = np.sin(x - mu_param)\n            sum_sin = np.sum(w * sin_term)\n            sum_cos = np.sum(w * cos_term)\n            ratio = i1(kappa_param) / i0(kappa_param)\n            g_mu = kappa_param * sum_sin\n            g_kappa = -sum_cos + w_sum * ratio\n            return np.array([g_mu, g_kappa], dtype=float)\n\n        if method_key == \"moments\":\n            mu_hat = self._wrap_direction(mu_mom)\n            kappa_hat = kappa_mom\n            info = {\n                \"method\": \"moments\",\n                \"loglik\": float(-nll((mu_hat, kappa_hat))),\n                \"n_effective\": float(n_eff),\n                \"converged\": True,\n            }\n        else:\n            bounds = [(0.0, 2.0 * np.pi), (1e-9, 1e6)]\n            init = np.array([mu_mom, kappa_mom], dtype=float)\n            result = minimize(\n                nll,\n                init,\n                method=optimizer,\n                jac=grad,\n                bounds=bounds,\n                **kwargs,\n            )\n            if not result.success:\n                raise RuntimeError(\n                    f\"vonmises.fit(method='mle') failed: {result.message}\"\n                )\n            mu_hat = self._wrap_direction(float(result.x[0]))\n            kappa_hat = float(np.clip(result.x[1], 1e-9, 1e6))\n            info = {\n                \"method\": \"mle\",\n                \"loglik\": float(-result.fun),\n                \"n_effective\": float(n_eff),\n                \"converged\": bool(result.success),\n                \"nit\": result.nit,\n                \"grad_norm\": float(np.linalg.norm(result.jac))\n                if getattr(result, \"jac\", None) is not None\n                else np.nan,\n                \"optimizer\": optimizer,\n            }\n\n        estimates = (mu_hat, kappa_hat)\n        if return_info:\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.pdf","title":"<code>pdf(x, mu, kappa, *args, **kwargs)</code>","text":"<p>Probability density function of the Von Mises distribution.</p> \\[ f(\\theta) = \\frac{e^{\\kappa \\cos(\\theta - \\mu)}}{2\\pi I_0(\\kappa)} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>mu</code> <code>float</code> <p>The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).</p> required <code>kappa</code> <code>float</code> <p>The concentration parameter of the distribution (kappa &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, kappa, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Von Mises distribution.\n\n    $$\n    f(\\theta) = \\frac{e^{\\kappa \\cos(\\theta - \\mu)}}{2\\pi I_0(\\kappa)}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    mu : float\n        The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n    kappa : float\n        The concentration parameter of the distribution (kappa &gt; 0).\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n    return super().pdf(x, mu, kappa, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.logpdf","title":"<code>logpdf(x, mu, kappa, *args, **kwargs)</code>","text":"<p>Logarithm of the probability density function of the Von Mises distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the logarithm of the probability density function.</p> required <code>mu</code> <code>float</code> <p>The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).</p> required <code>kappa</code> <code>float</code> <p>The concentration parameter of the distribution (kappa &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>logpdf_values</code> <code>array_like</code> <p>Logarithm of the probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def logpdf(self, x, mu, kappa, *args, **kwargs):\n    \"\"\"\n    Logarithm of the probability density function of the Von Mises\n    distribution.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the logarithm of the probability density function.\n    mu : float\n        The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n    kappa : float\n        The concentration parameter of the distribution (kappa &gt; 0).\n\n    Returns\n    -------\n    logpdf_values : array_like\n        Logarithm of the probability density function evaluated at `x`.\n    \"\"\"\n    return super().logpdf(x, mu, kappa, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.cdf","title":"<code>cdf(x, mu, kappa, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Von Mises distribution.</p> \\[ F(\\theta) = \\frac{1}{2 \\pi I_0(\\kappa)}\\int_{0}^{\\theta} e^{\\kappa \\cos(\\theta - \\mu)} dx \\] <p>The CDF is evaluated via its Fourier-Bessel series expansion, $$ F(\\theta) = \\frac{1}{2} + \\frac{\\theta - \\mu}{2\\pi} + \\frac{1}{\\pi}\\sum_{n=1}^{\\infty} \\frac{I_n(\\kappa)}{I_0(\\kappa)\\,n} \\sin\\bigl(n(\\theta - \\mu)\\bigr), $$ truncated adaptively for numerical stability and re-normalised to the \\([0, 2\\pi)\\) support.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <code>mu</code> <code>float</code> <p>The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).</p> required <code>kappa</code> <code>float</code> <p>The concentration parameter of the distribution (kappa &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative distribution function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, kappa, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the Von Mises distribution.\n\n    $$\n    F(\\theta) = \\frac{1}{2 \\pi I_0(\\kappa)}\\int_{0}^{\\theta} e^{\\kappa \\cos(\\theta - \\mu)} dx\n    $$\n\n    The CDF is evaluated via its Fourier-Bessel series expansion,\n    $$\n    F(\\theta) = \\frac{1}{2} + \\frac{\\theta - \\mu}{2\\pi}\n    + \\frac{1}{\\pi}\\sum_{n=1}^{\\infty} \\frac{I_n(\\kappa)}{I_0(\\kappa)\\,n}\n    \\sin\\bigl(n(\\theta - \\mu)\\bigr),\n    $$\n    truncated adaptively for numerical stability and re-normalised to the\n    $[0, 2\\pi)$ support.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n    mu : float\n        The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n    kappa : float\n        The concentration parameter of the distribution (kappa &gt; 0).\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative distribution function evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, mu, kappa, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.ppf","title":"<code>ppf(q, mu, kappa, *args, **kwargs)</code>","text":"<p>Percent-point function (inverse of the CDF) of the Von Mises distribution.</p> <p>The quantile is obtained by inverting the analytic Fourier\u2013Bessel series using a safeguarded Newton iteration with the exact von Mises PDF as the slope, followed by a bisection polish.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles to evaluate.</p> required <code>mu</code> <code>float</code> <p>The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).</p> required <code>kappa</code> <code>float</code> <p>The concentration parameter of the distribution (kappa &gt; 0).</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Values at the given quantiles.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, mu, kappa, *args, **kwargs):\n    \"\"\"\n    Percent-point function (inverse of the CDF) of the Von Mises distribution.\n\n    The quantile is obtained by inverting the analytic Fourier\u2013Bessel series\n    using a safeguarded Newton iteration with the exact von Mises PDF as the\n    slope, followed by a bisection polish.\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles to evaluate.\n    mu : float\n        The mean direction of the distribution (0 &lt;= mu &lt;= 2*pi).\n    kappa : float\n        The concentration parameter of the distribution (kappa &gt; 0).\n\n    Returns\n    -------\n    ppf_values : array_like\n        Values at the given quantiles.\n    \"\"\"\n    return super().ppf(q, mu, kappa, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.rvs","title":"<code>rvs(size=None, random_state=None, *args, **kwargs)</code>","text":"<p>Draw random variates.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int or tuple</code> <p>Number of samples to generate.</p> <code>None</code> <code>random_state</code> <code>RandomState</code> <p>Random number generator instance.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray</code> <p>Random variates.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, size=None, random_state=None, *args, **kwargs):\n    \"\"\"\n    Draw random variates.\n\n    Parameters\n    ----------\n    size : int or tuple, optional\n        Number of samples to generate.\n    random_state : RandomState, optional\n        Random number generator instance.\n\n    Returns\n    -------\n    samples : ndarray\n        Random variates.\n    \"\"\"\n    # Check if instance-level parameters are set\n    mu = getattr(self, \"mu\", None)\n    kappa = getattr(self, \"kappa\", None)\n\n    # Override instance parameters if provided in args/kwargs\n    mu = kwargs.pop(\"mu\", mu)\n    kappa = kwargs.pop(\"kappa\", kappa)\n\n    # Ensure required parameters are provided\n    if mu is None or kappa is None:\n        raise ValueError(\"Both 'mu' and 'kappa' must be provided.\")\n\n    # Call the private _rvs method\n    return self._rvs(mu, kappa, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.mean","title":"<code>mean(*args, **kwargs)</code>","text":"<p>Circular mean of the Von Mises distribution.</p> <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>The circular mean direction (in radians), equal to <code>mu</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def mean(self, *args, **kwargs):\n    \"\"\"\n    Circular mean of the Von Mises distribution.\n\n    Returns\n    -------\n    mean : float\n        The circular mean direction (in radians), equal to `mu`.\n    \"\"\"\n    (mu, _) = self._parse_args(*args, **kwargs)[0]\n    return mu\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.median","title":"<code>median(*args, **kwargs)</code>","text":"<p>Circular median of the Von Mises distribution.</p> <p>Returns:</p> Name Type Description <code>median</code> <code>float</code> <p>The circular median direction (in radians), equal to <code>mu</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def median(self, *args, **kwargs):\n    \"\"\"\n    Circular median of the Von Mises distribution.\n\n    Returns\n    -------\n    median : float\n        The circular median direction (in radians), equal to `mu`.\n    \"\"\"\n    return self.mean(*args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.var","title":"<code>var(*args, **kwargs)</code>","text":"<p>Circular variance of the Von Mises distribution.</p> <p>Returns:</p> Name Type Description <code>variance</code> <code>float</code> <p>The circular variance, derived from <code>kappa</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def var(self, *args, **kwargs):\n    \"\"\"\n    Circular variance of the Von Mises distribution.\n\n    Returns\n    -------\n    variance : float\n        The circular variance, derived from `kappa`.\n    \"\"\"\n    (_, kappa) = self._parse_args(*args, **kwargs)[0]\n    return 1 - i1(kappa) / i0(kappa)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.std","title":"<code>std(*args, **kwargs)</code>","text":"<p>Circular standard deviation of the Von Mises distribution.</p> <p>Returns:</p> Name Type Description <code>std</code> <code>float</code> <p>The circular standard deviation, derived from <code>kappa</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def std(self, *args, **kwargs):\n    \"\"\"\n    Circular standard deviation of the Von Mises distribution.\n\n    Returns\n    -------\n    std : float\n        The circular standard deviation, derived from `kappa`.\n    \"\"\"\n    (_, kappa) = self._parse_args(*args, **kwargs)[0]\n    r = i1(kappa) / i0(kappa)\n\n    return np.sqrt(-2 * np.log(r))\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.entropy","title":"<code>entropy(*args, **kwargs)</code>","text":"<p>Entropy of the Von Mises distribution.</p> <p>Returns:</p> Name Type Description <code>entropy</code> <code>float</code> <p>The entropy of the distribution.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def entropy(self, *args, **kwargs):\n    \"\"\"\n    Entropy of the Von Mises distribution.\n\n    Returns\n    -------\n    entropy : float\n        The entropy of the distribution.\n    \"\"\"\n    (_, kappa) = self._parse_args(*args, **kwargs)[0]\n    return -np.log(i0(kappa)) + (kappa * i1(kappa)) / i0(kappa)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', return_info=False, optimizer='L-BFGS-B', **kwargs)</code>","text":"<p>Estimate <code>mu</code> and <code>kappa</code> for the von Mises distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample angles (radians). Values are wrapped to <code>[0, 2\u03c0)</code> internally.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights broadcastable to <code>data</code>.</p> <code>None</code> <code>method</code> <code>(moments, mle)</code> <p>Estimation strategy. <code>\"moments\"</code> (alias <code>\"analytical\"</code>) returns the circular mean together with the standard approximation for <code>kappa</code>. <code>\"mle\"</code> (alias <code>\"numerical\"</code>) maximises the weighted log-likelihood using a bounded optimiser.</p> <code>\"moments\"</code> <code>return_info</code> <code>bool</code> <p>If True, return a diagnostics dictionary alongside the estimates.</p> <code>False</code> <code>optimizer</code> <code>str</code> <p>Optimiser passed to <code>scipy.optimize.minimize</code> when <code>method=\"mle\"</code>.</p> <code>'L-BFGS-B'</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to the optimiser.</p> <code>{}</code> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    return_info=False,\n    optimizer=\"L-BFGS-B\",\n    **kwargs,\n):\n    \"\"\"\n    Estimate ``mu`` and ``kappa`` for the von Mises distribution.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample angles (radians). Values are wrapped to ``[0, 2\u03c0)`` internally.\n    weights : array_like, optional\n        Non-negative weights broadcastable to ``data``.\n    method : {\"moments\", \"mle\"}, optional\n        Estimation strategy. ``\"moments\"`` (alias ``\"analytical\"``) returns\n        the circular mean together with the standard approximation for\n        ``kappa``. ``\"mle\"`` (alias ``\"numerical\"``) maximises the weighted\n        log-likelihood using a bounded optimiser.\n    return_info : bool, optional\n        If True, return a diagnostics dictionary alongside the estimates.\n    optimizer : str, optional\n        Optimiser passed to ``scipy.optimize.minimize`` when\n        ``method=\"mle\"``.\n    **kwargs :\n        Additional keyword arguments forwarded to the optimiser.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    mu_mom, r_mom = circ_mean_and_r(alpha=x, w=w)\n    if not np.isfinite(mu_mom):\n        mu_mom = float(0.0)\n    mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n    r_mom = float(np.clip(r_mom, 1e-12, 1.0 - 1e-12))\n    n_adjust = int(max(1, round(w_sum)))\n    kappa_mom = float(np.clip(circ_kappa(r=r_mom, n=n_adjust), 1e-9, 1e6))\n\n    method_key = method.lower()\n    alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n    method_key = alias.get(method_key, method_key)\n\n    if \"algorithm\" in kwargs:\n        optimizer = kwargs.pop(\"algorithm\")\n\n    if method_key not in {\"moments\", \"mle\"}:\n        raise ValueError(\"`method` must be one of {'moments', 'mle', 'analytical', 'numerical'}.\")\n\n    def nll(params):\n        mu_param, kappa_param = params\n        if not (kappa_param &gt; 0.0):\n            return np.inf\n        cos_term = np.cos(x - mu_param)\n        sum_cos = np.sum(w * cos_term)\n        log_i0_val = np.log(i0(kappa_param))\n        return float(\n            -kappa_param * sum_cos + w_sum * (np.log(2.0 * np.pi) + log_i0_val)\n        )\n\n    def grad(params):\n        mu_param, kappa_param = params\n        cos_term = np.cos(x - mu_param)\n        sin_term = np.sin(x - mu_param)\n        sum_sin = np.sum(w * sin_term)\n        sum_cos = np.sum(w * cos_term)\n        ratio = i1(kappa_param) / i0(kappa_param)\n        g_mu = kappa_param * sum_sin\n        g_kappa = -sum_cos + w_sum * ratio\n        return np.array([g_mu, g_kappa], dtype=float)\n\n    if method_key == \"moments\":\n        mu_hat = self._wrap_direction(mu_mom)\n        kappa_hat = kappa_mom\n        info = {\n            \"method\": \"moments\",\n            \"loglik\": float(-nll((mu_hat, kappa_hat))),\n            \"n_effective\": float(n_eff),\n            \"converged\": True,\n        }\n    else:\n        bounds = [(0.0, 2.0 * np.pi), (1e-9, 1e6)]\n        init = np.array([mu_mom, kappa_mom], dtype=float)\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            jac=grad,\n            bounds=bounds,\n            **kwargs,\n        )\n        if not result.success:\n            raise RuntimeError(\n                f\"vonmises.fit(method='mle') failed: {result.message}\"\n            )\n        mu_hat = self._wrap_direction(float(result.x[0]))\n        kappa_hat = float(np.clip(result.x[1], 1e-9, 1e6))\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-result.fun),\n            \"n_effective\": float(n_eff),\n            \"converged\": bool(result.success),\n            \"nit\": result.nit,\n            \"grad_norm\": float(np.linalg.norm(result.jac))\n            if getattr(result, \"jac\", None) is not None\n            else np.nan,\n            \"optimizer\": optimizer,\n        }\n\n    estimates = (mu_hat, kappa_hat)\n    if return_info:\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_flattopped_gen","title":"<code>vonmises_flattopped_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Flat-topped von Mises Distribution</p> <p>The Flat-topped von Mises distribution is a modification of the von Mises distribution that allows for more flexible peak shapes, including flattened or sharper tops, depending on the value of the shape parameter \\(\\nu\\).</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> Note <p>Parameters must be scalar; cached normalization tables are built per parameter set. Implementation based on Section 4.3.10 of Pewsey et al. (2014)</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class vonmises_flattopped_gen(CircularContinuous):\n    r\"\"\"Flat-topped von Mises Distribution\n\n    The Flat-topped von Mises distribution is a modification of the von Mises distribution\n    that allows for more flexible peak shapes, including flattened or sharper tops, depending\n    on the value of the shape parameter $\\nu$.\n\n    ![vonmises-ext](../images/circ-mod-vonmises-flat-topped.png)\n\n    Methods\n    -------\n    pdf(x, mu, kappa, nu)\n        Probability density function.\n\n    cdf(x, mu, kappa, nu)\n        Cumulative distribution function.\n\n    Note\n    ----\n    Parameters must be scalar; cached normalization tables are built per parameter set.\n    Implementation based on Section 4.3.10 of Pewsey et al. (2014)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._vmft_table_cache = {}\n        self._vmft_sampler_cache = {}\n\n    def _validate_params(self, mu, kappa, nu):\n        mu_arr, kappa_arr, nu_arr = np.broadcast_arrays(mu, kappa, nu)\n        return (\n            (mu_arr &gt;= 0.0)\n            &amp; (mu_arr &lt;= 2.0 * np.pi)\n            &amp; (kappa_arr &gt;= 0.0)\n            &amp; (kappa_arr &lt;= _VMFT_KAPPA_UPPER)\n            &amp; (nu_arr &gt;= -1.0)\n            &amp; (nu_arr &lt;= 1.0)\n        )\n\n    def _argcheck(self, mu, kappa, nu):\n        try:\n            return self._validate_params(mu, kappa, nu)\n        except ValueError:\n            return False\n\n    def _clear_normalization_cache(self):\n        super()._clear_normalization_cache()\n        self._vmft_table_cache = {}\n        self._vmft_sampler_cache = {}\n\n    def _pdf(self, x, mu, kappa, nu):\n        x_arr = np.asarray(x, dtype=float)\n        mu_val = _vmft_ensure_scalar(mu, \"mu\")\n        kappa_val = float(np.clip(_vmft_ensure_scalar(kappa, \"kappa\"), 0.0, _VMFT_KAPPA_UPPER))\n        nu_val = _vmft_ensure_scalar(nu, \"nu\")\n\n        if not np.isfinite(mu_val) or not np.isfinite(kappa_val) or not np.isfinite(nu_val):\n            return np.full_like(x_arr, np.nan, dtype=float)\n\n        if kappa_val &lt;= _VMFT_KAPPA_TOL:\n            self._c = 1.0 / (2.0 * np.pi)\n            return np.full_like(x_arr, self._c, dtype=float)\n\n        table = self._get_vmft_table(kappa_val, nu_val)\n        phi = ((x_arr - mu_val + np.pi) % (2.0 * np.pi)) - np.pi\n        log_kernel = kappa_val * np.cos(phi + nu_val * np.sin(phi))\n        log_pdf = log_kernel + table[\"log_normalizer\"]\n        pdf_vals = np.exp(log_pdf)\n        self._c = table[\"normalizer\"]  # retain attribute for existing code paths\n        return pdf_vals\n\n    def pdf(self, x, mu, kappa, nu, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Flat-topped von Mises distribution.\n\n        $$\n        f(\\theta) = c \\exp(\\kappa \\cos(\\theta - \\mu + \\nu \\sin(\\theta - \\mu)))\n        $$\n\n        , where `c` is the normalizing constant:\n\n        $$\n        c = \\frac{1}{\\int_{-\\pi}^{\\pi} \\exp(\\kappa \\cos(\\theta - \\mu + \\nu \\sin(\\theta - \\mu))) d\\theta}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n        mu : float\n            Location parameter, $0 \\leq \\mu \\leq 2\\pi$. This is the mean direction when $\\nu = 0$.\n        kappa : float\n            Concentration parameter, $\\kappa \\geq 0$. Higher values indicate a sharper peak around $\\mu$.\n        nu : float\n            Shape parameter, $-1 \\leq \\nu \\leq 1$. Controls the flattening or sharpening of the peak:\n            - $\\nu &gt; 0$: sharper peaks.\n            - $\\nu &lt; 0$: flatter peaks.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Values of the probability density function at the specified points.\n\n\n        Notes\n        -----\n        - The normalization constant $c$ is computed numerically, as the integral generally\n        does not have a closed-form solution.\n        - Special cases:\n            - When $\\nu = 0$, the distribution reduces to the standard von Mises distribution.\n            - When $\\kappa = 0$, the distribution becomes uniform on $[0, 2\\pi)$.\n        \"\"\"\n        mu_val = _vmft_ensure_scalar(mu, \"mu\")\n        kappa_val = float(np.clip(_vmft_ensure_scalar(kappa, \"kappa\"), 0.0, _VMFT_KAPPA_UPPER))\n        nu_val = _vmft_ensure_scalar(nu, \"nu\")\n        return super().pdf(x, mu_val, kappa_val, nu_val, *args, **kwargs)\n\n    def _cdf(self, x, mu, kappa, nu):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n\n        if flat.size == 0:\n            return arr.astype(float)\n\n        mu_val = _vmft_ensure_scalar(mu, \"mu\")\n        kappa_val = float(np.clip(_vmft_ensure_scalar(kappa, \"kappa\"), 0.0, _VMFT_KAPPA_UPPER))\n        nu_val = _vmft_ensure_scalar(nu, \"nu\")\n\n        if not np.isfinite(mu_val) or not np.isfinite(kappa_val) or not np.isfinite(nu_val):\n            return np.full_like(arr, np.nan, dtype=float)\n\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt;= _VMFT_KAPPA_TOL:\n            cdf_flat = flat / two_pi\n        else:\n            table = self._get_vmft_table(kappa_val, nu_val)\n            phi = ((flat - mu_val + np.pi) % two_pi) - np.pi\n            phi_start = ((-mu_val + np.pi) % two_pi) - np.pi\n            H = table[\"cdf_interp\"](phi)\n            H_start = float(table[\"cdf_interp\"](phi_start))\n            cdf_flat = np.where(H &lt; H_start, H - H_start + 1.0, H - H_start)\n            cdf_flat = np.clip(cdf_flat, 0.0, 1.0)\n\n        if arr.ndim == 0:\n            value = float(cdf_flat[0])\n            if np.isclose(float(wrapped), two_pi, rtol=0.0, atol=1e-12):\n                return 1.0\n            return value\n\n        result = cdf_flat.reshape(arr.shape)\n        mask_upper = np.isclose(arr, two_pi, rtol=0.0, atol=1e-12)\n        if np.any(mask_upper):\n            result = result.copy()\n            result[mask_upper] = 1.0\n        return result\n\n    def _ppf(self, q, mu, kappa, nu):\n        mu_val = _vmft_ensure_scalar(mu, \"mu\")\n        kappa_val = _vmft_ensure_scalar(kappa, \"kappa\")\n        nu_val = _vmft_ensure_scalar(nu, \"nu\")\n\n        q_arr = np.asarray(q, dtype=float)\n        flat = q_arr.reshape(-1)\n        if flat.size == 0:\n            return q_arr.astype(float)\n\n        if not np.isfinite(mu_val) or not np.isfinite(kappa_val) or not np.isfinite(nu_val):\n            return np.full_like(q_arr, np.nan, dtype=float)\n\n        two_pi = 2.0 * np.pi\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if not np.any(valid):\n            shaped = result.reshape(q_arr.shape)\n            return float(shaped) if q_arr.ndim == 0 else shaped\n\n        q_valid = flat[valid]\n        close_zero = np.isclose(q_valid, 0.0, rtol=0.0, atol=1e-12)\n        close_one = np.isclose(q_valid, 1.0, rtol=0.0, atol=1e-12)\n\n        if kappa_val &lt;= _VMFT_KAPPA_TOL:\n            theta = (two_pi * q_valid) % two_pi\n            if np.any(close_zero):\n                theta[close_zero] = 0.0\n            if np.any(close_one):\n                theta[close_one] = two_pi\n            result[valid] = theta\n        else:\n            table = self._get_vmft_table(kappa_val, nu_val)\n            phi_grid = table[\"phi\"]\n            cdf_grid = table[\"cdf\"]\n            cdf_interp = table[\"cdf_interp\"]\n            inv_interp = table[\"inv_cdf_interp\"]\n\n            phi_start = ((-mu_val + np.pi) % two_pi) - np.pi\n            H_start = float(cdf_interp(phi_start))\n\n            # Prepare bracket indices for each quantile\n            targets = (H_start + q_valid) % 1.0\n            phi_guess = (\n                inv_interp(targets)\n                if inv_interp is not None\n                else np.interp(targets, cdf_grid, phi_grid, left=phi_grid[0], right=phi_grid[-1])\n            )\n\n            theta = np.empty_like(q_valid)\n            for idx, (q_val, target, phi0) in enumerate(zip(q_valid, targets, phi_guess)):\n                if close_zero[idx]:\n                    theta[idx] = 0.0\n                    continue\n                if close_one[idx]:\n                    theta[idx] = two_pi\n                    continue\n\n                i_hi = int(np.clip(np.searchsorted(cdf_grid, target, side=\"right\"), 1, len(phi_grid) - 1))\n                phi_lo = float(phi_grid[i_hi - 1])\n                phi_hi = float(phi_grid[i_hi])\n                phi = float(np.clip(phi0, phi_lo, phi_hi))\n\n                for _ in range(_VMFT_NEWTON_MAXITER):\n                    H_phi = float(cdf_interp(phi))\n                    residual = H_phi - target\n                    derivative = np.exp(\n                        kappa_val * np.cos(phi + nu_val * np.sin(phi)) + table[\"log_normalizer\"]\n                    )\n                    derivative = max(derivative, np.finfo(float).tiny)\n\n                    if abs(residual) &lt;= _VMFT_NEWTON_TOL and (phi_hi - phi_lo) &lt;= _VMFT_NEWTON_WIDTH_TOL:\n                        break\n\n                    if residual &gt; 0.0:\n                        phi_hi = min(phi_hi, phi)\n                    else:\n                        phi_lo = max(phi_lo, phi)\n\n                    step = residual / derivative\n                    phi_candidate = phi - step\n                    if not np.isfinite(phi_candidate) or phi_candidate &lt;= phi_lo or phi_candidate &gt;= phi_hi:\n                        phi_candidate = 0.5 * (phi_lo + phi_hi)\n                    phi = float(np.clip(phi_candidate, phi_lo, phi_hi))\n\n                theta[idx] = (mu_val + phi) % two_pi\n\n            result[valid] = theta\n\n        shaped = result.reshape(q_arr.shape)\n        if q_arr.ndim == 0:\n            return float(shaped)\n        return shaped\n\n    def ppf(self, q, mu, kappa, nu, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (quantile) of the flat-topped von Mises distribution.\n\n        Quantiles are computed by reusing the cached cumulative table described in\n        `cdf`. Starting from the monotone inverse of the tabulated primitive\n        $H_{\\kappa,\\nu}$, the implementation applies up to\n        :data:`_VMFT_NEWTON_MAXITER` safeguarded Newton steps with derivative\n        $f(\\theta) = \\exp[\\kappa \\cos(\\phi + \\nu \\sin \\phi)]/Z$ to achieve\n        machine-precision agreement (dual stopping on residual and bracket width).\n        Boundary quantiles default to the support endpoints $0$ and $2\\pi$.\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles to evaluate (0 &lt;= q &lt;= 1).\n        mu : float\n            Location parameter, $0 \\le \\mu \\le 2\\pi$.\n        kappa : float\n            Concentration parameter, $\\kappa \\ge 0$.\n        nu : float\n            Shape parameter, $-1 \\le \\nu \\le 1$.\n\n        Returns\n        -------\n        ppf_values : array_like\n            Angles corresponding to the probabilities in `q`.\n        \"\"\"\n        mu_val = _vmft_ensure_scalar(mu, \"mu\")\n        kappa_val = _vmft_ensure_scalar(kappa, \"kappa\")\n        nu_val = _vmft_ensure_scalar(nu, \"nu\")\n        return super().ppf(q, mu_val, kappa_val, nu_val, *args, **kwargs)\n\n    def _rvs(self, mu, kappa, nu, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        mu_val = _vmft_ensure_scalar(mu, \"mu\") % (2.0 * np.pi)\n        kappa_val = float(np.clip(_vmft_ensure_scalar(kappa, \"kappa\"), 0.0, _VMFT_KAPPA_UPPER))\n        nu_val = _vmft_ensure_scalar(nu, \"nu\")\n\n        if not np.isfinite(mu_val) or not np.isfinite(kappa_val) or not np.isfinite(nu_val):\n            raise ValueError(\"`mu`, `kappa`, and `nu` must be finite scalars.\")\n\n        if size is None:\n            shape = ()\n            total = 1\n        else:\n            if np.isscalar(size):\n                shape = (int(size),)\n            else:\n                shape = tuple(int(dim) for dim in np.atleast_1d(size))\n            total = int(np.prod(shape, dtype=int))\n            if total &lt; 0:\n                raise ValueError(\"`size` must describe a non-negative number of samples.\")\n        two_pi = 2.0 * np.pi\n\n        if total == 0:\n            empty = np.empty(shape, dtype=float)\n            return float(empty) if empty.ndim == 0 else empty\n\n        if kappa_val &lt;= _VMFT_KAPPA_TOL:\n            samples = rng.uniform(0.0, two_pi, size=shape)\n            if samples.ndim == 0:\n                return float(samples)\n            return samples\n\n        table = self._get_vmft_table(kappa_val, nu_val)\n        sampler_params = self._get_vmft_sampler_params(kappa_val, nu_val)\n        kappa_env = sampler_params[\"kappa_env\"]\n        log_env_norm = sampler_params[\"log_env_norm\"]\n        log_multiplier = sampler_params[\"log_multiplier\"]\n\n        samples = np.empty(total, dtype=float)\n        filled = 0\n        batch_base = max(8, min(4 * total, 4096))\n\n        while filled &lt; total:\n            batch = min(batch_base, total - filled) if filled &gt; 0 else batch_base\n            proposals = rng.vonmises(mu_val, kappa_env, size=batch)\n            phi = ((proposals - mu_val + np.pi) % two_pi) - np.pi\n\n            log_target = kappa_val * np.cos(phi + nu_val * np.sin(phi)) + table[\"log_normalizer\"]\n            log_env = kappa_env * np.cos(phi) - log_env_norm\n            log_accept = log_target - log_env - log_multiplier\n\n            accept_mask = np.log(rng.random(size=batch)) &lt;= log_accept\n            if not np.any(accept_mask):\n                continue\n\n            accepted = proposals[accept_mask]\n            take = min(accepted.size, total - filled)\n            samples[filled : filled + take] = accepted[:take]\n            filled += take\n\n        samples = np.mod(samples, two_pi)\n        samples = samples.reshape(shape)\n        if samples.ndim == 0:\n            return float(samples)\n        return samples\n\n    def rvs(self, mu=None, kappa=None, nu=None, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the flat-topped von Mises distribution.\n\n        Sampling uses an acceptance\u2013rejection scheme with a curvature-matched\n        von Mises envelope. Writing $\\phi = \\theta - \\mu$ and matching the\n        curvature at the mode yields a proposal concentration\n        $\\kappa_e = \\kappa(1+\\nu)^2$ (clipped to a small positive value). The\n        envelope constant $M \\ge \\sup_\\phi f(\\phi)/g(\\phi)$ is precomputed on\n        the same spectral grid used for `cdf`, so once calibrated the\n        sampler draws each variate with a single von Mises proposal followed by\n        a scalar acceptance test.\n\n        Parameters\n        ----------\n        mu : float\n            Location parameter, $0 \\le \\mu \\le 2\\pi$.\n        kappa : float\n            Concentration parameter, $\\kappa \\ge 0$.\n        nu : float\n            Shape parameter, $-1 \\le \\nu \\le 1$.\n        size : int or tuple of ints, optional\n            Output shape.\n        random_state : {None, int, np.random.Generator}, optional\n            Random number generator specification.\n\n        Returns\n        -------\n        rvs : array_like\n            Random variates on $[0, 2\\pi)$.\n        \"\"\"\n        return super().rvs(mu, kappa, nu, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        optimizer=\"L-BFGS-B\",\n        options=None,\n        nu_grid=None,\n        kappa_bounds=(1e-6, _VMFT_KAPPA_UPPER),\n        nu_bounds=(-0.99, 0.99),\n        return_info=False,\n        **minimize_kwargs,\n    ):\n        r\"\"\"\n        Estimate $(\\mu, \\kappa, \\nu)$ from circular data.\n\n        The default ``method='mle'`` maximises the weighted log-likelihood\n\n        $$\n        \\ell(\\mu, \\kappa, \\nu) = \\sum_i w_i\n        \\left[\n            \\kappa \\cos(\\phi_i + \\nu \\sin \\phi_i) - \\log Z(\\kappa, \\nu)\n        \\right],\\quad\n        \\phi_i = (\\theta_i - \\mu) \\bmod 2\\pi,\n        $$\n\n        where $Z$ is the normalising constant reused from the cached spectral\n        table. The routine initialises $(\\mu, \\kappa)$ from the first trigonometric\n        moment and profiles a small grid for $\\nu$ before bounded optimisation\n        (default L-BFGS-B) with $\\kappa \\in$ ``kappa_bounds`` and\n        $\\nu \\in$ ``nu_bounds``.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample of angles.\n        weights : array_like, optional\n            Non-negative weights broadcastable to ``data``.\n        method : {'mle', 'moments'}, default 'mle'\n            Estimation method. ``'moments'`` returns the circular mean,\n            ``circ_kappa``, and $\\nu=0$.\n        optimizer : str, optional\n            SciPy optimiser to use when ``method='mle'``.\n        options : dict, optional\n            Optimiser options forwarded to :func:`scipy.optimize.minimize`.\n        nu_grid : array_like, optional\n            Candidate $\\nu$ values for initial profiling. Defaults to a small grid\n            spanning ``nu_bounds``.\n        kappa_bounds : tuple, optional\n            Lower/upper bounds for $\\kappa$ during optimisation.\n        nu_bounds : tuple, optional\n            Lower/upper bounds for $\\nu$ during optimisation.\n        return_info : bool, optional\n            If True, also return a dictionary with optimisation diagnostics.\n        **minimize_kwargs :\n            Additional keyword arguments passed to :func:`scipy.optimize.minimize`.\n\n        Returns\n        -------\n        params : tuple\n            Estimated parameters ``(mu, kappa, nu)``.\n        info : dict, optional\n            Returned when ``return_info=True`` with fields such as ``loglik``,\n            ``n_effective`` and ``converged``.\n        \"\"\"\n\n        minimize_kwargs = self._sanitize_fit_kwargs(minimize_kwargs)\n        minimize_kwargs.pop(\"floc\", None)\n        minimize_kwargs.pop(\"fscale\", None)\n\n        data_arr = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if data_arr.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(data_arr, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, data_arr.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0.0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = float(w_sum**2 / np.sum(w**2))\n\n        mu_mom, r1 = circ_mean_and_r(alpha=data_arr, w=w)\n        if not np.isfinite(mu_mom):\n            mu_mom = 0.0\n        mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n        r1 = float(np.clip(r1, 1e-12, 1.0 - 1e-12))\n\n        n_adjust = int(max(1, round(w_sum)))\n        kappa_mom = float(np.clip(circ_kappa(r=r1, n=n_adjust), kappa_bounds[0], kappa_bounds[1]))\n\n        if nu_grid is None:\n            lower_nu = float(max(nu_bounds[0], -0.9))\n            upper_nu = float(min(nu_bounds[1], 0.9))\n            nu_grid = np.linspace(lower_nu, upper_nu, 7)\n        else:\n            nu_grid = np.asarray(nu_grid, dtype=float)\n\n        def nll(params):\n            mu_param, kappa_param, nu_param = params\n\n            if not (0.0 &lt;= mu_param &lt;= 2.0 * np.pi):\n                return np.inf\n            if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n                return np.inf\n            if not (nu_bounds[0] &lt;= nu_param &lt;= nu_bounds[1]):\n                return np.inf\n\n            mu_wrapped = float(np.mod(mu_param, 2.0 * np.pi))\n            two_pi = 2.0 * np.pi\n            phi = ((data_arr - mu_wrapped + np.pi) % two_pi) - np.pi\n\n            if kappa_param &lt;= _VMFT_KAPPA_TOL:\n                log_pdf = -np.log(two_pi)\n                return float(-np.sum(w * log_pdf))\n\n            table = self._get_vmft_table(float(kappa_param), float(nu_param))\n\n            log_kernel = kappa_param * np.cos(phi + nu_param * np.sin(phi))\n            log_pdf = log_kernel + table[\"log_normalizer\"]\n            if not np.all(np.isfinite(log_pdf)):\n                return np.inf\n            return float(-np.sum(w * log_pdf))\n\n        method_key = str(method).lower()\n\n        if method_key == \"moments\":\n            estimates = (mu_mom, kappa_mom, 0.0)\n            if return_info:\n                info = {\n                    \"method\": \"moments\",\n                    \"converged\": True,\n                    \"loglik\": float(-nll(estimates)),\n                    \"n_effective\": n_eff,\n                }\n                return estimates, info\n            return estimates\n\n        if method_key != \"mle\":\n            raise ValueError(\"`method` must be one of {'mle', 'moments'}.\")\n\n        best_nu = 0.0\n        best_score = nll((mu_mom, kappa_mom, best_nu))\n        for candidate in np.unique(np.concatenate(([0.0], nu_grid))):\n            score = nll((mu_mom, kappa_mom, float(candidate)))\n            if score &lt; best_score:\n                best_score = score\n                best_nu = float(candidate)\n\n        init = np.array([mu_mom, kappa_mom, best_nu], dtype=float)\n        bounds = [\n            (0.0, 2.0 * np.pi),\n            (kappa_bounds[0], kappa_bounds[1]),\n            (nu_bounds[0], nu_bounds[1]),\n        ]\n\n        options = {} if options is None else dict(options)\n\n        optimizer_used = optimizer\n\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            bounds=bounds,\n            options=options,\n            **minimize_kwargs,\n        )\n\n        if not result.success and optimizer != \"Powell\":\n            fallback = minimize(\n                nll,\n                init,\n                method=\"Powell\",\n                bounds=bounds,\n                options={},\n                **minimize_kwargs,\n            )\n            if fallback.success:\n                result = fallback\n                optimizer_used = \"Powell\"\n\n        if not result.success:\n            raise RuntimeError(f\"Maximum likelihood fit failed: {result.message}\")\n\n        mu_hat = self._wrap_direction(float(result.x[0]))\n        kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n        nu_hat = float(np.clip(result.x[2], nu_bounds[0], nu_bounds[1]))\n\n        estimates = (mu_hat, kappa_hat, nu_hat)\n        if not return_info:\n            return estimates\n\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-result.fun),\n            \"n_effective\": n_eff,\n            \"converged\": bool(result.success),\n            \"optimizer\": optimizer_used,\n            \"nit\": getattr(result, \"nit\", np.nan),\n            \"nfev\": getattr(result, \"nfev\", np.nan),\n            \"message\": result.message,\n        }\n        return estimates, info\n\n    def cdf(self, x, mu, kappa, nu, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the flat-topped von Mises distribution.\n\n        Let $\\phi = (\\theta - \\mu) \\bmod 2\\pi$ re-centred onto $[-\\pi, \\pi]$ and\n        $g_{\\kappa,\\nu}(\\phi) = \\exp\\!\\bigl[\\kappa \\cos(\\phi + \\nu \\sin \\phi)\\bigr]$.\n        The normalised primitive\n        $$\n        H_{\\kappa,\\nu}(\\phi) = \\frac{1}{Z} \\int_{-\\pi}^{\\phi} g_{\\kappa,\\nu}(t)\\,dt,\n        \\qquad Z = \\int_{-\\pi}^{\\pi} g_{\\kappa,\\nu}(t)\\,dt,\n        $$\n        is approximated with spectral accuracy by a trapezoidal rule on an\n        equispaced grid (size selected from $O(\\sqrt{\\kappa})$). The CDF on\n        $[0, 2\\pi)$ then follows from $F(\\theta) = H_{\\kappa,\\nu}(\\phi) -\n        H_{\\kappa,\\nu}(\\phi_0)$ with $\\phi_0 = ((-\\mu) \\bmod 2\\pi) - \\pi$. The\n        precomputed cumulative grid is cached per $(\\kappa, \\nu)$, so repeated\n        evaluations are $O(1)$ once the table is built.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n        mu : float\n            Location parameter, $0 \\le \\mu \\le 2\\pi$.\n        kappa : float\n            Concentration parameter, $\\kappa \\ge 0$ (capped internally at\n            :data:`_VMFT_KAPPA_UPPER` for numerical stability).\n        nu : float\n            Shape parameter, $-1 \\le \\nu \\le 1$.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative probabilities corresponding to `x`.\n        \"\"\"\n        mu_val = _vmft_ensure_scalar(mu, \"mu\")\n        kappa_val = _vmft_ensure_scalar(kappa, \"kappa\")\n        nu_val = _vmft_ensure_scalar(nu, \"nu\")\n        return super().cdf(x, mu_val, kappa_val, nu_val, *args, **kwargs)\n\n    def _get_vmft_table(self, kappa, nu, grid_size=None):\n        kappa_val = float(kappa)\n        nu_val = float(nu)\n        if grid_size is None:\n            grid_size = _vmft_grid_size(kappa_val, nu_val)\n        grid_int = int(grid_size)\n        key = (kappa_val, nu_val, grid_int)\n        table = self._vmft_table_cache.get(key)\n        if table is None:\n            table = _vmft_build_table(kappa_val, nu_val, grid_int)\n            self._vmft_table_cache[key] = table\n        return table\n\n    def _get_vmft_sampler_params(self, kappa, nu):\n        key = (float(kappa), float(nu))\n        params = self._vmft_sampler_cache.get(key)\n        if params is not None:\n            return params\n\n        table = self._get_vmft_table(kappa, nu)\n        kappa_env = float(np.clip(kappa * (1.0 + nu) ** 2, _VMFT_ENV_MIN_KAPPA, _VMFT_KAPPA_UPPER))\n\n        log_env_norm = (\n            np.log(2.0 * np.pi)\n            + np.log(i0e(kappa_env))\n            + kappa_env\n        )\n        log_env_pdf = kappa_env * np.cos(table[\"phi\"]) - log_env_norm\n        log_ratio = np.log(table[\"pdf\"]) - log_env_pdf\n        log_multiplier = float(np.max(log_ratio))\n        multiplier = float(np.exp(log_multiplier) * (1.0 + 5e-12))\n\n        params = {\n            \"kappa_env\": kappa_env,\n            \"log_env_norm\": float(log_env_norm),\n            \"log_multiplier\": float(np.log(multiplier)),\n            \"multiplier\": multiplier,\n        }\n        self._vmft_sampler_cache[key] = params\n        return params\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_flattopped_gen.pdf","title":"<code>pdf(x, mu, kappa, nu, *args, **kwargs)</code>","text":"<p>Probability density function of the Flat-topped von Mises distribution.</p> \\[ f(\\theta) = c \\exp(\\kappa \\cos(\\theta - \\mu + \\nu \\sin(\\theta - \\mu))) \\] <p>, where <code>c</code> is the normalizing constant:</p> \\[ c = \\frac{1}{\\int_{-\\pi}^{\\pi} \\exp(\\kappa \\cos(\\theta - \\mu + \\nu \\sin(\\theta - \\mu))) d\\theta} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the PDF, defined on the interval \\([0, 2\\pi)\\).</p> required <code>mu</code> <code>float</code> <p>Location parameter, \\(0 \\leq \\mu \\leq 2\\pi\\). This is the mean direction when \\(\\nu = 0\\).</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\geq 0\\). Higher values indicate a sharper peak around \\(\\mu\\).</p> required <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\leq \\nu \\leq 1\\). Controls the flattening or sharpening of the peak: - \\(\\nu &gt; 0\\): sharper peaks. - \\(\\nu &lt; 0\\): flatter peaks.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Values of the probability density function at the specified points.</p> Notes <ul> <li>The normalization constant \\(c\\) is computed numerically, as the integral generally does not have a closed-form solution.</li> <li>Special cases:<ul> <li>When \\(\\nu = 0\\), the distribution reduces to the standard von Mises distribution.</li> <li>When \\(\\kappa = 0\\), the distribution becomes uniform on \\([0, 2\\pi)\\).</li> </ul> </li> </ul> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, kappa, nu, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Flat-topped von Mises distribution.\n\n    $$\n    f(\\theta) = c \\exp(\\kappa \\cos(\\theta - \\mu + \\nu \\sin(\\theta - \\mu)))\n    $$\n\n    , where `c` is the normalizing constant:\n\n    $$\n    c = \\frac{1}{\\int_{-\\pi}^{\\pi} \\exp(\\kappa \\cos(\\theta - \\mu + \\nu \\sin(\\theta - \\mu))) d\\theta}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n    mu : float\n        Location parameter, $0 \\leq \\mu \\leq 2\\pi$. This is the mean direction when $\\nu = 0$.\n    kappa : float\n        Concentration parameter, $\\kappa \\geq 0$. Higher values indicate a sharper peak around $\\mu$.\n    nu : float\n        Shape parameter, $-1 \\leq \\nu \\leq 1$. Controls the flattening or sharpening of the peak:\n        - $\\nu &gt; 0$: sharper peaks.\n        - $\\nu &lt; 0$: flatter peaks.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Values of the probability density function at the specified points.\n\n\n    Notes\n    -----\n    - The normalization constant $c$ is computed numerically, as the integral generally\n    does not have a closed-form solution.\n    - Special cases:\n        - When $\\nu = 0$, the distribution reduces to the standard von Mises distribution.\n        - When $\\kappa = 0$, the distribution becomes uniform on $[0, 2\\pi)$.\n    \"\"\"\n    mu_val = _vmft_ensure_scalar(mu, \"mu\")\n    kappa_val = float(np.clip(_vmft_ensure_scalar(kappa, \"kappa\"), 0.0, _VMFT_KAPPA_UPPER))\n    nu_val = _vmft_ensure_scalar(nu, \"nu\")\n    return super().pdf(x, mu_val, kappa_val, nu_val, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_flattopped_gen.ppf","title":"<code>ppf(q, mu, kappa, nu, *args, **kwargs)</code>","text":"<p>Percent-point function (quantile) of the flat-topped von Mises distribution.</p> <p>Quantiles are computed by reusing the cached cumulative table described in <code>cdf</code>. Starting from the monotone inverse of the tabulated primitive \\(H_{\\kappa,\\nu}\\), the implementation applies up to :data:<code>_VMFT_NEWTON_MAXITER</code> safeguarded Newton steps with derivative \\(f(\\theta) = \\exp[\\kappa \\cos(\\phi + \\nu \\sin \\phi)]/Z\\) to achieve machine-precision agreement (dual stopping on residual and bracket width). Boundary quantiles default to the support endpoints \\(0\\) and \\(2\\pi\\).</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles to evaluate (0 &lt;= q &lt;= 1).</p> required <code>mu</code> <code>float</code> <p>Location parameter, \\(0 \\le \\mu \\le 2\\pi\\).</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\ge 0\\).</p> required <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\le \\nu \\le 1\\).</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Angles corresponding to the probabilities in <code>q</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, mu, kappa, nu, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (quantile) of the flat-topped von Mises distribution.\n\n    Quantiles are computed by reusing the cached cumulative table described in\n    `cdf`. Starting from the monotone inverse of the tabulated primitive\n    $H_{\\kappa,\\nu}$, the implementation applies up to\n    :data:`_VMFT_NEWTON_MAXITER` safeguarded Newton steps with derivative\n    $f(\\theta) = \\exp[\\kappa \\cos(\\phi + \\nu \\sin \\phi)]/Z$ to achieve\n    machine-precision agreement (dual stopping on residual and bracket width).\n    Boundary quantiles default to the support endpoints $0$ and $2\\pi$.\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles to evaluate (0 &lt;= q &lt;= 1).\n    mu : float\n        Location parameter, $0 \\le \\mu \\le 2\\pi$.\n    kappa : float\n        Concentration parameter, $\\kappa \\ge 0$.\n    nu : float\n        Shape parameter, $-1 \\le \\nu \\le 1$.\n\n    Returns\n    -------\n    ppf_values : array_like\n        Angles corresponding to the probabilities in `q`.\n    \"\"\"\n    mu_val = _vmft_ensure_scalar(mu, \"mu\")\n    kappa_val = _vmft_ensure_scalar(kappa, \"kappa\")\n    nu_val = _vmft_ensure_scalar(nu, \"nu\")\n    return super().ppf(q, mu_val, kappa_val, nu_val, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_flattopped_gen.rvs","title":"<code>rvs(mu=None, kappa=None, nu=None, size=None, random_state=None)</code>","text":"<p>Draw random variates from the flat-topped von Mises distribution.</p> <p>Sampling uses an acceptance\u2013rejection scheme with a curvature-matched von Mises envelope. Writing \\(\\phi = \\theta - \\mu\\) and matching the curvature at the mode yields a proposal concentration \\(\\kappa_e = \\kappa(1+\\nu)^2\\) (clipped to a small positive value). The envelope constant \\(M \\ge \\sup_\\phi f(\\phi)/g(\\phi)\\) is precomputed on the same spectral grid used for <code>cdf</code>, so once calibrated the sampler draws each variate with a single von Mises proposal followed by a scalar acceptance test.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Location parameter, \\(0 \\le \\mu \\le 2\\pi\\).</p> <code>None</code> <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\ge 0\\).</p> <code>None</code> <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\le \\nu \\le 1\\).</p> <code>None</code> <code>size</code> <code>int or tuple of ints</code> <p>Output shape.</p> <code>None</code> <code>random_state</code> <code>(None, int, Generator)</code> <p>Random number generator specification.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rvs</code> <code>array_like</code> <p>Random variates on \\([0, 2\\pi)\\).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, mu=None, kappa=None, nu=None, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the flat-topped von Mises distribution.\n\n    Sampling uses an acceptance\u2013rejection scheme with a curvature-matched\n    von Mises envelope. Writing $\\phi = \\theta - \\mu$ and matching the\n    curvature at the mode yields a proposal concentration\n    $\\kappa_e = \\kappa(1+\\nu)^2$ (clipped to a small positive value). The\n    envelope constant $M \\ge \\sup_\\phi f(\\phi)/g(\\phi)$ is precomputed on\n    the same spectral grid used for `cdf`, so once calibrated the\n    sampler draws each variate with a single von Mises proposal followed by\n    a scalar acceptance test.\n\n    Parameters\n    ----------\n    mu : float\n        Location parameter, $0 \\le \\mu \\le 2\\pi$.\n    kappa : float\n        Concentration parameter, $\\kappa \\ge 0$.\n    nu : float\n        Shape parameter, $-1 \\le \\nu \\le 1$.\n    size : int or tuple of ints, optional\n        Output shape.\n    random_state : {None, int, np.random.Generator}, optional\n        Random number generator specification.\n\n    Returns\n    -------\n    rvs : array_like\n        Random variates on $[0, 2\\pi)$.\n    \"\"\"\n    return super().rvs(mu, kappa, nu, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_flattopped_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', optimizer='L-BFGS-B', options=None, nu_grid=None, kappa_bounds=(1e-06, _VMFT_KAPPA_UPPER), nu_bounds=(-0.99, 0.99), return_info=False, **minimize_kwargs)</code>","text":"<p>Estimate \\((\\mu, \\kappa, \\nu)\\) from circular data.</p> <p>The default <code>method='mle'</code> maximises the weighted log-likelihood</p> \\[ \\ell(\\mu, \\kappa, \\nu) = \\sum_i w_i \\left[     \\kappa \\cos(\\phi_i + \\nu \\sin \\phi_i) - \\log Z(\\kappa, \\nu) \\right],\\quad \\phi_i = (\\theta_i - \\mu) \\bmod 2\\pi, \\] <p>where \\(Z\\) is the normalising constant reused from the cached spectral table. The routine initialises \\((\\mu, \\kappa)\\) from the first trigonometric moment and profiles a small grid for \\(\\nu\\) before bounded optimisation (default L-BFGS-B) with \\(\\kappa \\in\\) <code>kappa_bounds</code> and \\(\\nu \\in\\) <code>nu_bounds</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample of angles.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights broadcastable to <code>data</code>.</p> <code>None</code> <code>method</code> <code>(mle, moments)</code> <p>Estimation method. <code>'moments'</code> returns the circular mean, <code>circ_kappa</code>, and \\(\\nu=0\\).</p> <code>'mle'</code> <code>optimizer</code> <code>str</code> <p>SciPy optimiser to use when <code>method='mle'</code>.</p> <code>'L-BFGS-B'</code> <code>options</code> <code>dict</code> <p>Optimiser options forwarded to :func:<code>scipy.optimize.minimize</code>.</p> <code>None</code> <code>nu_grid</code> <code>array_like</code> <p>Candidate \\(\\nu\\) values for initial profiling. Defaults to a small grid spanning <code>nu_bounds</code>.</p> <code>None</code> <code>kappa_bounds</code> <code>tuple</code> <p>Lower/upper bounds for \\(\\kappa\\) during optimisation.</p> <code>(1e-06, _VMFT_KAPPA_UPPER)</code> <code>nu_bounds</code> <code>tuple</code> <p>Lower/upper bounds for \\(\\nu\\) during optimisation.</p> <code>(-0.99, 0.99)</code> <code>return_info</code> <code>bool</code> <p>If True, also return a dictionary with optimisation diagnostics.</p> <code>False</code> <code>**minimize_kwargs</code> <p>Additional keyword arguments passed to :func:<code>scipy.optimize.minimize</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>params</code> <code>tuple</code> <p>Estimated parameters <code>(mu, kappa, nu)</code>.</p> <code>info</code> <code>(dict, optional)</code> <p>Returned when <code>return_info=True</code> with fields such as <code>loglik</code>, <code>n_effective</code> and <code>converged</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    optimizer=\"L-BFGS-B\",\n    options=None,\n    nu_grid=None,\n    kappa_bounds=(1e-6, _VMFT_KAPPA_UPPER),\n    nu_bounds=(-0.99, 0.99),\n    return_info=False,\n    **minimize_kwargs,\n):\n    r\"\"\"\n    Estimate $(\\mu, \\kappa, \\nu)$ from circular data.\n\n    The default ``method='mle'`` maximises the weighted log-likelihood\n\n    $$\n    \\ell(\\mu, \\kappa, \\nu) = \\sum_i w_i\n    \\left[\n        \\kappa \\cos(\\phi_i + \\nu \\sin \\phi_i) - \\log Z(\\kappa, \\nu)\n    \\right],\\quad\n    \\phi_i = (\\theta_i - \\mu) \\bmod 2\\pi,\n    $$\n\n    where $Z$ is the normalising constant reused from the cached spectral\n    table. The routine initialises $(\\mu, \\kappa)$ from the first trigonometric\n    moment and profiles a small grid for $\\nu$ before bounded optimisation\n    (default L-BFGS-B) with $\\kappa \\in$ ``kappa_bounds`` and\n    $\\nu \\in$ ``nu_bounds``.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample of angles.\n    weights : array_like, optional\n        Non-negative weights broadcastable to ``data``.\n    method : {'mle', 'moments'}, default 'mle'\n        Estimation method. ``'moments'`` returns the circular mean,\n        ``circ_kappa``, and $\\nu=0$.\n    optimizer : str, optional\n        SciPy optimiser to use when ``method='mle'``.\n    options : dict, optional\n        Optimiser options forwarded to :func:`scipy.optimize.minimize`.\n    nu_grid : array_like, optional\n        Candidate $\\nu$ values for initial profiling. Defaults to a small grid\n        spanning ``nu_bounds``.\n    kappa_bounds : tuple, optional\n        Lower/upper bounds for $\\kappa$ during optimisation.\n    nu_bounds : tuple, optional\n        Lower/upper bounds for $\\nu$ during optimisation.\n    return_info : bool, optional\n        If True, also return a dictionary with optimisation diagnostics.\n    **minimize_kwargs :\n        Additional keyword arguments passed to :func:`scipy.optimize.minimize`.\n\n    Returns\n    -------\n    params : tuple\n        Estimated parameters ``(mu, kappa, nu)``.\n    info : dict, optional\n        Returned when ``return_info=True`` with fields such as ``loglik``,\n        ``n_effective`` and ``converged``.\n    \"\"\"\n\n    minimize_kwargs = self._sanitize_fit_kwargs(minimize_kwargs)\n    minimize_kwargs.pop(\"floc\", None)\n    minimize_kwargs.pop(\"fscale\", None)\n\n    data_arr = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if data_arr.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(data_arr, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, data_arr.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0.0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = float(w_sum**2 / np.sum(w**2))\n\n    mu_mom, r1 = circ_mean_and_r(alpha=data_arr, w=w)\n    if not np.isfinite(mu_mom):\n        mu_mom = 0.0\n    mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n    r1 = float(np.clip(r1, 1e-12, 1.0 - 1e-12))\n\n    n_adjust = int(max(1, round(w_sum)))\n    kappa_mom = float(np.clip(circ_kappa(r=r1, n=n_adjust), kappa_bounds[0], kappa_bounds[1]))\n\n    if nu_grid is None:\n        lower_nu = float(max(nu_bounds[0], -0.9))\n        upper_nu = float(min(nu_bounds[1], 0.9))\n        nu_grid = np.linspace(lower_nu, upper_nu, 7)\n    else:\n        nu_grid = np.asarray(nu_grid, dtype=float)\n\n    def nll(params):\n        mu_param, kappa_param, nu_param = params\n\n        if not (0.0 &lt;= mu_param &lt;= 2.0 * np.pi):\n            return np.inf\n        if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n            return np.inf\n        if not (nu_bounds[0] &lt;= nu_param &lt;= nu_bounds[1]):\n            return np.inf\n\n        mu_wrapped = float(np.mod(mu_param, 2.0 * np.pi))\n        two_pi = 2.0 * np.pi\n        phi = ((data_arr - mu_wrapped + np.pi) % two_pi) - np.pi\n\n        if kappa_param &lt;= _VMFT_KAPPA_TOL:\n            log_pdf = -np.log(two_pi)\n            return float(-np.sum(w * log_pdf))\n\n        table = self._get_vmft_table(float(kappa_param), float(nu_param))\n\n        log_kernel = kappa_param * np.cos(phi + nu_param * np.sin(phi))\n        log_pdf = log_kernel + table[\"log_normalizer\"]\n        if not np.all(np.isfinite(log_pdf)):\n            return np.inf\n        return float(-np.sum(w * log_pdf))\n\n    method_key = str(method).lower()\n\n    if method_key == \"moments\":\n        estimates = (mu_mom, kappa_mom, 0.0)\n        if return_info:\n            info = {\n                \"method\": \"moments\",\n                \"converged\": True,\n                \"loglik\": float(-nll(estimates)),\n                \"n_effective\": n_eff,\n            }\n            return estimates, info\n        return estimates\n\n    if method_key != \"mle\":\n        raise ValueError(\"`method` must be one of {'mle', 'moments'}.\")\n\n    best_nu = 0.0\n    best_score = nll((mu_mom, kappa_mom, best_nu))\n    for candidate in np.unique(np.concatenate(([0.0], nu_grid))):\n        score = nll((mu_mom, kappa_mom, float(candidate)))\n        if score &lt; best_score:\n            best_score = score\n            best_nu = float(candidate)\n\n    init = np.array([mu_mom, kappa_mom, best_nu], dtype=float)\n    bounds = [\n        (0.0, 2.0 * np.pi),\n        (kappa_bounds[0], kappa_bounds[1]),\n        (nu_bounds[0], nu_bounds[1]),\n    ]\n\n    options = {} if options is None else dict(options)\n\n    optimizer_used = optimizer\n\n    result = minimize(\n        nll,\n        init,\n        method=optimizer,\n        bounds=bounds,\n        options=options,\n        **minimize_kwargs,\n    )\n\n    if not result.success and optimizer != \"Powell\":\n        fallback = minimize(\n            nll,\n            init,\n            method=\"Powell\",\n            bounds=bounds,\n            options={},\n            **minimize_kwargs,\n        )\n        if fallback.success:\n            result = fallback\n            optimizer_used = \"Powell\"\n\n    if not result.success:\n        raise RuntimeError(f\"Maximum likelihood fit failed: {result.message}\")\n\n    mu_hat = self._wrap_direction(float(result.x[0]))\n    kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n    nu_hat = float(np.clip(result.x[2], nu_bounds[0], nu_bounds[1]))\n\n    estimates = (mu_hat, kappa_hat, nu_hat)\n    if not return_info:\n        return estimates\n\n    info = {\n        \"method\": \"mle\",\n        \"loglik\": float(-result.fun),\n        \"n_effective\": n_eff,\n        \"converged\": bool(result.success),\n        \"optimizer\": optimizer_used,\n        \"nit\": getattr(result, \"nit\", np.nan),\n        \"nfev\": getattr(result, \"nfev\", np.nan),\n        \"message\": result.message,\n    }\n    return estimates, info\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.vonmises_flattopped_gen.cdf","title":"<code>cdf(x, mu, kappa, nu, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the flat-topped von Mises distribution.</p> <p>Let \\(\\phi = (\\theta - \\mu) \\bmod 2\\pi\\) re-centred onto \\([-\\pi, \\pi]\\) and \\(g_{\\kappa,\\nu}(\\phi) = \\exp\\!\\bigl[\\kappa \\cos(\\phi + \\nu \\sin \\phi)\\bigr]\\). The normalised primitive $$ H_{\\kappa,\\nu}(\\phi) = \\frac{1}{Z} \\int_{-\\pi}^{\\phi} g_{\\kappa,\\nu}(t)\\,dt, \\qquad Z = \\int_{-\\pi}^{\\pi} g_{\\kappa,\\nu}(t)\\,dt, $$ is approximated with spectral accuracy by a trapezoidal rule on an equispaced grid (size selected from \\(O(\\sqrt{\\kappa})\\)). The CDF on \\([0, 2\\pi)\\) then follows from \\(F(\\theta) = H_{\\kappa,\\nu}(\\phi) - H_{\\kappa,\\nu}(\\phi_0)\\) with \\(\\phi_0 = ((-\\mu) \\bmod 2\\pi) - \\pi\\). The precomputed cumulative grid is cached per \\((\\kappa, \\nu)\\), so repeated evaluations are \\(O(1)\\) once the table is built.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <code>mu</code> <code>float</code> <p>Location parameter, \\(0 \\le \\mu \\le 2\\pi\\).</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\ge 0\\) (capped internally at :data:<code>_VMFT_KAPPA_UPPER</code> for numerical stability).</p> required <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\le \\nu \\le 1\\).</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative probabilities corresponding to <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, kappa, nu, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the flat-topped von Mises distribution.\n\n    Let $\\phi = (\\theta - \\mu) \\bmod 2\\pi$ re-centred onto $[-\\pi, \\pi]$ and\n    $g_{\\kappa,\\nu}(\\phi) = \\exp\\!\\bigl[\\kappa \\cos(\\phi + \\nu \\sin \\phi)\\bigr]$.\n    The normalised primitive\n    $$\n    H_{\\kappa,\\nu}(\\phi) = \\frac{1}{Z} \\int_{-\\pi}^{\\phi} g_{\\kappa,\\nu}(t)\\,dt,\n    \\qquad Z = \\int_{-\\pi}^{\\pi} g_{\\kappa,\\nu}(t)\\,dt,\n    $$\n    is approximated with spectral accuracy by a trapezoidal rule on an\n    equispaced grid (size selected from $O(\\sqrt{\\kappa})$). The CDF on\n    $[0, 2\\pi)$ then follows from $F(\\theta) = H_{\\kappa,\\nu}(\\phi) -\n    H_{\\kappa,\\nu}(\\phi_0)$ with $\\phi_0 = ((-\\mu) \\bmod 2\\pi) - \\pi$. The\n    precomputed cumulative grid is cached per $(\\kappa, \\nu)$, so repeated\n    evaluations are $O(1)$ once the table is built.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n    mu : float\n        Location parameter, $0 \\le \\mu \\le 2\\pi$.\n    kappa : float\n        Concentration parameter, $\\kappa \\ge 0$ (capped internally at\n        :data:`_VMFT_KAPPA_UPPER` for numerical stability).\n    nu : float\n        Shape parameter, $-1 \\le \\nu \\le 1$.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative probabilities corresponding to `x`.\n    \"\"\"\n    mu_val = _vmft_ensure_scalar(mu, \"mu\")\n    kappa_val = _vmft_ensure_scalar(kappa, \"kappa\")\n    nu_val = _vmft_ensure_scalar(nu, \"nu\")\n    return super().cdf(x, mu_val, kappa_val, nu_val, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_gen","title":"<code>jonespewsey_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Jones-Pewsey Distribution</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> Note <p>Parameters must be scalar; cached normalisation tables are built per parameter set. Implementation based on Section 4.3.9 of Pewsey et al. (2014)</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class jonespewsey_gen(CircularContinuous):\n    \"\"\"Jones-Pewsey Distribution\n\n    ![jonespewsey](../images/circ-mod-jonespewsey.png)\n\n    Methods\n    -------\n    pdf(x, mu, kappa, psi)\n        Probability density function.\n\n    cdf(x, mu, kappa, psi)\n        Cumulative distribution function.\n\n\n    Note\n    ----\n    Parameters must be scalar; cached normalisation tables are built per parameter set.\n    Implementation based on Section 4.3.9 of Pewsey et al. (2014)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._sampler_cache = {}\n        self._series_cache = {}\n\n    def _validate_params(self, mu, kappa, psi):\n        mu_arr, kappa_arr, psi_arr = np.broadcast_arrays(mu, kappa, psi)\n        return (\n            (mu_arr &gt;= 0.0)\n            &amp; (mu_arr &lt;= 2.0 * np.pi)\n            &amp; (kappa_arr &gt;= 0.0)\n            &amp; np.isfinite(kappa_arr)\n            &amp; np.isfinite(psi_arr)\n        )\n\n    def _argcheck(self, mu, kappa, psi):\n        try:\n            return self._validate_params(mu, kappa, psi)\n        except ValueError:\n            return False\n\n    def _pdf(self, x, mu, kappa, psi):\n        x = np.asarray(x, dtype=float)\n        kappa_scalar = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_scalar = _jp_ensure_scalar(psi, \"psi\")\n\n        if not np.isfinite(kappa_scalar) or not np.isfinite(psi_scalar):\n            return np.full_like(x, np.nan, dtype=float)\n\n        if abs(kappa_scalar) &lt; _JP_KAPPA_TOL:\n            return np.full_like(x, 1.0 / (2.0 * np.pi), dtype=float)\n\n        normalizer = self._get_cached_normalizer(\n            lambda: _c_jonespewsey(mu, kappa_scalar, psi_scalar),\n            mu,\n            kappa_scalar,\n            psi_scalar,\n        )\n        self._c = normalizer\n\n        if abs(psi_scalar) &lt; _JP_PSI_TOL:\n            return normalizer * np.exp(kappa_scalar * np.cos(x - mu))\n\n        return normalizer * _kernel_jonespewsey(x, mu, kappa_scalar, psi_scalar)\n\n    def pdf(self, x, mu, kappa, psi, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Jones-Pewsey distribution.\n\n        $$\n        f(\\theta) = c(\\kappa, \\psi)\n        \\Big(\\cosh(\\kappa \\psi) + \\sinh(\\kappa \\psi) \\cos(\\theta - \\mu)\\Big)^{1/\\psi},\n        $$\n\n        where ``c(\\kappa, \\psi)`` is the normalizing constant, evaluated numerically with\n        stable special-case reductions:\n\n            - ``c = 1 / (2\\pi)`` when ``\\kappa`` is effectively zero (uniform limit).\n            - ``c = 1 / (2\\pi I_0(\\kappa))`` as ``\\psi \\to 0`` (von Mises limit).\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        mu : float\n            Mean direction, 0 &lt;= mu &lt;= 2*pi.\n        kappa : float\n            Concentration parameter, kappa &gt;= 0.\n        psi : float\n            Shape parameter, -\u221e &lt;= psi &lt;= \u221e.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n        return super().pdf(x, mu, kappa, psi, *args, **kwargs)\n\n    def _cdf(self, x, mu, kappa, psi):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n        if flat.size == 0:\n            return arr.astype(float)\n\n        mu_val = _jp_ensure_scalar(mu, \"mu\")\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt; _JP_KAPPA_TOL:\n            result = np.mod(flat, two_pi) / two_pi\n            return result.reshape(arr.shape)\n\n        if abs(psi_val) &lt; _JP_PSI_TOL:\n            return vonmises.cdf(arr, mu=mu_val, kappa=kappa_val)\n\n        try:\n            n_idx, coeffs = self._jp_get_series(kappa_val, psi_val)\n        except Exception:  # pragma: no cover - defensive fallback\n            cdf_vals = self._cdf_from_pdf(arr, mu_val, kappa_val, psi_val)\n            return np.asarray(cdf_vals, dtype=float).reshape(arr.shape)\n\n        phi_start = (-mu_val) % two_pi\n        phi_end = (flat - mu_val) % two_pi\n\n        H_start = float(self._jp_series_cumulative(np.array([phi_start]), n_idx, coeffs)[0])\n        H_end = self._jp_series_cumulative(phi_end, n_idx, coeffs)\n\n        cdf = np.where(\n            phi_end &gt;= phi_start,\n            np.clip(H_end - H_start, 0.0, 1.0),\n            np.clip(1.0 - (H_start - H_end), 0.0, 1.0),\n        )\n\n        return cdf.reshape(arr.shape)\n\n    def cdf(self, x, mu, kappa, psi, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the Jones--Pewsey distribution.\n\n        $$\n        F(\\theta)=\\frac{\\theta-\\mu}{2\\pi}+\\frac{1}{\\pi}\n        \\sum_{n\\ge 1}\\frac{\\alpha_n(\\kappa,\\psi)}{n}\n        \\sin\\bigl(n(\\theta-\\mu)\\bigr),\n        $$\n        where the cosine moments $\\alpha_n$ are evaluated through the\n        associated Legendre expression reported by Jones &amp; Pewsey (2005).\n        Coefficients are cached per parameter set and the routine falls back to\n        numerical quadrature only when the series becomes unstable,\n        reproducing the von Mises limit as $\\psi \\to 0$ and the uniform limit\n        as $\\kappa \\to 0$.\n\n        Parameters\n        ----------\n        x : array_like\n            Evaluation points (radians), automatically wrapped onto [0, 2\u03c0).\n        mu, kappa, psi : float\n            Jones--Pewsey location, concentration, and shape parameters.\n\n        Returns\n        -------\n        ndarray\n            CDF values matching the shape of x.\n        \"\"\"\n        return super().cdf(x, mu, kappa, psi, *args, **kwargs)\n\n    def _ppf(self, q, mu, kappa, psi):\n        mu_val = _jp_ensure_scalar(mu, \"mu\")\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n        two_pi = 2.0 * np.pi\n\n        q_arr = np.asarray(q, dtype=float)\n        if q_arr.size == 0:\n            return q_arr.astype(float)\n\n        flat = q_arr.reshape(-1)\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if np.any(valid):\n            q_valid = flat[valid]\n\n            boundary_lo = q_valid &lt;= 0.0\n            boundary_hi = q_valid &gt;= 1.0\n            interior = (~boundary_lo) &amp; (~boundary_hi)\n            theta_vals = np.zeros_like(q_valid)\n\n            theta_vals[boundary_lo] = 0.0\n            theta_vals[boundary_hi] = two_pi\n\n            if np.any(interior):\n                q_int = q_valid[interior]\n                eps = 1e-15\n                q_clipped = np.clip(q_int, eps, 1.0 - eps)\n                if kappa_val &lt; _JP_KAPPA_TOL:\n                    theta_vals[interior] = two_pi * q_clipped\n                elif abs(psi_val) &lt; _JP_PSI_TOL:\n                    vm = vonmises(kappa=kappa_val, mu=mu_val)\n                    theta_vals[interior] = vm.ppf(q_clipped)\n                else:\n                    theta_curr = two_pi * q_clipped\n                    L = np.zeros_like(theta_curr)\n                    H = np.full_like(theta_curr, two_pi)\n                    tol_cdf = 1e-12\n                    tol_theta = 1e-10\n                    max_iter = 8\n\n                    for _ in range(max_iter):\n                        cdf_vals = np.asarray(\n                            self.cdf(theta_curr, mu_val, kappa_val, psi_val), dtype=float\n                        )\n                        pdf_vals = np.asarray(\n                            self.pdf(theta_curr, mu_val, kappa_val, psi_val), dtype=float\n                        )\n                        delta = cdf_vals - q_clipped\n\n                        L = np.where(delta &lt;= 0.0, theta_curr, L)\n                        H = np.where(delta &gt; 0.0, theta_curr, H)\n\n                        converged = (np.abs(delta) &lt;= tol_cdf) &amp; ((H - L) &lt;= tol_theta)\n                        if np.all(converged):\n                            break\n\n                        denom = np.clip(pdf_vals, 1e-15, None)\n                        step = np.clip(delta / denom, -np.pi, np.pi)\n                        theta_next = theta_curr - step\n                        midpoint = 0.5 * (L + H)\n                        theta_next = np.where(\n                            (theta_next &lt;= L) | (theta_next &gt;= H),\n                            midpoint,\n                            theta_next,\n                        )\n                        theta_curr = np.clip(theta_next, 0.0, two_pi)\n\n                    residual = np.asarray(\n                        self.cdf(theta_curr, mu_val, kappa_val, psi_val),\n                        dtype=float,\n                    ) - q_clipped\n                    mask = (np.abs(residual) &gt; tol_cdf) | ((H - L) &gt; tol_theta)\n                    if np.any(mask):\n                        theta_b = theta_curr.copy()\n                        L_b = L.copy()\n                        H_b = H.copy()\n                        for _ in range(30):\n                            if not np.any(mask):\n                                break\n                            mid = 0.5 * (L_b + H_b)\n                            cdf_mid = np.asarray(\n                                self.cdf(mid, mu_val, kappa_val, psi_val),\n                                dtype=float,\n                            )\n                            delta_mid = cdf_mid - q_clipped\n                            take_upper = (delta_mid &gt; 0.0) &amp; mask\n                            take_lower = (~take_upper) &amp; mask\n                            H_b = np.where(take_upper, mid, H_b)\n                            L_b = np.where(take_lower, mid, L_b)\n                            theta_b = np.where(mask, mid, theta_b)\n                            mask = mask &amp; (np.abs(delta_mid) &gt; tol_cdf)\n                        theta_curr = np.where(mask, 0.5 * (L_b + H_b), theta_b)\n\n                    theta_vals[interior] = theta_curr\n\n            result_vals = theta_vals\n            result_vals[boundary_lo] = 0.0\n            result_vals[boundary_hi] = two_pi\n            result[valid] = result_vals\n\n        result = result.reshape(q_arr.shape)\n        return result\n\n    def ppf(self, q, mu, kappa, psi, *args, **kwargs):\n        r\"\"\"\n        Quantile function of the Jones--Pewsey law.\n\n        The inverse CDF is obtained by a safeguarded Newton iteration that uses\n        the series-based CDF as the residual and the fully normalised PDF as the\n        slope.  Bracketing and bisection polishing guarantee convergence on the\n        circular interval [0, 2\u03c0] while the implementation switches to the\n        closed-form von Mises or uniform solutions in their respective limits.\n\n        Parameters\n        ----------\n        q : array_like\n            Probabilities in [0, 1].\n        mu, kappa, psi : float\n            Jones--Pewsey parameters.\n\n        Returns\n        -------\n        ndarray\n            Quantiles with the same shape as q.\n        \"\"\"\n        return super().ppf(q, mu, kappa, psi, *args, **kwargs)\n\n    def _rvs(self, mu, kappa, psi, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        mu_val = _jp_ensure_scalar(mu, \"mu\")\n        mu_val = float(np.mod(mu_val, 2.0 * np.pi))\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n\n        if size is None:\n            size_tuple = ()\n            total = 1\n        elif np.isscalar(size):\n            size_tuple = (int(size),)\n            total = int(size_tuple[0])\n        else:\n            size_tuple = tuple(int(s) for s in np.atleast_1d(size))\n            total = int(np.prod(size_tuple))\n\n        two_pi = 2.0 * np.pi\n        if kappa_val &lt; _JP_KAPPA_TOL:\n            samples = rng.uniform(0.0, two_pi, size=total)\n            return samples.reshape(size_tuple)\n\n        if abs(psi_val) &lt; _JP_PSI_TOL:\n            return vonmises.rvs(mu=mu_val, kappa=kappa_val, size=size_tuple or None, random_state=rng)\n\n        kappa_env, envelope_const = self._jp_sampler_envelope(mu_val, kappa_val, psi_val)\n        samples = np.empty(total, dtype=float)\n        filled = 0\n\n        while filled &lt; total:\n            remaining = total - filled\n            proposals = vonmises.rvs(\n                mu=mu_val,\n                kappa=kappa_env,\n                size=remaining,\n                random_state=rng,\n            )\n            target_vals = self.pdf(proposals, mu_val, kappa_val, psi_val)\n            proposal_vals = vonmises.pdf(proposals, mu=mu_val, kappa=kappa_env)\n            ratio = np.where(proposal_vals &gt; 0.0, target_vals / (envelope_const * proposal_vals), 0.0)\n            u = rng.uniform(0.0, 1.0, size=remaining)\n            accept = ratio &gt;= u\n            n_accept = int(np.sum(accept))\n            if n_accept &gt; 0:\n                samples[filled:filled + n_accept] = proposals[accept][:n_accept]\n                filled += n_accept\n\n        return samples.reshape(size_tuple)\n\n    def rvs(self, mu, kappa, psi, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the Jones-Pewsey distribution.\n\n        A von Mises envelope is tuned to the target density via local curvature\n        matching and a grid-based optimisation, yielding an acceptance-rejection\n        sampler that is both exact and efficient across the parameter space.\n\n        Parameters\n        ----------\n        mu, kappa, psi : float\n            Jones-Pewsey parameters.\n        size : int or tuple of ints, optional\n            Output shape.  When omitted a single draw is returned.\n        random_state : numpy.random.Generator or compatible seed, optional\n            Source of randomness.\n\n        Returns\n        -------\n        ndarray\n            Sample(s) wrapped to [0, 2\u03c0).\n        \"\"\"\n        return super().rvs(mu, kappa, psi, size=size, random_state=random_state)\n\n    def _jp_sampler_envelope(self, mu, kappa, psi):\n        key = (float(np.mod(mu, 2.0 * np.pi)), float(kappa), float(psi))\n        cached = self._sampler_cache.get(key)\n        if cached is not None:\n            return cached\n\n        kappa_env = _jp_effective_kappa(kappa, psi)\n        phi_grid = np.linspace(0.0, 2.0 * np.pi, 2048, endpoint=False)\n        theta_grid = np.mod(mu + phi_grid, 2.0 * np.pi)\n\n        target_vals = self.pdf(theta_grid, mu, kappa, psi)\n        log_target = np.log(np.clip(target_vals, np.finfo(float).tiny, None))\n\n        kappa_env, envelope_const = _optimize_vonmises_envelope(\n            theta_grid,\n            log_target,\n            mu,\n            max(kappa_env, 1e-6),\n        )\n\n        self._sampler_cache[key] = (kappa_env, envelope_const)\n        return kappa_env, envelope_const\n\n    def _jp_get_series(self, kappa, psi, max_harmonics=256, grid_size=4096):\n        key = (float(kappa), float(psi))\n        cached = self._series_cache.get(key)\n        if cached is not None:\n            return cached\n\n        phi = np.linspace(-np.pi, np.pi, int(grid_size), endpoint=False)\n        theta = np.mod(phi, 2.0 * np.pi)\n        pdf_vals = self.pdf(theta, 0.0, kappa, psi)\n        pdf_vals = np.asarray(pdf_vals, dtype=float)\n\n        delta = (2.0 * np.pi) / float(grid_size)\n        harmonics = np.arange(0, max_harmonics + 1, dtype=float)\n        cos_matrix = np.cos(np.outer(harmonics, phi))\n        cos_coeffs = delta * cos_matrix @ pdf_vals\n        cos_coeffs[0] = 1.0\n        cos_coeffs = np.clip(cos_coeffs, -1.0, 1.0)\n\n        n_idx = harmonics[1:]\n        coeffs = cos_coeffs[1:]\n        if n_idx.size == 0:\n            result = (n_idx, coeffs)\n            self._series_cache[key] = result\n            return result\n\n        contributions = np.abs(coeffs / n_idx)\n        tol = 5e-12\n        mask = contributions &gt; tol\n        if not np.any(mask):\n            n_used = n_idx[:1]\n            coeffs_used = coeffs[:1]\n        else:\n            last = int(np.nonzero(mask)[0][-1]) + 1\n            n_used = n_idx[:last]\n            coeffs_used = coeffs[:last]\n        result = (n_used, coeffs_used)\n        self._series_cache[key] = result\n        return result\n\n    @staticmethod\n    def _jp_series_cumulative(phi_values, n_idx, coeffs):\n        phi_values = np.asarray(phi_values, dtype=float)\n        phi_flat = phi_values.reshape(-1)\n        result = phi_flat / (2.0 * np.pi)\n        if n_idx.size:\n            sin_terms = np.sin(np.outer(phi_flat, n_idx))\n            result += (sin_terms @ (coeffs / n_idx)) / np.pi\n        return result.reshape(phi_values.shape)\n\n    @staticmethod\n    def _jp_series_skew_integral(phi_values, n_idx, coeffs):\n        phi_values = np.asarray(phi_values, dtype=float)\n        phi_flat = phi_values.reshape(-1)\n        base = (1.0 - np.cos(phi_flat)) / (2.0 * np.pi)\n        if n_idx.size:\n            n_arr = n_idx\n            coeff_arr = coeffs\n            contributions = np.zeros_like(phi_flat)\n\n            mask_one = np.isclose(n_arr, 1.0)\n            if np.any(mask_one):\n                coeff_one = float(np.sum(coeff_arr[mask_one]))\n                contributions += coeff_one * ((1.0 - np.cos(2.0 * phi_flat)) / 4.0)\n\n            mask_other = ~mask_one\n            if np.any(mask_other):\n                n_other = n_arr[mask_other]\n                coeff_other = coeff_arr[mask_other]\n                phi_matrix = np.outer(phi_flat, n_other)\n                term_plus = (1.0 - np.cos(phi_matrix + phi_flat[:, None])) / (n_other + 1.0)\n                term_minus = (1.0 - np.cos(phi_matrix - phi_flat[:, None])) / (n_other - 1.0)\n                contributions += 0.5 * (term_plus - term_minus) @ coeff_other\n\n            base += contributions / np.pi\n        return base.reshape(phi_values.shape)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        return_info=False,\n        psi_bounds=(-4.0, 4.0),\n        kappa_bounds=(1e-6, 1e3),\n        optimizer=\"L-BFGS-B\",\n        **kwargs,\n    ):\n        r\"\"\"\n        Estimate Jones--Pewsey parameters from data.\n\n        A moment-based start is built from the sample circular mean\n        \u03bc\u0302 and resultant length r\u2081 with the usual von Mises\n        approximation for \u03ba.  The shape parameter \u03c8 is seeded\n        by scanning a coarse grid and the three parameters are then refined via\n        constrained maximum likelihood:\n\n        ```\n        \u2113(\u03bc, \u03ba, \u03c8) = \u03a3\u1d62 w\u1d62 log( c(\u03ba, \u03c8) K_JP(\u03b8\u1d62 \u2212 \u03bc; \u03ba, \u03c8) ).\n        ```\n\n        The normalising constant c is evaluated using the associated\n        Legendre function whenever stable, with numerical quadrature as a\n        fallback.  Set method=\"moments\" to skip the optimisation and\n        return the analytic seed.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample angles (radians), wrapped internally.\n        weights : array_like, optional\n            Non-negative weights broadcastable to data.\n        method : {\"moments\", \"mle\"}, optional\n            Whether to return the analytic seed or run the numerical MLE.\n        return_info : bool, optional\n            If True return a diagnostics dictionary alongside the estimates.\n        psi_bounds, kappa_bounds : tuple, optional\n            Parameter bounds used by the optimiser.\n        optimizer : str, optional\n            Name of the ``scipy.optimize.minimize`` method.\n\n        Returns\n        -------\n        tuple or (tuple, dict)\n            Estimated parameters (mu, kappa, psi) and, optionally,\n            optimisation diagnostics when return_info is True.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        mu_mom, r1 = circ_mean_and_r(alpha=x, w=w)\n        if not np.isfinite(mu_mom):\n            mu_mom = 0.0\n        mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n        r1 = float(np.clip(r1, 1e-12, 1.0 - 1e-12))\n        n_adjust = int(max(1, round(w_sum)))\n        kappa_mom = float(np.clip(circ_kappa(r=r1, n=n_adjust), kappa_bounds[0], kappa_bounds[1]))\n\n        psi_low, psi_high = psi_bounds\n        psi_grid = np.linspace(psi_low, psi_high, 9)\n\n        def nll(params):\n            mu_param, kappa_param, psi_param = params\n            if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n                return np.inf\n            if not (psi_low &lt;= psi_param &lt;= psi_high):\n                return np.inf\n            mu_wrapped = float(np.mod(mu_param, 2.0 * np.pi))\n            pdf_vals = self.pdf(x, mu_wrapped, kappa_param, psi_param)\n            if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n                return np.inf\n            return float(-np.sum(w * np.log(pdf_vals)))\n\n        psi_init = 0.0\n        best_score = nll((mu_mom, kappa_mom, psi_init))\n        for candidate in psi_grid:\n            score = nll((mu_mom, kappa_mom, candidate))\n            if score &lt; best_score:\n                best_score = score\n                psi_init = float(candidate)\n\n        method_key = method.lower()\n        alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n        method_key = alias.get(method_key, method_key)\n        if method_key not in {\"moments\", \"mle\"}:\n            raise ValueError(\"`method` must be either 'moments' or 'mle'.\")\n\n        if method_key == \"moments\":\n            estimates = (self._wrap_direction(mu_mom), kappa_mom, 0.0)\n            info = {\n                \"method\": \"moments\",\n                \"loglik\": float(-best_score),\n                \"n_effective\": float(n_eff),\n                \"converged\": True,\n            }\n        else:\n            bounds = [(0.0, 2.0 * np.pi), kappa_bounds, psi_bounds]\n            init = np.array([mu_mom, kappa_mom, psi_init], dtype=float)\n            result = minimize(\n                nll,\n                init,\n                method=optimizer,\n                bounds=bounds,\n                **kwargs,\n            )\n            if not result.success:\n                raise RuntimeError(f\"jonespewsey.fit(method='mle') failed: {result.message}\")\n            mu_hat = self._wrap_direction(float(result.x[0]))\n            kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n            psi_hat = float(np.clip(result.x[2], psi_bounds[0], psi_bounds[1]))\n            final_nll = float(result.fun)\n            estimates = (mu_hat, kappa_hat, psi_hat)\n            info = {\n                \"method\": \"mle\",\n                \"loglik\": float(-final_nll),\n                \"n_effective\": float(n_eff),\n                \"converged\": bool(result.success),\n                \"nit\": result.nit,\n                \"optimizer\": optimizer,\n                \"initial\": (mu_mom, kappa_mom, psi_init),\n            }\n\n        if return_info:\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_gen.pdf","title":"<code>pdf(x, mu, kappa, psi, *args, **kwargs)</code>","text":"<p>Probability density function of the Jones-Pewsey distribution.</p> \\[ f(\\theta) = c(\\kappa, \\psi) \\Big(\\cosh(\\kappa \\psi) + \\sinh(\\kappa \\psi) \\cos(\\theta - \\mu)\\Big)^{1/\\psi}, \\] <p>where <code>c(\\kappa, \\psi)</code> is the normalizing constant, evaluated numerically with stable special-case reductions:</p> <pre><code>- ``c = 1 / (2\\pi)`` when ``\\kappa`` is effectively zero (uniform limit).\n- ``c = 1 / (2\\pi I_0(\\kappa))`` as ``\\psi \\to 0`` (von Mises limit).\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, 0 &lt;= mu &lt;= 2*pi.</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, kappa &gt;= 0.</p> required <code>psi</code> <code>float</code> <p>Shape parameter, -\u221e &lt;= psi &lt;= \u221e.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, kappa, psi, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Jones-Pewsey distribution.\n\n    $$\n    f(\\theta) = c(\\kappa, \\psi)\n    \\Big(\\cosh(\\kappa \\psi) + \\sinh(\\kappa \\psi) \\cos(\\theta - \\mu)\\Big)^{1/\\psi},\n    $$\n\n    where ``c(\\kappa, \\psi)`` is the normalizing constant, evaluated numerically with\n    stable special-case reductions:\n\n        - ``c = 1 / (2\\pi)`` when ``\\kappa`` is effectively zero (uniform limit).\n        - ``c = 1 / (2\\pi I_0(\\kappa))`` as ``\\psi \\to 0`` (von Mises limit).\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    mu : float\n        Mean direction, 0 &lt;= mu &lt;= 2*pi.\n    kappa : float\n        Concentration parameter, kappa &gt;= 0.\n    psi : float\n        Shape parameter, -\u221e &lt;= psi &lt;= \u221e.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n    return super().pdf(x, mu, kappa, psi, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_gen.cdf","title":"<code>cdf(x, mu, kappa, psi, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Jones--Pewsey distribution.</p> <p>$$ F(\\theta)=\\frac{\\theta-\\mu}{2\\pi}+\\frac{1}{\\pi} \\sum_{n\\ge 1}\\frac{\\alpha_n(\\kappa,\\psi)}{n} \\sin\\bigl(n(\\theta-\\mu)\\bigr), $$ where the cosine moments \\(\\alpha_n\\) are evaluated through the associated Legendre expression reported by Jones &amp; Pewsey (2005). Coefficients are cached per parameter set and the routine falls back to numerical quadrature only when the series becomes unstable, reproducing the von Mises limit as \\(\\psi \\to 0\\) and the uniform limit as \\(\\kappa \\to 0\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Evaluation points (radians), automatically wrapped onto [0, 2\u03c0).</p> required <code>mu</code> <code>float</code> <p>Jones--Pewsey location, concentration, and shape parameters.</p> required <code>kappa</code> <code>float</code> <p>Jones--Pewsey location, concentration, and shape parameters.</p> required <code>psi</code> <code>float</code> <p>Jones--Pewsey location, concentration, and shape parameters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>CDF values matching the shape of x.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, kappa, psi, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the Jones--Pewsey distribution.\n\n    $$\n    F(\\theta)=\\frac{\\theta-\\mu}{2\\pi}+\\frac{1}{\\pi}\n    \\sum_{n\\ge 1}\\frac{\\alpha_n(\\kappa,\\psi)}{n}\n    \\sin\\bigl(n(\\theta-\\mu)\\bigr),\n    $$\n    where the cosine moments $\\alpha_n$ are evaluated through the\n    associated Legendre expression reported by Jones &amp; Pewsey (2005).\n    Coefficients are cached per parameter set and the routine falls back to\n    numerical quadrature only when the series becomes unstable,\n    reproducing the von Mises limit as $\\psi \\to 0$ and the uniform limit\n    as $\\kappa \\to 0$.\n\n    Parameters\n    ----------\n    x : array_like\n        Evaluation points (radians), automatically wrapped onto [0, 2\u03c0).\n    mu, kappa, psi : float\n        Jones--Pewsey location, concentration, and shape parameters.\n\n    Returns\n    -------\n    ndarray\n        CDF values matching the shape of x.\n    \"\"\"\n    return super().cdf(x, mu, kappa, psi, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_gen.ppf","title":"<code>ppf(q, mu, kappa, psi, *args, **kwargs)</code>","text":"<p>Quantile function of the Jones--Pewsey law.</p> <p>The inverse CDF is obtained by a safeguarded Newton iteration that uses the series-based CDF as the residual and the fully normalised PDF as the slope.  Bracketing and bisection polishing guarantee convergence on the circular interval [0, 2\u03c0] while the implementation switches to the closed-form von Mises or uniform solutions in their respective limits.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Probabilities in [0, 1].</p> required <code>mu</code> <code>float</code> <p>Jones--Pewsey parameters.</p> required <code>kappa</code> <code>float</code> <p>Jones--Pewsey parameters.</p> required <code>psi</code> <code>float</code> <p>Jones--Pewsey parameters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Quantiles with the same shape as q.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, mu, kappa, psi, *args, **kwargs):\n    r\"\"\"\n    Quantile function of the Jones--Pewsey law.\n\n    The inverse CDF is obtained by a safeguarded Newton iteration that uses\n    the series-based CDF as the residual and the fully normalised PDF as the\n    slope.  Bracketing and bisection polishing guarantee convergence on the\n    circular interval [0, 2\u03c0] while the implementation switches to the\n    closed-form von Mises or uniform solutions in their respective limits.\n\n    Parameters\n    ----------\n    q : array_like\n        Probabilities in [0, 1].\n    mu, kappa, psi : float\n        Jones--Pewsey parameters.\n\n    Returns\n    -------\n    ndarray\n        Quantiles with the same shape as q.\n    \"\"\"\n    return super().ppf(q, mu, kappa, psi, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_gen.rvs","title":"<code>rvs(mu, kappa, psi, size=None, random_state=None)</code>","text":"<p>Draw random variates from the Jones-Pewsey distribution.</p> <p>A von Mises envelope is tuned to the target density via local curvature matching and a grid-based optimisation, yielding an acceptance-rejection sampler that is both exact and efficient across the parameter space.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Jones-Pewsey parameters.</p> required <code>kappa</code> <code>float</code> <p>Jones-Pewsey parameters.</p> required <code>psi</code> <code>float</code> <p>Jones-Pewsey parameters.</p> required <code>size</code> <code>int or tuple of ints</code> <p>Output shape.  When omitted a single draw is returned.</p> <code>None</code> <code>random_state</code> <code>numpy.random.Generator or compatible seed</code> <p>Source of randomness.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Sample(s) wrapped to [0, 2\u03c0).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, mu, kappa, psi, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the Jones-Pewsey distribution.\n\n    A von Mises envelope is tuned to the target density via local curvature\n    matching and a grid-based optimisation, yielding an acceptance-rejection\n    sampler that is both exact and efficient across the parameter space.\n\n    Parameters\n    ----------\n    mu, kappa, psi : float\n        Jones-Pewsey parameters.\n    size : int or tuple of ints, optional\n        Output shape.  When omitted a single draw is returned.\n    random_state : numpy.random.Generator or compatible seed, optional\n        Source of randomness.\n\n    Returns\n    -------\n    ndarray\n        Sample(s) wrapped to [0, 2\u03c0).\n    \"\"\"\n    return super().rvs(mu, kappa, psi, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', return_info=False, psi_bounds=(-4.0, 4.0), kappa_bounds=(1e-06, 1000.0), optimizer='L-BFGS-B', **kwargs)</code>","text":"<p>Estimate Jones--Pewsey parameters from data.</p> <p>A moment-based start is built from the sample circular mean \u03bc\u0302 and resultant length r\u2081 with the usual von Mises approximation for \u03ba.  The shape parameter \u03c8 is seeded by scanning a coarse grid and the three parameters are then refined via constrained maximum likelihood:</p> <pre><code>\u2113(\u03bc, \u03ba, \u03c8) = \u03a3\u1d62 w\u1d62 log( c(\u03ba, \u03c8) K_JP(\u03b8\u1d62 \u2212 \u03bc; \u03ba, \u03c8) ).\n</code></pre> <p>The normalising constant c is evaluated using the associated Legendre function whenever stable, with numerical quadrature as a fallback.  Set method=\"moments\" to skip the optimisation and return the analytic seed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample angles (radians), wrapped internally.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights broadcastable to data.</p> <code>None</code> <code>method</code> <code>(moments, mle)</code> <p>Whether to return the analytic seed or run the numerical MLE.</p> <code>\"moments\"</code> <code>return_info</code> <code>bool</code> <p>If True return a diagnostics dictionary alongside the estimates.</p> <code>False</code> <code>psi_bounds</code> <code>tuple</code> <p>Parameter bounds used by the optimiser.</p> <code>(-4.0, 4.0)</code> <code>kappa_bounds</code> <code>tuple</code> <p>Parameter bounds used by the optimiser.</p> <code>(-4.0, 4.0)</code> <code>optimizer</code> <code>str</code> <p>Name of the <code>scipy.optimize.minimize</code> method.</p> <code>'L-BFGS-B'</code> <p>Returns:</p> Type Description <code>tuple or (tuple, dict)</code> <p>Estimated parameters (mu, kappa, psi) and, optionally, optimisation diagnostics when return_info is True.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    return_info=False,\n    psi_bounds=(-4.0, 4.0),\n    kappa_bounds=(1e-6, 1e3),\n    optimizer=\"L-BFGS-B\",\n    **kwargs,\n):\n    r\"\"\"\n    Estimate Jones--Pewsey parameters from data.\n\n    A moment-based start is built from the sample circular mean\n    \u03bc\u0302 and resultant length r\u2081 with the usual von Mises\n    approximation for \u03ba.  The shape parameter \u03c8 is seeded\n    by scanning a coarse grid and the three parameters are then refined via\n    constrained maximum likelihood:\n\n    ```\n    \u2113(\u03bc, \u03ba, \u03c8) = \u03a3\u1d62 w\u1d62 log( c(\u03ba, \u03c8) K_JP(\u03b8\u1d62 \u2212 \u03bc; \u03ba, \u03c8) ).\n    ```\n\n    The normalising constant c is evaluated using the associated\n    Legendre function whenever stable, with numerical quadrature as a\n    fallback.  Set method=\"moments\" to skip the optimisation and\n    return the analytic seed.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample angles (radians), wrapped internally.\n    weights : array_like, optional\n        Non-negative weights broadcastable to data.\n    method : {\"moments\", \"mle\"}, optional\n        Whether to return the analytic seed or run the numerical MLE.\n    return_info : bool, optional\n        If True return a diagnostics dictionary alongside the estimates.\n    psi_bounds, kappa_bounds : tuple, optional\n        Parameter bounds used by the optimiser.\n    optimizer : str, optional\n        Name of the ``scipy.optimize.minimize`` method.\n\n    Returns\n    -------\n    tuple or (tuple, dict)\n        Estimated parameters (mu, kappa, psi) and, optionally,\n        optimisation diagnostics when return_info is True.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    mu_mom, r1 = circ_mean_and_r(alpha=x, w=w)\n    if not np.isfinite(mu_mom):\n        mu_mom = 0.0\n    mu_mom = float(np.mod(mu_mom, 2.0 * np.pi))\n    r1 = float(np.clip(r1, 1e-12, 1.0 - 1e-12))\n    n_adjust = int(max(1, round(w_sum)))\n    kappa_mom = float(np.clip(circ_kappa(r=r1, n=n_adjust), kappa_bounds[0], kappa_bounds[1]))\n\n    psi_low, psi_high = psi_bounds\n    psi_grid = np.linspace(psi_low, psi_high, 9)\n\n    def nll(params):\n        mu_param, kappa_param, psi_param = params\n        if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n            return np.inf\n        if not (psi_low &lt;= psi_param &lt;= psi_high):\n            return np.inf\n        mu_wrapped = float(np.mod(mu_param, 2.0 * np.pi))\n        pdf_vals = self.pdf(x, mu_wrapped, kappa_param, psi_param)\n        if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n            return np.inf\n        return float(-np.sum(w * np.log(pdf_vals)))\n\n    psi_init = 0.0\n    best_score = nll((mu_mom, kappa_mom, psi_init))\n    for candidate in psi_grid:\n        score = nll((mu_mom, kappa_mom, candidate))\n        if score &lt; best_score:\n            best_score = score\n            psi_init = float(candidate)\n\n    method_key = method.lower()\n    alias = {\"analytical\": \"moments\", \"numerical\": \"mle\"}\n    method_key = alias.get(method_key, method_key)\n    if method_key not in {\"moments\", \"mle\"}:\n        raise ValueError(\"`method` must be either 'moments' or 'mle'.\")\n\n    if method_key == \"moments\":\n        estimates = (self._wrap_direction(mu_mom), kappa_mom, 0.0)\n        info = {\n            \"method\": \"moments\",\n            \"loglik\": float(-best_score),\n            \"n_effective\": float(n_eff),\n            \"converged\": True,\n        }\n    else:\n        bounds = [(0.0, 2.0 * np.pi), kappa_bounds, psi_bounds]\n        init = np.array([mu_mom, kappa_mom, psi_init], dtype=float)\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            bounds=bounds,\n            **kwargs,\n        )\n        if not result.success:\n            raise RuntimeError(f\"jonespewsey.fit(method='mle') failed: {result.message}\")\n        mu_hat = self._wrap_direction(float(result.x[0]))\n        kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n        psi_hat = float(np.clip(result.x[2], psi_bounds[0], psi_bounds[1]))\n        final_nll = float(result.fun)\n        estimates = (mu_hat, kappa_hat, psi_hat)\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-final_nll),\n            \"n_effective\": float(n_eff),\n            \"converged\": bool(result.success),\n            \"nit\": result.nit,\n            \"optimizer\": optimizer,\n            \"initial\": (mu_mom, kappa_mom, psi_init),\n        }\n\n    if return_info:\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_sineskewed_gen","title":"<code>jonespewsey_sineskewed_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Sine-Skewed Jones-Pewsey Distribution</p> <p>The Sine-Skewed Jones-Pewsey distribution is a circular distribution defined on \\([0, 2\\pi)\\) that extends the Jones-Pewsey family by incorporating a sine-based skewness adjustment.</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> Note <p>Parameters must be scalar; cached normalisation tables are built per parameter set. Implementation based on Section 4.3.11 of Pewsey et al. (2014)</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class jonespewsey_sineskewed_gen(CircularContinuous):\n    r\"\"\"Sine-Skewed Jones-Pewsey Distribution\n\n    The Sine-Skewed Jones-Pewsey distribution is a circular distribution defined on $[0, 2\\pi)$\n    that extends the Jones-Pewsey family by incorporating a sine-based skewness adjustment.\n\n    ![jonespewsey-sineskewed](../images/circ-mod-jonespewsey-sineskewed.png)\n\n    Methods\n    -------\n    pdf(x, xi, kappa, psi, lmbd)\n        Probability density function.\n\n    cdf(x, xi, kappa, psi, lmbd)\n        Cumulative distribution function.\n\n\n    Note\n    ----\n    Parameters must be scalar; cached normalisation tables are built per parameter set.\n    Implementation based on Section 4.3.11 of Pewsey et al. (2014)\n    \"\"\"\n\n    def _validate_params(self, xi, kappa, psi, lmbd):\n        xi_arr, kappa_arr, psi_arr, lmbd_arr = np.broadcast_arrays(xi, kappa, psi, lmbd)\n        return (\n            (xi_arr &gt;= 0.0)\n            &amp; (xi_arr &lt;= 2.0 * np.pi)\n            &amp; (kappa_arr &gt;= 0.0)\n            &amp; np.isfinite(kappa_arr)\n            &amp; np.isfinite(psi_arr)\n            &amp; (lmbd_arr &gt;= -1.0)\n            &amp; (lmbd_arr &lt;= 1.0)\n        )\n\n    def _argcheck(self, xi, kappa, psi, lmbd):\n        try:\n            return self._validate_params(xi, kappa, psi, lmbd)\n        except ValueError:\n            return False\n\n    def _pdf(self, x, xi, kappa, psi, lmbd):\n        x = np.asarray(x, dtype=float)\n        xi_scalar = _jp_ensure_scalar(xi, \"xi\")\n        kappa_scalar = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_scalar = _jp_ensure_scalar(psi, \"psi\")\n        lmbd_scalar = _jp_ensure_scalar(lmbd, \"lmbd\")\n\n        if abs(kappa_scalar) &lt; _JP_KAPPA_TOL:\n            return (1.0 / (2.0 * np.pi)) * (1.0 + lmbd_scalar * np.sin(x - xi_scalar))\n\n        normalizer = self._get_cached_normalizer(\n            lambda: _c_jonespewsey(xi_scalar, kappa_scalar, psi_scalar),\n            xi_scalar,\n            kappa_scalar,\n            psi_scalar,\n        )\n        self._c = normalizer\n\n        base = _kernel_jonespewsey(x, xi_scalar, kappa_scalar, psi_scalar)\n        return normalizer * base * (1.0 + lmbd_scalar * np.sin(x - xi_scalar))\n\n    def pdf(self, x, xi, kappa, psi, lmbd, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Sine-Skewed Jones-Pewsey distribution.\n\n        $$\n        f(\\theta) = c(\\kappa,\\psi)\\Bigl(\\cosh(\\kappa\\psi)+\n        \\sinh(\\kappa\\psi)\\cos(\\theta-\\xi)\\Bigr)^{1/\\psi}\n        \\bigl(1+\\lambda \\sin(\\theta-\\xi)\\bigr).\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        xi : float\n            Direction parameter (generally not the mean), 0 &lt;= \u03be &lt;= 2*pi.\n        kappa : float\n            Concentration parameter, \u03ba &gt;= 0. Higher values indicate a sharper peak.\n        psi : float\n            Shape parameter, -\u221e &lt;= \u03c8 &lt;= \u221e. When \u03c8=-1, the distribution reduces to the wrapped Cauchy,\n            when \u03c8=0, von Mises, and when \u03c8=1, cardioid.\n        lmbd : float\n            Skewness parameter, -1 &lt; \u03bb &lt; 1. Controls the asymmetry introduced by the sine-skewing.\n\n        Returns\n        -------\n        pdf_values: float\n            Values of the probability density function at the specified points.\n        \"\"\"\n\n        return super().pdf(x, xi, kappa, psi, lmbd, *args, **kwargs)\n\n    def _cdf(self, x, xi, kappa, psi, lmbd):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n        if flat.size == 0:\n            return arr.astype(float)\n\n        xi_val = _jp_ensure_scalar(xi, \"xi\")\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n        lmbd_val = _jp_ensure_scalar(lmbd, \"lmbd\")\n\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt; _JP_KAPPA_TOL:\n            phi = (flat - xi_val) % two_pi\n            base = phi / two_pi\n            skew = (1.0 - np.cos(phi)) / (2.0 * np.pi)\n            cdf = base + lmbd_val * skew\n            return np.clip(cdf, 0.0, 1.0).reshape(arr.shape)\n\n        if abs(psi_val) &lt; _JP_PSI_TOL and abs(lmbd_val) &lt; 1e-12:\n            return jonespewsey.cdf(arr, mu=xi_val, kappa=kappa_val, psi=psi_val)\n\n        n_idx, coeffs = jonespewsey._jp_get_series(kappa_val, psi_val)\n\n        phi_start = (-xi_val) % two_pi\n        phi_end = (flat - xi_val) % two_pi\n\n        H_start = float(jonespewsey._jp_series_cumulative(np.array([phi_start]), n_idx, coeffs)[0])\n        H_end = jonespewsey._jp_series_cumulative(phi_end, n_idx, coeffs)\n\n        if abs(lmbd_val) &gt; 0:\n            J_start = float(jonespewsey._jp_series_skew_integral(np.array([phi_start]), n_idx, coeffs)[0])\n            J_end = jonespewsey._jp_series_skew_integral(phi_end, n_idx, coeffs)\n        else:\n            J_start = 0.0\n            J_end = np.zeros_like(H_end)\n\n        base_cdf = np.where(\n            phi_end &gt;= phi_start,\n            H_end - H_start,\n            1.0 - (H_start - H_end),\n        )\n\n        skew_cdf = np.where(\n            phi_end &gt;= phi_start,\n            J_end - J_start,\n            -(J_start - J_end),\n        )\n\n        cdf = base_cdf + lmbd_val * skew_cdf\n        return np.clip(cdf, 0.0, 1.0).reshape(arr.shape)\n\n    def cdf(self, x, xi, kappa, psi, lmbd, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the sine-skewed Jones--Pewsey law.\n\n        No closed form is available; the implementation integrates the PDF on\n        [0, 2\u03c0) using adaptive quadrature, honouring the symmetric JP and\n        uniform limits when ``lambda`` or ``kappa`` approach zero.\n        \"\"\"\n        return super().cdf(x, xi, kappa, psi, lmbd, *args, **kwargs)\n\n    def _ppf(self, q, xi, kappa, psi, lmbd):\n        xi_val = _jp_ensure_scalar(xi, \"xi\")\n        xi_val = float(np.mod(xi_val, 2.0 * np.pi))\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n        lmbd_val = _jp_ensure_scalar(lmbd, \"lmbd\")\n\n        two_pi = 2.0 * np.pi\n        q_arr = np.asarray(q, dtype=float)\n        if q_arr.size == 0:\n            return q_arr.astype(float)\n\n        flat = q_arr.reshape(-1)\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if np.any(valid):\n            q_valid = flat[valid]\n            boundary_lo = q_valid &lt;= 0.0\n            boundary_hi = q_valid &gt;= 1.0\n            interior = (~boundary_lo) &amp; (~boundary_hi)\n            theta_vals = np.zeros_like(q_valid)\n            theta_vals[boundary_lo] = 0.0\n            theta_vals[boundary_hi] = two_pi\n\n            if np.any(interior):\n                q_int = q_valid[interior]\n                eps = 1e-15\n                q_clipped = np.clip(q_int, eps, 1.0 - eps)\n                if kappa_val &lt; _JP_KAPPA_TOL:\n                    theta_vals[interior] = two_pi * q_clipped\n                elif abs(lmbd_val) &lt; 1e-12:\n                    theta_vals[interior] = jonespewsey.ppf(\n                        q_clipped, mu=xi_val, kappa=kappa_val, psi=psi_val\n                    )\n                else:\n                    theta_curr = two_pi * q_clipped\n                    L = np.zeros_like(theta_curr)\n                    H = np.full_like(theta_curr, two_pi)\n                    tol_cdf = 1e-12\n                    tol_theta = 1e-10\n                    max_iter = 8\n\n                    for _ in range(max_iter):\n                        cdf_vals = np.asarray(\n                            self.cdf(theta_curr, xi_val, kappa_val, psi_val, lmbd_val),\n                            dtype=float,\n                        )\n                        pdf_vals = np.asarray(\n                            self.pdf(theta_curr, xi_val, kappa_val, psi_val, lmbd_val),\n                            dtype=float,\n                        )\n                        delta = cdf_vals - q_clipped\n                        L = np.where(delta &lt;= 0.0, theta_curr, L)\n                        H = np.where(delta &gt; 0.0, theta_curr, H)\n\n                        converged = (np.abs(delta) &lt;= tol_cdf) &amp; ((H - L) &lt;= tol_theta)\n                        if np.all(converged):\n                            break\n\n                        denom = np.clip(pdf_vals, 1e-15, None)\n                        step = np.clip(delta / denom, -np.pi, np.pi)\n                        theta_next = theta_curr - step\n                        midpoint = 0.5 * (L + H)\n                        theta_next = np.where(\n                            (theta_next &lt;= L) | (theta_next &gt;= H),\n                            midpoint,\n                            theta_next,\n                        )\n                        theta_curr = np.clip(theta_next, 0.0, two_pi)\n\n                    residual = np.asarray(\n                        self.cdf(theta_curr, xi_val, kappa_val, psi_val, lmbd_val),\n                        dtype=float,\n                    ) - q_clipped\n                    mask = (np.abs(residual) &gt; tol_cdf) | ((H - L) &gt; tol_theta)\n                    if np.any(mask):\n                        theta_b = theta_curr.copy()\n                        L_b = L.copy()\n                        H_b = H.copy()\n                        for _ in range(30):\n                            if not np.any(mask):\n                                break\n                            mid = 0.5 * (L_b + H_b)\n                            cdf_mid = np.asarray(\n                                self.cdf(mid, xi_val, kappa_val, psi_val, lmbd_val),\n                                dtype=float,\n                            )\n                            delta_mid = cdf_mid - q_clipped\n                            take_upper = (delta_mid &gt; 0.0) &amp; mask\n                            take_lower = (~take_upper) &amp; mask\n                            H_b = np.where(take_upper, mid, H_b)\n                            L_b = np.where(take_lower, mid, L_b)\n                            theta_b = np.where(mask, mid, theta_b)\n                            mask = mask &amp; (np.abs(delta_mid) &gt; tol_cdf)\n                        theta_curr = np.where(mask, 0.5 * (L_b + H_b), theta_b)\n\n                    theta_vals[interior] = theta_curr\n\n            result_vals = theta_vals\n            result_vals[boundary_lo] = 0.0\n            result_vals[boundary_hi] = two_pi\n            result[valid] = result_vals\n\n        return result.reshape(q_arr.shape)\n\n    def ppf(self, q, xi, kappa, psi, lmbd, *args, **kwargs):\n        r\"\"\"\n        Quantile function of the sine-skewed Jones--Pewsey distribution.\n\n        The solver mirrors the symmetric JP inverse CDF while reusing the\n        skew-aware CDF so that round-trip accuracy is preserved even for large\n        skewness.  Uniform and purely symmetric edge cases are delegated to the\n        corresponding closed forms.\n        \"\"\"\n        return super().ppf(q, xi, kappa, psi, lmbd, *args, **kwargs)\n\n    def _rvs(self, xi, kappa, psi, lmbd, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        xi_val = _jp_ensure_scalar(xi, \"xi\")\n        xi_val = float(np.mod(xi_val, 2.0 * np.pi))\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n        lmbd_val = _jp_ensure_scalar(lmbd, \"lmbd\")\n        if abs(lmbd_val) &gt;= 1.0:\n            raise ValueError(\"|lmbd| must be &lt; 1 for sine-skewed Jones-Pewsey.\")\n\n        if size is None:\n            size_tuple = ()\n            total = 1\n        elif np.isscalar(size):\n            size_tuple = (int(size),)\n            total = int(size_tuple[0])\n        else:\n            size_tuple = tuple(int(s) for s in np.atleast_1d(size))\n            total = int(np.prod(size_tuple))\n\n        base_dist = jonespewsey(kappa=kappa_val, psi=psi_val, mu=xi_val)\n        weights_max = 1.0 + abs(lmbd_val)\n\n        samples = np.empty(total, dtype=float)\n        filled = 0\n        while filled &lt; total:\n            remaining = total - filled\n            proposals = base_dist.rvs(size=remaining, random_state=rng)\n            accept_prob = (1.0 + lmbd_val * np.sin(proposals - xi_val)) / weights_max\n            u = rng.uniform(0.0, 1.0, size=remaining)\n            accept = u &lt;= accept_prob\n            n_accept = int(np.sum(accept))\n            if n_accept &gt; 0:\n                samples[filled:filled + n_accept] = proposals[accept][:n_accept]\n                filled += n_accept\n\n        return samples.reshape(size_tuple)\n\n    def rvs(self, xi, kappa, psi, lmbd, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the sine-skewed Jones--Pewsey distribution.\n\n        Sampling follows the acceptance-rejection construction of Abe &amp; Pewsey\n        (2011): draw from the symmetric JP base and accept with probability\n        $$\\frac{1 + \\lambda \\sin\\phi}{1 + |\\lambda|}.$$  This scheme is exact,\n        automatically respects the skew symmetry, and retains the base\n        sampler's efficiency.\n        \"\"\"\n        return super().rvs(xi, kappa, psi, lmbd, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"two-step\",\n        return_info=False,\n        optimizer=\"L-BFGS-B\",\n        refine=False,\n        psi_bounds=(-4.0, 4.0),\n        kappa_bounds=(1e-6, 1e3),\n        lmbd_bounds=(-0.99, 0.99),\n        base_kwargs=None,\n        **kwargs,\n    ):\n        r\"\"\"\n        Estimate sine-skewed JP parameters via a two-step maximum likelihood fit.\n\n        1. Fit the symmetric JP base (xi, kappa, psi) using the MLE routine.\n        2. Maximise the weighted log term sum log(1 + lambda sin(theta_i - xi)).\n        3. Optionally refine all four parameters jointly (set refine=True).\n\n        The acceptance-rejection sampler used for the skewed density makes the\n        likelihood well behaved across |lambda| &lt; 1, while moment starts ensure\n        stability near the uniform limit.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        base_kwargs = {} if base_kwargs is None else dict(base_kwargs)\n        base_estimates, base_info = jonespewsey.fit(\n            x,\n            weights=w,\n            method=\"mle\",\n            psi_bounds=psi_bounds,\n            kappa_bounds=kappa_bounds,\n            optimizer=optimizer,\n            return_info=True,\n            **base_kwargs,\n        )\n        xi_hat, kappa_hat, psi_hat = base_estimates\n\n        lam_low, lam_high = lmbd_bounds\n\n        def lambda_nll(lmbd):\n            if not (lam_low &lt; lmbd &lt; lam_high):\n                return np.inf\n            vals = 1.0 + lmbd * np.sin(x - xi_hat)\n            if np.any(vals &lt;= 0.0) or not np.all(np.isfinite(vals)):\n                return np.inf\n            return float(-np.sum(w * np.log(vals)))\n\n        lambda_result = minimize_scalar(\n            lambda_nll,\n            bounds=lmbd_bounds,\n            method=\"bounded\",\n        )\n        if not lambda_result.success:\n            raise RuntimeError(\"Failed to estimate skewness parameter `lmbd`.\")\n        lmbd_hat = float(np.clip(lambda_result.x, lam_low, lam_high))\n\n        method_key = method.lower()\n        alias = {\"twostep\": \"two-step\", \"two_step\": \"two-step\", \"mle\": \"mle\"}\n        method_key = alias.get(method_key, method_key)\n        if method_key not in {\"two-step\", \"mle\"}:\n            raise ValueError(\"`method` must be either 'two-step' or 'mle'.\")\n\n        if method_key == \"mle\":\n            refine = True\n\n        info = {\n            \"base\": base_info,\n            \"lambda_opt\": {\n                \"success\": bool(lambda_result.success),\n                \"nit\": getattr(lambda_result, \"nit\", None),\n                \"nfev\": getattr(lambda_result, \"nfev\", None),\n            },\n            \"n_effective\": float(n_eff),\n        }\n\n        if refine:\n            bounds = [\n                (0.0, 2.0 * np.pi),\n                kappa_bounds,\n                psi_bounds,\n                lmbd_bounds,\n            ]\n\n            def total_nll(params):\n                xi_param, kappa_param, psi_param, lmbd_param = params\n                if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n                    return np.inf\n                if not (psi_bounds[0] &lt;= psi_param &lt;= psi_bounds[1]):\n                    return np.inf\n                if not (lmbd_bounds[0] &lt; lmbd_param &lt; lmbd_bounds[1]):\n                    return np.inf\n                xi_wrapped = float(np.mod(xi_param, 2.0 * np.pi))\n                pdf_vals = self.pdf(x, xi_wrapped, kappa_param, psi_param, lmbd_param)\n                if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n                    return np.inf\n                return float(-np.sum(w * np.log(pdf_vals)))\n\n            init = np.array([xi_hat, kappa_hat, psi_hat, lmbd_hat], dtype=float)\n            result = minimize(\n                total_nll,\n                init,\n                method=optimizer,\n                bounds=bounds,\n                **kwargs,\n            )\n            if not result.success:\n                raise RuntimeError(\"Sine-skewed JP fit refinement failed: \" + result.message)\n            xi_hat = self._wrap_direction(float(result.x[0]))\n            kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n            psi_hat = float(np.clip(result.x[2], psi_bounds[0], psi_bounds[1]))\n            lmbd_hat = float(np.clip(result.x[3], lmbd_bounds[0], lmbd_bounds[1]))\n            info[\"refinement\"] = {\n                \"success\": bool(result.success),\n                \"nit\": result.nit,\n                \"optimizer\": optimizer,\n            }\n\n        final_pdf = self.pdf(x, xi_hat, kappa_hat, psi_hat, lmbd_hat)\n        loglik = float(np.sum(w * np.log(final_pdf)))\n\n        estimates = (xi_hat, kappa_hat, psi_hat, lmbd_hat)\n        if return_info:\n            info.update(\n                {\n                    \"loglik\": loglik,\n                    \"method\": method_key,\n                    \"estimates\": estimates,\n                }\n            )\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_sineskewed_gen.pdf","title":"<code>pdf(x, xi, kappa, psi, lmbd, *args, **kwargs)</code>","text":"<p>Probability density function of the Sine-Skewed Jones-Pewsey distribution.</p> \\[ f(\\theta) = c(\\kappa,\\psi)\\Bigl(\\cosh(\\kappa\\psi)+ \\sinh(\\kappa\\psi)\\cos(\\theta-\\xi)\\Bigr)^{1/\\psi} \\bigl(1+\\lambda \\sin(\\theta-\\xi)\\bigr). \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>xi</code> <code>float</code> <p>Direction parameter (generally not the mean), 0 &lt;= \u03be &lt;= 2*pi.</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \u03ba &gt;= 0. Higher values indicate a sharper peak.</p> required <code>psi</code> <code>float</code> <p>Shape parameter, -\u221e &lt;= \u03c8 &lt;= \u221e. When \u03c8=-1, the distribution reduces to the wrapped Cauchy, when \u03c8=0, von Mises, and when \u03c8=1, cardioid.</p> required <code>lmbd</code> <code>float</code> <p>Skewness parameter, -1 &lt; \u03bb &lt; 1. Controls the asymmetry introduced by the sine-skewing.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>float</code> <p>Values of the probability density function at the specified points.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, xi, kappa, psi, lmbd, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Sine-Skewed Jones-Pewsey distribution.\n\n    $$\n    f(\\theta) = c(\\kappa,\\psi)\\Bigl(\\cosh(\\kappa\\psi)+\n    \\sinh(\\kappa\\psi)\\cos(\\theta-\\xi)\\Bigr)^{1/\\psi}\n    \\bigl(1+\\lambda \\sin(\\theta-\\xi)\\bigr).\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    xi : float\n        Direction parameter (generally not the mean), 0 &lt;= \u03be &lt;= 2*pi.\n    kappa : float\n        Concentration parameter, \u03ba &gt;= 0. Higher values indicate a sharper peak.\n    psi : float\n        Shape parameter, -\u221e &lt;= \u03c8 &lt;= \u221e. When \u03c8=-1, the distribution reduces to the wrapped Cauchy,\n        when \u03c8=0, von Mises, and when \u03c8=1, cardioid.\n    lmbd : float\n        Skewness parameter, -1 &lt; \u03bb &lt; 1. Controls the asymmetry introduced by the sine-skewing.\n\n    Returns\n    -------\n    pdf_values: float\n        Values of the probability density function at the specified points.\n    \"\"\"\n\n    return super().pdf(x, xi, kappa, psi, lmbd, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_sineskewed_gen.cdf","title":"<code>cdf(x, xi, kappa, psi, lmbd, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the sine-skewed Jones--Pewsey law.</p> <p>No closed form is available; the implementation integrates the PDF on [0, 2\u03c0) using adaptive quadrature, honouring the symmetric JP and uniform limits when <code>lambda</code> or <code>kappa</code> approach zero.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, xi, kappa, psi, lmbd, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the sine-skewed Jones--Pewsey law.\n\n    No closed form is available; the implementation integrates the PDF on\n    [0, 2\u03c0) using adaptive quadrature, honouring the symmetric JP and\n    uniform limits when ``lambda`` or ``kappa`` approach zero.\n    \"\"\"\n    return super().cdf(x, xi, kappa, psi, lmbd, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_sineskewed_gen.ppf","title":"<code>ppf(q, xi, kappa, psi, lmbd, *args, **kwargs)</code>","text":"<p>Quantile function of the sine-skewed Jones--Pewsey distribution.</p> <p>The solver mirrors the symmetric JP inverse CDF while reusing the skew-aware CDF so that round-trip accuracy is preserved even for large skewness.  Uniform and purely symmetric edge cases are delegated to the corresponding closed forms.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, xi, kappa, psi, lmbd, *args, **kwargs):\n    r\"\"\"\n    Quantile function of the sine-skewed Jones--Pewsey distribution.\n\n    The solver mirrors the symmetric JP inverse CDF while reusing the\n    skew-aware CDF so that round-trip accuracy is preserved even for large\n    skewness.  Uniform and purely symmetric edge cases are delegated to the\n    corresponding closed forms.\n    \"\"\"\n    return super().ppf(q, xi, kappa, psi, lmbd, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_sineskewed_gen.rvs","title":"<code>rvs(xi, kappa, psi, lmbd, size=None, random_state=None)</code>","text":"<p>Draw random variates from the sine-skewed Jones--Pewsey distribution.</p> <p>Sampling follows the acceptance-rejection construction of Abe &amp; Pewsey (2011): draw from the symmetric JP base and accept with probability \\(\\(\\frac{1 + \\lambda \\sin\\phi}{1 + |\\lambda|}.\\)\\)  This scheme is exact, automatically respects the skew symmetry, and retains the base sampler's efficiency.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, xi, kappa, psi, lmbd, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the sine-skewed Jones--Pewsey distribution.\n\n    Sampling follows the acceptance-rejection construction of Abe &amp; Pewsey\n    (2011): draw from the symmetric JP base and accept with probability\n    $$\\frac{1 + \\lambda \\sin\\phi}{1 + |\\lambda|}.$$  This scheme is exact,\n    automatically respects the skew symmetry, and retains the base\n    sampler's efficiency.\n    \"\"\"\n    return super().rvs(xi, kappa, psi, lmbd, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_sineskewed_gen.fit","title":"<code>fit(data, *, weights=None, method='two-step', return_info=False, optimizer='L-BFGS-B', refine=False, psi_bounds=(-4.0, 4.0), kappa_bounds=(1e-06, 1000.0), lmbd_bounds=(-0.99, 0.99), base_kwargs=None, **kwargs)</code>","text":"<p>Estimate sine-skewed JP parameters via a two-step maximum likelihood fit.</p> <ol> <li>Fit the symmetric JP base (xi, kappa, psi) using the MLE routine.</li> <li>Maximise the weighted log term sum log(1 + lambda sin(theta_i - xi)).</li> <li>Optionally refine all four parameters jointly (set refine=True).</li> </ol> <p>The acceptance-rejection sampler used for the skewed density makes the likelihood well behaved across |lambda| &lt; 1, while moment starts ensure stability near the uniform limit.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"two-step\",\n    return_info=False,\n    optimizer=\"L-BFGS-B\",\n    refine=False,\n    psi_bounds=(-4.0, 4.0),\n    kappa_bounds=(1e-6, 1e3),\n    lmbd_bounds=(-0.99, 0.99),\n    base_kwargs=None,\n    **kwargs,\n):\n    r\"\"\"\n    Estimate sine-skewed JP parameters via a two-step maximum likelihood fit.\n\n    1. Fit the symmetric JP base (xi, kappa, psi) using the MLE routine.\n    2. Maximise the weighted log term sum log(1 + lambda sin(theta_i - xi)).\n    3. Optionally refine all four parameters jointly (set refine=True).\n\n    The acceptance-rejection sampler used for the skewed density makes the\n    likelihood well behaved across |lambda| &lt; 1, while moment starts ensure\n    stability near the uniform limit.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    base_kwargs = {} if base_kwargs is None else dict(base_kwargs)\n    base_estimates, base_info = jonespewsey.fit(\n        x,\n        weights=w,\n        method=\"mle\",\n        psi_bounds=psi_bounds,\n        kappa_bounds=kappa_bounds,\n        optimizer=optimizer,\n        return_info=True,\n        **base_kwargs,\n    )\n    xi_hat, kappa_hat, psi_hat = base_estimates\n\n    lam_low, lam_high = lmbd_bounds\n\n    def lambda_nll(lmbd):\n        if not (lam_low &lt; lmbd &lt; lam_high):\n            return np.inf\n        vals = 1.0 + lmbd * np.sin(x - xi_hat)\n        if np.any(vals &lt;= 0.0) or not np.all(np.isfinite(vals)):\n            return np.inf\n        return float(-np.sum(w * np.log(vals)))\n\n    lambda_result = minimize_scalar(\n        lambda_nll,\n        bounds=lmbd_bounds,\n        method=\"bounded\",\n    )\n    if not lambda_result.success:\n        raise RuntimeError(\"Failed to estimate skewness parameter `lmbd`.\")\n    lmbd_hat = float(np.clip(lambda_result.x, lam_low, lam_high))\n\n    method_key = method.lower()\n    alias = {\"twostep\": \"two-step\", \"two_step\": \"two-step\", \"mle\": \"mle\"}\n    method_key = alias.get(method_key, method_key)\n    if method_key not in {\"two-step\", \"mle\"}:\n        raise ValueError(\"`method` must be either 'two-step' or 'mle'.\")\n\n    if method_key == \"mle\":\n        refine = True\n\n    info = {\n        \"base\": base_info,\n        \"lambda_opt\": {\n            \"success\": bool(lambda_result.success),\n            \"nit\": getattr(lambda_result, \"nit\", None),\n            \"nfev\": getattr(lambda_result, \"nfev\", None),\n        },\n        \"n_effective\": float(n_eff),\n    }\n\n    if refine:\n        bounds = [\n            (0.0, 2.0 * np.pi),\n            kappa_bounds,\n            psi_bounds,\n            lmbd_bounds,\n        ]\n\n        def total_nll(params):\n            xi_param, kappa_param, psi_param, lmbd_param = params\n            if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n                return np.inf\n            if not (psi_bounds[0] &lt;= psi_param &lt;= psi_bounds[1]):\n                return np.inf\n            if not (lmbd_bounds[0] &lt; lmbd_param &lt; lmbd_bounds[1]):\n                return np.inf\n            xi_wrapped = float(np.mod(xi_param, 2.0 * np.pi))\n            pdf_vals = self.pdf(x, xi_wrapped, kappa_param, psi_param, lmbd_param)\n            if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n                return np.inf\n            return float(-np.sum(w * np.log(pdf_vals)))\n\n        init = np.array([xi_hat, kappa_hat, psi_hat, lmbd_hat], dtype=float)\n        result = minimize(\n            total_nll,\n            init,\n            method=optimizer,\n            bounds=bounds,\n            **kwargs,\n        )\n        if not result.success:\n            raise RuntimeError(\"Sine-skewed JP fit refinement failed: \" + result.message)\n        xi_hat = self._wrap_direction(float(result.x[0]))\n        kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n        psi_hat = float(np.clip(result.x[2], psi_bounds[0], psi_bounds[1]))\n        lmbd_hat = float(np.clip(result.x[3], lmbd_bounds[0], lmbd_bounds[1]))\n        info[\"refinement\"] = {\n            \"success\": bool(result.success),\n            \"nit\": result.nit,\n            \"optimizer\": optimizer,\n        }\n\n    final_pdf = self.pdf(x, xi_hat, kappa_hat, psi_hat, lmbd_hat)\n    loglik = float(np.sum(w * np.log(final_pdf)))\n\n    estimates = (xi_hat, kappa_hat, psi_hat, lmbd_hat)\n    if return_info:\n        info.update(\n            {\n                \"loglik\": loglik,\n                \"method\": method_key,\n                \"estimates\": estimates,\n            }\n        )\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_asym_gen","title":"<code>jonespewsey_asym_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Asymmetric Extended Jones-Pewsey Distribution</p> <p>This distribution is an extension of the Jones-Pewsey family, incorporating asymmetry through a secondary parameter \\(\\nu\\). It is defined on the circular domain \\([0, 2\\pi)\\).</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> Note <p>Parameters must be scalar; cached normalisation tables are built per parameter set. Implementation from 4.3.12 of Pewsey et al. (2014)</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class jonespewsey_asym_gen(CircularContinuous):\n    r\"\"\"Asymmetric Extended Jones-Pewsey Distribution\n\n    This distribution is an extension of the Jones-Pewsey family, incorporating asymmetry\n    through a secondary parameter $\\nu$. It is defined on the circular domain $[0, 2\\pi)$.\n\n    ![jonespewsey-asymext](../images/circ-mod-jonespewsey-asym.png)\n\n    Methods\n    -------\n    pdf(x, xi, kappa, psi, nu)\n        Probability density function.\n\n    cdf(x, xi, kappa, psi, nu)\n        Cumulative distribution function.\n\n\n    Note\n    ----\n    Parameters must be scalar; cached normalisation tables are built per parameter set.\n    Implementation from 4.3.12 of Pewsey et al. (2014)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._sampler_cache = {}\n        self._cdf_table_cache = {}\n\n    def _validate_params(self, xi, kappa, psi, nu):\n        xi_arr, kappa_arr, psi_arr, nu_arr = np.broadcast_arrays(xi, kappa, psi, nu)\n        return (\n            (xi_arr &gt;= 0.0)\n            &amp; (xi_arr &lt;= 2.0 * np.pi)\n            &amp; (kappa_arr &gt;= 0.0)\n            &amp; np.isfinite(kappa_arr)\n            &amp; np.isfinite(psi_arr)\n            &amp; (nu_arr &gt;= 0.0)\n            &amp; (nu_arr &lt; 1.0)\n        )\n\n    def _argcheck(self, xi, kappa, psi, nu):\n        try:\n            return self._validate_params(xi, kappa, psi, nu)\n        except ValueError:\n            return False\n\n    def _pdf(self, x, xi, kappa, psi, nu):\n        x = np.asarray(x, dtype=float)\n        xi_scalar = _jp_ensure_scalar(xi, \"xi\")\n        kappa_scalar = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_scalar = _jp_ensure_scalar(psi, \"psi\")\n        nu_scalar = _jp_ensure_scalar(nu, \"nu\")\n\n        if abs(kappa_scalar) &lt; _JP_KAPPA_TOL:\n            return np.full_like(x, 1.0 / (2.0 * np.pi), dtype=float)\n\n        norm = self._get_cached_normalizer(\n            lambda: _c_jonespewsey_asym(xi_scalar, kappa_scalar, psi_scalar, nu_scalar),\n            xi_scalar,\n            kappa_scalar,\n            psi_scalar,\n            nu_scalar,\n        )\n        self._c = norm\n        base = _kernel_jonespewsey_asym(x, xi_scalar, kappa_scalar, psi_scalar, nu_scalar)\n        return norm * base\n\n    def pdf(self, x, xi, kappa, psi, nu, *args, **kwargs):\n        r\"\"\"\n        Probability density function (PDF) of the Asymmetric Extended Jones-Pewsey distribution.\n\n        The PDF is given by:\n\n        $$\n        f(\\theta) = \\frac{k(\\theta; \\xi, \\kappa, \\psi, \\nu)}{c}\n        $$\n\n        where $k(\\theta; \\xi, \\kappa, \\psi, \\nu)$ is the kernel function defined as:\n\n        $$\n        k(\\theta; \\xi, \\kappa, \\psi, \\nu) =\n        \\begin{cases}\n        \\exp\\left(\\kappa \\cos(\\theta - \\xi + \\nu \\cos(\\theta - \\xi))\\right) &amp; \\text{if } \\psi = 0 \\\\\n        \\left[\\cosh(\\kappa \\psi) + \\sinh(\\kappa \\psi) \\cos(\\theta - \\xi + \\nu \\cos(\\theta - \\xi))\\right]^{1/\\psi} &amp; \\text{if } \\psi \\neq 0\n        \\end{cases}\n        $$\n\n        and $c$ is the normalization constant:\n\n        $$\n        c = \\int_{-\\pi}^{\\pi} k(\\theta; \\xi, \\kappa, \\psi, \\nu) \\, d\\theta\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n        xi : float\n            Direction parameter, $0 \\leq \\xi \\leq 2\\pi$. This typically represents the mode of the distribution.\n        kappa : float\n            Concentration parameter, $\\kappa \\geq 0$. Higher values result in a sharper peak around $\\xi$.\n        psi : float\n            Shape parameter, $-\\infty \\leq \\psi \\leq \\infty$. When $\\psi = 0$, the distribution reduces to a simpler von Mises-like form.\n        nu : float\n            Asymmetry parameter, $0 \\leq \\nu &lt; 1$. Introduces skewness in the circular distribution.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Values of the probability density function at the specified points.\n\n        Notes\n        -----\n        - The normalization constant $c$ is computed numerically using integration.\n        - Special cases:\n            - When $\\psi = 0$, the kernel simplifies to the von Mises-like asymmetric form.\n            - When $\\kappa = 0$, the distribution becomes uniform on $[0, 2\\pi)$.\n        \"\"\"\n        return super().pdf(x, xi, kappa, psi, nu, *args, **kwargs)\n\n    def _cdf(self, x, xi, kappa, psi, nu):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n        if flat.size == 0:\n            return arr.astype(float)\n\n        xi_val = _jp_ensure_scalar(xi, \"xi\")\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n        nu_val = _jp_ensure_scalar(nu, \"nu\")\n\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt; _JP_KAPPA_TOL and abs(nu_val) &lt; 1e-12:\n            return jonespewsey.cdf(arr, mu=xi_val, kappa=kappa_val, psi=psi_val)\n\n        phi_start = (-xi_val) % two_pi\n        phi_end = (flat - xi_val) % two_pi\n\n        phi_grid, cdf_grid = self._asym_cdf_table(xi_val, kappa_val, psi_val, nu_val)\n\n        H_start = float(np.interp(phi_start, phi_grid, cdf_grid, left=0.0, right=1.0))\n        H_end = np.interp(phi_end, phi_grid, cdf_grid, left=0.0, right=1.0)\n\n        cdf = np.where(\n            phi_end &gt;= phi_start,\n            np.clip(H_end - H_start, 0.0, 1.0),\n            np.clip(1.0 - (H_start - H_end), 0.0, 1.0),\n        )\n\n        return cdf.reshape(arr.shape)\n\n    def cdf(self, x, xi, kappa, psi, nu, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the argument-warped JP family.\n\n        The asymmetric transformation phi -&gt; phi + nu cos(phi) is handled by\n        precomputing a high-resolution trapezoidal cumulative table for each\n        parameter set.  Interpolation of this table gives fast evaluations while\n        preserving the limiting cases (nu -&gt; 0 reduces to the symmetric JP CDF).\n        \"\"\"\n        return super().cdf(x, xi, kappa, psi, nu, *args, **kwargs)\n\n    def _ppf(self, q, xi, kappa, psi, nu):\n        xi_val = _jp_ensure_scalar(xi, \"xi\")\n        xi_val = float(np.mod(xi_val, 2.0 * np.pi))\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n        nu_val = _jp_ensure_scalar(nu, \"nu\")\n\n        two_pi = 2.0 * np.pi\n        q_arr = np.asarray(q, dtype=float)\n        if q_arr.size == 0:\n            return q_arr.astype(float)\n\n        flat = q_arr.reshape(-1)\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if np.any(valid):\n            q_valid = flat[valid]\n            boundary_lo = q_valid &lt;= 0.0\n            boundary_hi = q_valid &gt;= 1.0\n            interior = (~boundary_lo) &amp; (~boundary_hi)\n            theta_vals = np.zeros_like(q_valid)\n            theta_vals[boundary_lo] = 0.0\n            theta_vals[boundary_hi] = two_pi\n\n            if np.any(interior):\n                q_int = q_valid[interior]\n                eps = 1e-15\n                q_clipped = np.clip(q_int, eps, 1.0 - eps)\n                if kappa_val &lt; _JP_KAPPA_TOL and nu_val &lt; 1e-12:\n                    theta_vals[interior] = two_pi * q_clipped\n                else:\n                    theta_curr = two_pi * q_clipped\n                    L = np.zeros_like(theta_curr)\n                    H = np.full_like(theta_curr, two_pi)\n                    tol_cdf = 1e-12\n                    tol_theta = 1e-10\n                    max_iter = 8\n\n                    for _ in range(max_iter):\n                        cdf_vals = np.asarray(\n                            self.cdf(theta_curr, xi_val, kappa_val, psi_val, nu_val),\n                            dtype=float,\n                        )\n                        pdf_vals = np.asarray(\n                            self.pdf(theta_curr, xi_val, kappa_val, psi_val, nu_val),\n                            dtype=float,\n                        )\n                        delta = cdf_vals - q_clipped\n                        L = np.where(delta &lt;= 0.0, theta_curr, L)\n                        H = np.where(delta &gt; 0.0, theta_curr, H)\n\n                        converged = (np.abs(delta) &lt;= tol_cdf) &amp; ((H - L) &lt;= tol_theta)\n                        if np.all(converged):\n                            break\n\n                        denom = np.clip(pdf_vals, 1e-15, None)\n                        step = np.clip(delta / denom, -np.pi, np.pi)\n                        theta_next = theta_curr - step\n                        midpoint = 0.5 * (L + H)\n                        theta_next = np.where(\n                            (theta_next &lt;= L) | (theta_next &gt;= H),\n                            midpoint,\n                            theta_next,\n                        )\n                        theta_curr = np.clip(theta_next, 0.0, two_pi)\n\n                    residual = np.asarray(\n                        self.cdf(theta_curr, xi_val, kappa_val, psi_val, nu_val),\n                        dtype=float,\n                    ) - q_clipped\n                    mask = (np.abs(residual) &gt; tol_cdf) | ((H - L) &gt; tol_theta)\n                    if np.any(mask):\n                        theta_b = theta_curr.copy()\n                        L_b = L.copy()\n                        H_b = H.copy()\n                        for _ in range(30):\n                            if not np.any(mask):\n                                break\n                            mid = 0.5 * (L_b + H_b)\n                            cdf_mid = np.asarray(\n                                self.cdf(mid, xi_val, kappa_val, psi_val, nu_val),\n                                dtype=float,\n                            )\n                            delta_mid = cdf_mid - q_clipped\n                            take_upper = (delta_mid &gt; 0.0) &amp; mask\n                            take_lower = (~take_upper) &amp; mask\n                            H_b = np.where(take_upper, mid, H_b)\n                            L_b = np.where(take_lower, mid, L_b)\n                            theta_b = np.where(mask, mid, theta_b)\n                            mask = mask &amp; (np.abs(delta_mid) &gt; tol_cdf)\n                        theta_curr = np.where(mask, 0.5 * (L_b + H_b), theta_b)\n\n                    theta_vals[interior] = theta_curr\n\n            result_vals = theta_vals\n            result_vals[boundary_lo] = 0.0\n            result_vals[boundary_hi] = two_pi\n            result[valid] = result_vals\n\n        return result.reshape(q_arr.shape)\n\n    def ppf(self, q, xi, kappa, psi, nu, *args, **kwargs):\n        r\"\"\"\n        Quantile function of the asymmetric Jones--Pewsey distribution.\n\n        Quantiles are obtained by the same safeguarded Newton iteration as in\n        the symmetric case, with the warp-aware CDF supplying residuals.  When\n        nu is effectively zero the method delegates to the symmetric JP solver.\n        \"\"\"\n        return super().ppf(q, xi, kappa, psi, nu, *args, **kwargs)\n\n    def _rvs(self, xi, kappa, psi, nu, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        xi_val = _jp_ensure_scalar(xi, \"xi\")\n        xi_val = float(np.mod(xi_val, 2.0 * np.pi))\n        kappa_val = _jp_ensure_scalar(kappa, \"kappa\")\n        psi_val = _jp_ensure_scalar(psi, \"psi\")\n        nu_val = _jp_ensure_scalar(nu, \"nu\")\n        if not (0.0 &lt;= nu_val &lt; 1.0):\n            raise ValueError(\"`nu` must lie in [0, 1).\")\n\n        if size is None:\n            size_tuple = ()\n            total = 1\n        elif np.isscalar(size):\n            size_tuple = (int(size),)\n            total = int(size_tuple[0])\n        else:\n            size_tuple = tuple(int(s) for s in np.atleast_1d(size))\n            total = int(np.prod(size_tuple))\n\n        two_pi = 2.0 * np.pi\n        if kappa_val &lt; _JP_KAPPA_TOL:\n            samples = rng.uniform(0.0, two_pi, size=total)\n            return samples.reshape(size_tuple)\n\n        if abs(psi_val) &lt; _JP_PSI_TOL and nu_val &lt; 1e-12:\n            return vonmises.rvs(mu=xi_val, kappa=kappa_val, size=size_tuple or None, random_state=rng)\n\n        kappa_env, envelope_const = self._asym_sampler_envelope(xi_val, kappa_val, psi_val, nu_val)\n        samples = np.empty(total, dtype=float)\n        filled = 0\n\n        while filled &lt; total:\n            remaining = total - filled\n            proposals = vonmises.rvs(\n                mu=xi_val,\n                kappa=kappa_env,\n                size=remaining,\n                random_state=rng,\n            )\n            target_vals = self.pdf(proposals, xi_val, kappa_val, psi_val, nu_val)\n            proposal_vals = vonmises.pdf(proposals, mu=xi_val, kappa=kappa_env)\n            ratio = np.where(proposal_vals &gt; 0.0, target_vals / (envelope_const * proposal_vals), 0.0)\n            u = rng.uniform(0.0, 1.0, size=remaining)\n            accept = ratio &gt;= u\n            n_accept = int(np.sum(accept))\n            if n_accept &gt; 0:\n                samples[filled:filled + n_accept] = proposals[accept][:n_accept]\n                filled += n_accept\n\n        return samples.reshape(size_tuple)\n\n    def rvs(self, xi, kappa, psi, nu, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the asymmetric Jones--Pewsey distribution.\n\n        Sampling uses a curvature-matched von Mises envelope tuned via the\n        optimisation helper, providing an exact acceptance-rejection scheme that\n        works well across nu in [0, 1).  Uniform and symmetric limits are\n        handled explicitly.\n        \"\"\"\n        return super().rvs(xi, kappa, psi, nu, size=size, random_state=random_state)\n\n    def _asym_sampler_envelope(self, xi, kappa, psi, nu):\n        key = (float(np.mod(xi, 2.0 * np.pi)), float(kappa), float(psi), float(nu))\n        cached = self._sampler_cache.get(key)\n        if cached is not None:\n            return cached\n\n        kappa_env = _jp_effective_kappa(kappa, psi)\n        phi_grid = np.linspace(0.0, 2.0 * np.pi, 2048, endpoint=False)\n        theta_grid = np.mod(xi + phi_grid, 2.0 * np.pi)\n\n        target_vals = self.pdf(theta_grid, xi, kappa, psi, nu)\n        log_target = np.log(np.clip(target_vals, np.finfo(float).tiny, None))\n\n        kappa_env, envelope_const = _optimize_vonmises_envelope(\n            theta_grid,\n            log_target,\n            xi,\n            max(kappa_env, 1e-6),\n        )\n\n        self._sampler_cache[key] = (kappa_env, envelope_const)\n        return kappa_env, envelope_const\n\n    def _asym_cdf_table(self, xi, kappa, psi, nu, grid_size=4096):\n        key = (float(np.mod(xi, 2.0 * np.pi)), float(kappa), float(psi), float(nu), int(grid_size))\n        cached = self._cdf_table_cache.get(key)\n        if cached is not None:\n            return cached\n\n        phi_grid = np.linspace(0.0, 2.0 * np.pi, int(grid_size) + 1)\n        theta = np.mod(xi + phi_grid, 2.0 * np.pi)\n        pdf_vals = self.pdf(theta, xi, kappa, psi, nu)\n        pdf_vals = np.asarray(pdf_vals, dtype=float)\n\n        delta = (2.0 * np.pi) / float(grid_size)\n        trap = 0.5 * (pdf_vals[:-1] + pdf_vals[1:]) * delta\n        cdf_vals = np.empty_like(phi_grid)\n        cdf_vals[0] = 0.0\n        cdf_vals[1:] = np.cumsum(trap)\n        total = cdf_vals[-1]\n        if not np.isfinite(total) or total &lt;= 0.0:\n            total = 1.0\n        cdf_vals /= total\n\n        result = (phi_grid, cdf_vals)\n        self._cdf_table_cache[key] = result\n        return result\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        return_info=False,\n        optimizer=\"L-BFGS-B\",\n        psi_bounds=(-4.0, 4.0),\n        kappa_bounds=(1e-6, 1e3),\n        nu_bounds=(0.0, 0.99),\n        base_kwargs=None,\n        **kwargs,\n    ):\n        r\"\"\"\n        Estimate asymmetric JP parameters by maximum likelihood.\n\n        The symmetric JP fit supplies starting values for (xi, kappa, psi) with\n        nu initialised at zero.  The full four-parameter log-likelihood is then\n        optimised under simple bounds, re-using the cached normalising constant\n        and envelope machinery developed for the JP core.\n        \"\"\"\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if x.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(x, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = w_sum**2 / np.sum(w**2)\n\n        base_kwargs = {} if base_kwargs is None else dict(base_kwargs)\n        init_estimates, base_info = jonespewsey.fit(\n            x,\n            weights=w,\n            method=\"mle\",\n            psi_bounds=psi_bounds,\n            kappa_bounds=kappa_bounds,\n            optimizer=optimizer,\n            return_info=True,\n            **base_kwargs,\n        )\n        xi_init, kappa_init, psi_init = init_estimates\n        nu_init = 0.0\n\n        kappa_low, kappa_high = kappa_bounds\n        psi_low, psi_high = psi_bounds\n        nu_low, nu_high = nu_bounds\n\n        def nll(params):\n            xi_param, kappa_param, psi_param, nu_param = params\n            if not (kappa_low &lt;= kappa_param &lt;= kappa_high):\n                return np.inf\n            if not (psi_low &lt;= psi_param &lt;= psi_high):\n                return np.inf\n            if not (nu_low &lt;= nu_param &lt; nu_high):\n                return np.inf\n            xi_wrapped = float(np.mod(xi_param, 2.0 * np.pi))\n            pdf_vals = self.pdf(x, xi_wrapped, kappa_param, psi_param, nu_param)\n            if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n                return np.inf\n            return float(-np.sum(w * np.log(pdf_vals)))\n\n        init = np.array([xi_init, kappa_init, psi_init, nu_init], dtype=float)\n        bounds = [\n            (0.0, 2.0 * np.pi),\n            kappa_bounds,\n            psi_bounds,\n            nu_bounds,\n        ]\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            bounds=bounds,\n            **kwargs,\n        )\n        if not result.success:\n            raise RuntimeError(\"jonespewsey_asym.fit failed: \" + result.message)\n\n        xi_hat = self._wrap_direction(float(result.x[0]))\n        kappa_hat = float(np.clip(result.x[1], kappa_low, kappa_high))\n        psi_hat = float(np.clip(result.x[2], psi_low, psi_high))\n        nu_hat = float(np.clip(result.x[3], nu_low, nu_high - 1e-9))\n\n        final_pdf = self.pdf(x, xi_hat, kappa_hat, psi_hat, nu_hat)\n        loglik = float(np.sum(w * np.log(final_pdf)))\n\n        estimates = (xi_hat, kappa_hat, psi_hat, nu_hat)\n        if return_info:\n            info = {\n                \"base\": base_info,\n                \"loglik\": loglik,\n                \"converged\": bool(result.success),\n                \"nit\": result.nit,\n                \"optimizer\": optimizer,\n                \"n_effective\": float(n_eff),\n            }\n            return estimates, info\n        return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_asym_gen.pdf","title":"<code>pdf(x, xi, kappa, psi, nu, *args, **kwargs)</code>","text":"<p>Probability density function (PDF) of the Asymmetric Extended Jones-Pewsey distribution.</p> <p>The PDF is given by:</p> \\[ f(\\theta) = \\frac{k(\\theta; \\xi, \\kappa, \\psi, \\nu)}{c} \\] <p>where \\(k(\\theta; \\xi, \\kappa, \\psi, \\nu)\\) is the kernel function defined as:</p> \\[ k(\\theta; \\xi, \\kappa, \\psi, \\nu) = \\begin{cases} \\exp\\left(\\kappa \\cos(\\theta - \\xi + \\nu \\cos(\\theta - \\xi))\\right) &amp; \\text{if } \\psi = 0 \\\\ \\left[\\cosh(\\kappa \\psi) + \\sinh(\\kappa \\psi) \\cos(\\theta - \\xi + \\nu \\cos(\\theta - \\xi))\\right]^{1/\\psi} &amp; \\text{if } \\psi \\neq 0 \\end{cases} \\] <p>and \\(c\\) is the normalization constant:</p> \\[ c = \\int_{-\\pi}^{\\pi} k(\\theta; \\xi, \\kappa, \\psi, \\nu) \\, d\\theta \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the PDF, defined on the interval \\([0, 2\\pi)\\).</p> required <code>xi</code> <code>float</code> <p>Direction parameter, \\(0 \\leq \\xi \\leq 2\\pi\\). This typically represents the mode of the distribution.</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\geq 0\\). Higher values result in a sharper peak around \\(\\xi\\).</p> required <code>psi</code> <code>float</code> <p>Shape parameter, \\(-\\infty \\leq \\psi \\leq \\infty\\). When \\(\\psi = 0\\), the distribution reduces to a simpler von Mises-like form.</p> required <code>nu</code> <code>float</code> <p>Asymmetry parameter, \\(0 \\leq \\nu &lt; 1\\). Introduces skewness in the circular distribution.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Values of the probability density function at the specified points.</p> Notes <ul> <li>The normalization constant \\(c\\) is computed numerically using integration.</li> <li>Special cases:<ul> <li>When \\(\\psi = 0\\), the kernel simplifies to the von Mises-like asymmetric form.</li> <li>When \\(\\kappa = 0\\), the distribution becomes uniform on \\([0, 2\\pi)\\).</li> </ul> </li> </ul> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, xi, kappa, psi, nu, *args, **kwargs):\n    r\"\"\"\n    Probability density function (PDF) of the Asymmetric Extended Jones-Pewsey distribution.\n\n    The PDF is given by:\n\n    $$\n    f(\\theta) = \\frac{k(\\theta; \\xi, \\kappa, \\psi, \\nu)}{c}\n    $$\n\n    where $k(\\theta; \\xi, \\kappa, \\psi, \\nu)$ is the kernel function defined as:\n\n    $$\n    k(\\theta; \\xi, \\kappa, \\psi, \\nu) =\n    \\begin{cases}\n    \\exp\\left(\\kappa \\cos(\\theta - \\xi + \\nu \\cos(\\theta - \\xi))\\right) &amp; \\text{if } \\psi = 0 \\\\\n    \\left[\\cosh(\\kappa \\psi) + \\sinh(\\kappa \\psi) \\cos(\\theta - \\xi + \\nu \\cos(\\theta - \\xi))\\right]^{1/\\psi} &amp; \\text{if } \\psi \\neq 0\n    \\end{cases}\n    $$\n\n    and $c$ is the normalization constant:\n\n    $$\n    c = \\int_{-\\pi}^{\\pi} k(\\theta; \\xi, \\kappa, \\psi, \\nu) \\, d\\theta\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n    xi : float\n        Direction parameter, $0 \\leq \\xi \\leq 2\\pi$. This typically represents the mode of the distribution.\n    kappa : float\n        Concentration parameter, $\\kappa \\geq 0$. Higher values result in a sharper peak around $\\xi$.\n    psi : float\n        Shape parameter, $-\\infty \\leq \\psi \\leq \\infty$. When $\\psi = 0$, the distribution reduces to a simpler von Mises-like form.\n    nu : float\n        Asymmetry parameter, $0 \\leq \\nu &lt; 1$. Introduces skewness in the circular distribution.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Values of the probability density function at the specified points.\n\n    Notes\n    -----\n    - The normalization constant $c$ is computed numerically using integration.\n    - Special cases:\n        - When $\\psi = 0$, the kernel simplifies to the von Mises-like asymmetric form.\n        - When $\\kappa = 0$, the distribution becomes uniform on $[0, 2\\pi)$.\n    \"\"\"\n    return super().pdf(x, xi, kappa, psi, nu, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_asym_gen.cdf","title":"<code>cdf(x, xi, kappa, psi, nu, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the argument-warped JP family.</p> <p>The asymmetric transformation phi -&gt; phi + nu cos(phi) is handled by precomputing a high-resolution trapezoidal cumulative table for each parameter set.  Interpolation of this table gives fast evaluations while preserving the limiting cases (nu -&gt; 0 reduces to the symmetric JP CDF).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, xi, kappa, psi, nu, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the argument-warped JP family.\n\n    The asymmetric transformation phi -&gt; phi + nu cos(phi) is handled by\n    precomputing a high-resolution trapezoidal cumulative table for each\n    parameter set.  Interpolation of this table gives fast evaluations while\n    preserving the limiting cases (nu -&gt; 0 reduces to the symmetric JP CDF).\n    \"\"\"\n    return super().cdf(x, xi, kappa, psi, nu, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_asym_gen.ppf","title":"<code>ppf(q, xi, kappa, psi, nu, *args, **kwargs)</code>","text":"<p>Quantile function of the asymmetric Jones--Pewsey distribution.</p> <p>Quantiles are obtained by the same safeguarded Newton iteration as in the symmetric case, with the warp-aware CDF supplying residuals.  When nu is effectively zero the method delegates to the symmetric JP solver.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, xi, kappa, psi, nu, *args, **kwargs):\n    r\"\"\"\n    Quantile function of the asymmetric Jones--Pewsey distribution.\n\n    Quantiles are obtained by the same safeguarded Newton iteration as in\n    the symmetric case, with the warp-aware CDF supplying residuals.  When\n    nu is effectively zero the method delegates to the symmetric JP solver.\n    \"\"\"\n    return super().ppf(q, xi, kappa, psi, nu, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_asym_gen.rvs","title":"<code>rvs(xi, kappa, psi, nu, size=None, random_state=None)</code>","text":"<p>Draw random variates from the asymmetric Jones--Pewsey distribution.</p> <p>Sampling uses a curvature-matched von Mises envelope tuned via the optimisation helper, providing an exact acceptance-rejection scheme that works well across nu in [0, 1).  Uniform and symmetric limits are handled explicitly.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, xi, kappa, psi, nu, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the asymmetric Jones--Pewsey distribution.\n\n    Sampling uses a curvature-matched von Mises envelope tuned via the\n    optimisation helper, providing an exact acceptance-rejection scheme that\n    works well across nu in [0, 1).  Uniform and symmetric limits are\n    handled explicitly.\n    \"\"\"\n    return super().rvs(xi, kappa, psi, nu, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.jonespewsey_asym_gen.fit","title":"<code>fit(data, *, weights=None, return_info=False, optimizer='L-BFGS-B', psi_bounds=(-4.0, 4.0), kappa_bounds=(1e-06, 1000.0), nu_bounds=(0.0, 0.99), base_kwargs=None, **kwargs)</code>","text":"<p>Estimate asymmetric JP parameters by maximum likelihood.</p> <p>The symmetric JP fit supplies starting values for (xi, kappa, psi) with nu initialised at zero.  The full four-parameter log-likelihood is then optimised under simple bounds, re-using the cached normalising constant and envelope machinery developed for the JP core.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    return_info=False,\n    optimizer=\"L-BFGS-B\",\n    psi_bounds=(-4.0, 4.0),\n    kappa_bounds=(1e-6, 1e3),\n    nu_bounds=(0.0, 0.99),\n    base_kwargs=None,\n    **kwargs,\n):\n    r\"\"\"\n    Estimate asymmetric JP parameters by maximum likelihood.\n\n    The symmetric JP fit supplies starting values for (xi, kappa, psi) with\n    nu initialised at zero.  The full four-parameter log-likelihood is then\n    optimised under simple bounds, re-using the cached normalising constant\n    and envelope machinery developed for the JP core.\n    \"\"\"\n    kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n    x = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if x.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(x, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, x.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = w_sum**2 / np.sum(w**2)\n\n    base_kwargs = {} if base_kwargs is None else dict(base_kwargs)\n    init_estimates, base_info = jonespewsey.fit(\n        x,\n        weights=w,\n        method=\"mle\",\n        psi_bounds=psi_bounds,\n        kappa_bounds=kappa_bounds,\n        optimizer=optimizer,\n        return_info=True,\n        **base_kwargs,\n    )\n    xi_init, kappa_init, psi_init = init_estimates\n    nu_init = 0.0\n\n    kappa_low, kappa_high = kappa_bounds\n    psi_low, psi_high = psi_bounds\n    nu_low, nu_high = nu_bounds\n\n    def nll(params):\n        xi_param, kappa_param, psi_param, nu_param = params\n        if not (kappa_low &lt;= kappa_param &lt;= kappa_high):\n            return np.inf\n        if not (psi_low &lt;= psi_param &lt;= psi_high):\n            return np.inf\n        if not (nu_low &lt;= nu_param &lt; nu_high):\n            return np.inf\n        xi_wrapped = float(np.mod(xi_param, 2.0 * np.pi))\n        pdf_vals = self.pdf(x, xi_wrapped, kappa_param, psi_param, nu_param)\n        if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n            return np.inf\n        return float(-np.sum(w * np.log(pdf_vals)))\n\n    init = np.array([xi_init, kappa_init, psi_init, nu_init], dtype=float)\n    bounds = [\n        (0.0, 2.0 * np.pi),\n        kappa_bounds,\n        psi_bounds,\n        nu_bounds,\n    ]\n    result = minimize(\n        nll,\n        init,\n        method=optimizer,\n        bounds=bounds,\n        **kwargs,\n    )\n    if not result.success:\n        raise RuntimeError(\"jonespewsey_asym.fit failed: \" + result.message)\n\n    xi_hat = self._wrap_direction(float(result.x[0]))\n    kappa_hat = float(np.clip(result.x[1], kappa_low, kappa_high))\n    psi_hat = float(np.clip(result.x[2], psi_low, psi_high))\n    nu_hat = float(np.clip(result.x[3], nu_low, nu_high - 1e-9))\n\n    final_pdf = self.pdf(x, xi_hat, kappa_hat, psi_hat, nu_hat)\n    loglik = float(np.sum(w * np.log(final_pdf)))\n\n    estimates = (xi_hat, kappa_hat, psi_hat, nu_hat)\n    if return_info:\n        info = {\n            \"base\": base_info,\n            \"loglik\": loglik,\n            \"converged\": bool(result.success),\n            \"nit\": result.nit,\n            \"optimizer\": optimizer,\n            \"n_effective\": float(n_eff),\n        }\n        return estimates, info\n    return estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.inverse_batschelet_gen","title":"<code>inverse_batschelet_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Inverse Batschelet Distribution</p> <p></p> <p>The inverse Batschelet family (Pewsey, Neuha\u00fcser &amp; Ruxton, 2014, \u00a74.3.13) extends the von Mises distribution by applying two inverse angular warps: a \"peakedness\" transform controlled by \\(\\nu\\), and an inverse Batschelet skew transform governed by \\(\\lambda\\). The resulting density on \\([0, 2\\pi)\\) takes the form</p> \\[ f(\\theta) = c(\\kappa, \\lambda) \\exp\\left[\\kappa \\cos\\left(a\\,t_\\nu^{-1}(\\varphi) + b\\,s_\\lambda^{-1}\\bigl(t_\\nu^{-1}(\\varphi)\\bigr)\\right)\\right], \\] <p>where \\(\\varphi = (\\theta - \\xi) \\bmod 2\\pi - \\pi\\), \\(a = \\tfrac{1 - \\lambda}{1 + \\lambda}\\), \\(b = \\tfrac{2\\lambda}{1 + \\lambda}\\), and the normalising constant \\(c(\\kappa, \\lambda)\\) depends only on \\(\\kappa\\) and \\(\\lambda\\). Setting \\(\\nu = \\lambda = 0\\) recovers the von Mises distribution, while \\(\\kappa \\to 0\\) yields the circular uniform law. Parameters must be scalar; cached normalisation tables are built per parameter set.</p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function.</p> <code>ppf</code> <p>Percent-point function (inverse CDF).</p> <code>rvs</code> <p>Random variates via von Mises acceptance\u2013rejection.</p> <code>fit</code> <p>Moments or maximum-likelihood parameter estimation.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class inverse_batschelet_gen(CircularContinuous):\n    r\"\"\"Inverse Batschelet Distribution\n\n    ![inverse-batschelet](../images/circ-mod-inverse-batschelet.png)\n\n    The inverse Batschelet family (Pewsey, Neuha\u00fcser &amp; Ruxton, 2014, \u00a74.3.13)\n    extends the von Mises distribution by applying two inverse angular warps:\n    a \"peakedness\" transform controlled by $\\nu$, and an inverse\n    Batschelet skew transform governed by $\\lambda$. The resulting density on\n    $[0, 2\\pi)$ takes the form\n\n    $$\n    f(\\theta) = c(\\kappa, \\lambda)\n    \\exp\\left[\\kappa \\cos\\left(a\\,t_\\nu^{-1}(\\varphi) + b\\,s_\\lambda^{-1}\\bigl(t_\\nu^{-1}(\\varphi)\\bigr)\\right)\\right],\n    $$\n\n    where $\\varphi = (\\theta - \\xi) \\bmod 2\\pi - \\pi$,\n    $a = \\tfrac{1 - \\lambda}{1 + \\lambda}$,\n    $b = \\tfrac{2\\lambda}{1 + \\lambda}$, and the normalising constant\n    $c(\\kappa, \\lambda)$ depends only on $\\kappa$ and $\\lambda$.\n    Setting $\\nu = \\lambda = 0$ recovers the von Mises distribution, while\n    $\\kappa \\to 0$ yields the circular uniform law. Parameters must be scalar;\n    cached normalisation tables are built per parameter set.\n\n    Methods\n    -------\n    pdf(x, xi, kappa, nu, lmbd)\n        Probability density function.\n\n    cdf(x, xi, kappa, nu, lmbd)\n        Cumulative distribution function.\n\n    ppf(q, xi, kappa, nu, lmbd)\n        Percent-point function (inverse CDF).\n\n    rvs(xi, kappa, nu, lmbd, size=None, random_state=None)\n        Random variates via von Mises acceptance\u2013rejection.\n\n    fit(data, *, method='mle', ...)\n        Moments or maximum-likelihood parameter estimation.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._invbat_table_cache = {}\n        self._invbat_sampler_cache = {}\n\n    def _clear_normalization_cache(self):\n        super()._clear_normalization_cache()\n        self._invbat_table_cache = {}\n        self._invbat_sampler_cache = {}\n\n    def _validate_params(self, xi, kappa, nu, lmbd):\n        xi_arr, kappa_arr, nu_arr, lmbd_arr = np.broadcast_arrays(xi, kappa, nu, lmbd)\n        return (\n            (xi_arr &gt;= 0.0)\n            &amp; (xi_arr &lt;= 2.0 * np.pi)\n            &amp; (kappa_arr &gt;= 0.0)\n            &amp; np.isfinite(kappa_arr)\n            &amp; (nu_arr &gt;= -1.0)\n            &amp; (nu_arr &lt;= 1.0)\n            &amp; (lmbd_arr &gt;= -1.0)\n            &amp; (lmbd_arr &lt;= 1.0)\n        )\n\n    def _argcheck(self, xi, kappa, nu, lmbd):\n        try:\n            return self._validate_params(xi, kappa, nu, lmbd)\n        except ValueError:\n            return False\n\n    def _pdf(self, x, xi, kappa, nu, lmbd):\n        scalar_input = np.isscalar(x)\n        x_arr = np.asarray([x], dtype=float) if scalar_input else np.asarray(x, dtype=float)\n        if x_arr.size == 0:\n            return x_arr.astype(float)\n\n        xi_val = _invbat_ensure_scalar(xi, \"xi\")\n        kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n        nu_val = _invbat_ensure_scalar(nu, \"nu\")\n        lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n\n        if not (\n            np.isfinite(xi_val)\n            and np.isfinite(kappa_val)\n            and np.isfinite(nu_val)\n            and np.isfinite(lmbd_val)\n        ):\n            result = np.full_like(x_arr, np.nan, dtype=float)\n            return float(result[0]) if scalar_input else result\n\n        if kappa_val &lt;= _INVBAT_KAPPA_TOL:\n            self._c = 1.0 / (2.0 * np.pi)\n            result = np.full_like(x_arr, self._c, dtype=float)\n            return float(result[0]) if scalar_input else result\n\n        normalizer = self._get_cached_normalizer(\n            lambda: _c_invbatschelet(kappa_val, lmbd_val),\n            kappa_val,\n            lmbd_val,\n        )\n        if not np.isfinite(normalizer) or normalizer &lt;= 0.0:\n            normalizer = _c_invbatschelet_numeric(kappa_val, lmbd_val, grid_size=_INVBAT_NUMERIC_GRID)\n        self._c = normalizer\n\n        phi = _tnu(x_arr, nu_val, xi_val)\n        skew = _slmbdinv(phi, lmbd_val)\n\n        if np.isclose(lmbd_val, -1.0):\n            log_kernel = kappa_val * np.cos(phi - np.sin(phi))\n        else:\n            con1 = (1.0 - lmbd_val) / (1.0 + lmbd_val)\n            con2 = (2.0 * lmbd_val) / (1.0 + lmbd_val)\n            log_kernel = kappa_val * np.cos(con1 * phi + con2 * skew)\n\n        pdf_vals = normalizer * np.exp(log_kernel)\n        pdf_vals = np.clip(pdf_vals, 0.0, None).astype(float, copy=False)\n\n        if scalar_input:\n            return float(pdf_vals.reshape(-1)[0])\n        return pdf_vals\n\n    def pdf(self, x, xi, kappa, nu, lmbd, *args, **kwargs):\n        r\"\"\"\n        Probability density function (PDF) of the inverse Batschelet distribution.\n\n        Let\n\n        - $\\varphi = ((\\theta - \\xi + \\pi) \\bmod 2\\pi) - \\pi$,\n        - $t_\\nu^{-1}(\\varphi)$ solve $y - \\nu (1 + \\cos y) = \\varphi$,\n        - $s_\\lambda^{-1}(\\cdot)$ solve $u - \\tfrac{1 + \\lambda}{2} \\sin u = \\cdot$,\n\n        and set\n        $\\phi^\\star = t_\\nu^{-1}(\\varphi)$,\n        $u^\\star = s_\\lambda^{-1}(\\phi^\\star)$,\n        $a = \\tfrac{1 - \\lambda}{1 + \\lambda}$,\n        $b = \\tfrac{2 \\lambda}{1 + \\lambda}$.\n        The inverse Batschelet density is\n\n        $$\n        f(\\theta) = c(\\kappa, \\lambda)\n        \\exp\\bigl[\\kappa \\cos\\bigl(a\\,\\phi^\\star + b\\,u^\\star\\bigr)\\bigr],\n        $$\n\n        where $c(\\kappa,\\lambda)$ is the normalising constant (independent of\n        $\\xi$ and $\\nu$). For $\\kappa \\rightarrow 0$ the distribution reduces to\n        the circular uniform density $1/(2\\pi)$.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n        xi : float\n            Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n        kappa : float\n            Concentration parameter, $\\kappa \\geq 0$. Higher values result in sharper peaks around $\\xi$.\n        nu : float\n            Shape parameter, $-1 \\leq \\nu \\leq 1$. Controls asymmetry through angular transformation.\n        lmbd : float\n            Skewness parameter, $-1 \\leq \\lambda \\leq 1$. Controls the degree of skewness in the distribution.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Values of the probability density function at the specified points.\n        \"\"\"\n        return super().pdf(x, xi, kappa, nu, lmbd, *args, **kwargs)\n\n    def _cdf(self, x, xi, kappa, nu, lmbd):\n        wrapped = self._wrap_angles(x)\n        arr = np.asarray(wrapped, dtype=float)\n        flat = arr.reshape(-1)\n\n        if flat.size == 0:\n            return arr.astype(float)\n\n        xi_val = _invbat_ensure_scalar(xi, \"xi\")\n        kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n        nu_val = _invbat_ensure_scalar(nu, \"nu\")\n        lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n\n        if not (\n            np.isfinite(xi_val)\n            and np.isfinite(kappa_val)\n            and np.isfinite(nu_val)\n            and np.isfinite(lmbd_val)\n        ):\n            return np.full_like(arr, np.nan, dtype=float)\n\n        two_pi = 2.0 * np.pi\n\n        if kappa_val &lt;= _INVBAT_KAPPA_TOL:\n            cdf_flat = flat / two_pi\n        else:\n            table = self._get_invbat_table(kappa_val, nu_val, lmbd_val)\n            phi = ((flat - xi_val + np.pi) % two_pi) - np.pi\n            phi_start = ((-xi_val + np.pi) % two_pi) - np.pi\n            H = table[\"cdf_interp\"](phi)\n            H_start = float(table[\"cdf_interp\"](phi_start))\n            diff = H - H_start\n            cdf_flat = np.where(diff &lt; 0.0, diff + 1.0, diff)\n            cdf_flat = np.clip(cdf_flat, 0.0, 1.0)\n\n        if arr.ndim == 0:\n            value = float(cdf_flat[0])\n            if np.isclose(float(wrapped), two_pi, rtol=0.0, atol=1e-12):\n                return 1.0\n        else:\n            value = cdf_flat.reshape(arr.shape)\n            mask_upper = np.isclose(arr, two_pi, rtol=0.0, atol=1e-12)\n            if np.any(mask_upper):\n                value = value.copy()\n                value[mask_upper] = 1.0\n        return value\n\n    def cdf(self, x, xi, kappa, nu, lmbd, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the inverse Batschelet distribution.\n\n        The implementation precomputes the normalised primitive on a periodic grid\n        in the centred angle $\\varphi = (\\theta - \\xi) \\bmod 2\\pi - \\pi$. For each\n        grid node, the inverse peakedness transform $t_\\nu^{-1}$ and inverse\n        Batschelet skew $s_\\lambda^{-1}$ are evaluated, and the resulting kernel is\n        accumulated via a trapezoidal rule. The cumulative table is cached per\n        parameter triple $(\\kappa, \\nu, \\lambda)$, enabling $O(1)$ queries after the\n        initial $O(N)$ precomputation. The limit $\\kappa \\to 0$ reduces to the\n        circular uniform CDF $\\theta / (2\\pi)$.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n        xi : float\n            Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n        kappa : float\n            Concentration parameter, $\\kappa \\geq 0$.\n        nu : float\n            Shape parameter, $-1 \\leq \\nu \\leq 1$.\n        lmbd : float\n            Skewness parameter, $-1 \\leq \\lambda \\leq 1$.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative probabilities corresponding to `x`.\n        \"\"\"\n        xi_val = _invbat_ensure_scalar(xi, \"xi\")\n        kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n        nu_val = _invbat_ensure_scalar(nu, \"nu\")\n        lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n        return super().cdf(x, xi_val, kappa_val, nu_val, lmbd_val, *args, **kwargs)\n\n    def _ppf(self, q, xi, kappa, nu, lmbd):\n        xi_val = _invbat_ensure_scalar(xi, \"xi\")\n        kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n        nu_val = _invbat_ensure_scalar(nu, \"nu\")\n        lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n\n        q_arr = np.asarray(q, dtype=float)\n        flat = q_arr.reshape(-1)\n        if flat.size == 0:\n            return q_arr.astype(float)\n\n        two_pi = 2.0 * np.pi\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if not np.any(valid):\n            shaped = result.reshape(q_arr.shape)\n            return float(shaped) if q_arr.ndim == 0 else shaped\n\n        q_valid = flat[valid]\n        close_zero = np.isclose(q_valid, 0.0, rtol=0.0, atol=1e-12)\n        close_one = np.isclose(q_valid, 1.0, rtol=0.0, atol=1e-12)\n\n        if kappa_val &lt;= _INVBAT_KAPPA_TOL:\n            theta = (two_pi * q_valid) % two_pi\n            if np.any(close_zero):\n                theta[close_zero] = 0.0\n            if np.any(close_one):\n                theta[close_one] = two_pi\n            result[valid] = theta\n        else:\n            table = self._get_invbat_table(kappa_val, nu_val, lmbd_val)\n            phi_grid = table[\"phi\"]\n            cdf_grid = table[\"cdf\"]\n            cdf_interp = table[\"cdf_interp\"]\n            inv_interp = table[\"inv_cdf_interp\"]\n            pdf_interp = table[\"pdf_interp\"]\n\n            phi_start = ((-xi_val + np.pi) % two_pi) - np.pi\n            H_start = float(cdf_interp(phi_start))\n            targets = (H_start + q_valid) % 1.0\n\n            phi_candidates = (\n                inv_interp(targets)\n                if inv_interp is not None\n                else np.interp(targets, cdf_grid, phi_grid, left=phi_grid[0], right=phi_grid[-1])\n            )\n\n            theta_vals = np.empty_like(q_valid)\n            for idx, (target, phi0) in enumerate(zip(targets, phi_candidates)):\n                if close_zero[idx]:\n                    theta_vals[idx] = 0.0\n                    continue\n                if close_one[idx]:\n                    theta_vals[idx] = two_pi\n                    continue\n\n                i_hi = int(np.clip(np.searchsorted(cdf_grid, target, side=\"right\"), 1, len(phi_grid) - 1))\n                phi_lo = float(phi_grid[i_hi - 1])\n                phi_hi = float(phi_grid[i_hi])\n                phi = float(np.clip(phi0, phi_lo, phi_hi))\n\n                for _ in range(_INVBAT_NEWTON_MAXITER):\n                    H_phi = float(cdf_interp(phi))\n                    residual = H_phi - target\n                    pdf_val = float(pdf_interp(phi))\n                    pdf_val = max(pdf_val, np.finfo(float).tiny)\n\n                    if abs(residual) &lt;= _INVBAT_NEWTON_TOL and (phi_hi - phi_lo) &lt;= _INVBAT_NEWTON_WIDTH_TOL:\n                        break\n\n                    if residual &gt; 0.0:\n                        phi_hi = min(phi_hi, phi)\n                    else:\n                        phi_lo = max(phi_lo, phi)\n\n                    step = residual / pdf_val\n                    phi_candidate = phi - step\n                    if not np.isfinite(phi_candidate) or phi_candidate &lt;= phi_lo or phi_candidate &gt;= phi_hi:\n                        phi_candidate = 0.5 * (phi_lo + phi_hi)\n                    phi = float(np.clip(phi_candidate, phi_lo, phi_hi))\n\n                theta_vals[idx] = (xi_val + phi) % two_pi\n\n            result[valid] = theta_vals\n\n        shaped = result.reshape(q_arr.shape)\n        if q_arr.ndim == 0:\n            return float(shaped)\n        return shaped\n\n    def ppf(self, q, xi, kappa, nu, lmbd, *args, **kwargs):\n        r\"\"\"\n        Percent-point function (quantile) of the inverse Batschelet distribution.\n\n        Quantiles are obtained by inverting the cached cumulative table described in\n        `cdf`. A monotone initial guess supplied by the table inverse is refined\n        with safeguarded Newton steps that leverage the tabulated density, while\n        preserving a bracketing interval. For $\\kappa \\rightarrow 0$, the quantile\n        reduces to the linear uniform mapping $2\\pi q$.\n\n        Parameters\n        ----------\n        q : array_like\n            Quantiles to evaluate (0 &lt;= q &lt;= 1).\n        xi : float\n            Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n        kappa : float\n            Concentration parameter, $\\kappa \\geq 0$.\n        nu : float\n            Shape parameter, $-1 \\leq \\nu \\leq 1$.\n        lmbd : float\n            Skewness parameter, $-1 \\leq \\lambda \\leq 1$.\n\n        Returns\n        -------\n        ppf_values : array_like\n            Angles corresponding to the probabilities in `q`.\n        \"\"\"\n        xi_val = _invbat_ensure_scalar(xi, \"xi\")\n        kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n        nu_val = _invbat_ensure_scalar(nu, \"nu\")\n        lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n        return super().ppf(q, xi_val, kappa_val, nu_val, lmbd_val, *args, **kwargs)\n\n    def _get_invbat_sampler_params(self, kappa, nu, lmbd):\n        key = (float(kappa), float(nu), float(lmbd))\n        params = self._invbat_sampler_cache.get(key)\n        if params is not None:\n            return params\n\n        table = self._get_invbat_table(kappa, nu, lmbd)\n        phi = table[\"phi\"]\n        pdf = table[\"pdf\"]\n        log_pdf = np.log(np.clip(pdf, np.finfo(float).tiny, None))\n\n        idx0 = int(np.argmin(np.abs(phi)))\n        if idx0 == 0:\n            idx0 = 1\n        elif idx0 == phi.size - 1:\n            idx0 = phi.size - 2\n\n        h1 = phi[idx0] - phi[idx0 - 1]\n        h2 = phi[idx0 + 1] - phi[idx0]\n        if not np.isfinite(h1) or not np.isfinite(h2) or h1 == 0.0 or h2 == 0.0:\n            curvature = max(kappa, 1.0)\n        else:\n            d2 = (\n                log_pdf[idx0 + 1]\n                - 2.0 * log_pdf[idx0]\n                + log_pdf[idx0 - 1]\n            ) / ((0.5 * (h1 + h2)) ** 2)\n            curvature = max(-d2, 1e-3)\n\n        kappa_env = float(np.clip(curvature, _INVBAT_ENV_MIN_KAPPA, _INVBAT_KAPPA_UPPER))\n        log_vm_norm = np.log(2.0 * np.pi) + np.log(i0e(kappa_env)) + kappa_env\n        log_ratio = log_pdf + log_vm_norm - kappa_env * np.cos(phi)\n        log_multiplier = float(np.max(log_ratio))\n        multiplier = float(np.exp(log_multiplier) * 1.02)\n\n        params = {\n            \"kappa_env\": kappa_env,\n            \"log_vm_norm\": log_vm_norm,\n            \"log_multiplier\": np.log(multiplier),\n            \"multiplier\": multiplier,\n        }\n        self._invbat_sampler_cache[key] = params\n        return params\n\n    def _rvs(self, xi, kappa, nu, lmbd, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        xi_val = float(np.mod(_invbat_ensure_scalar(xi, \"xi\"), 2.0 * np.pi))\n        kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n        nu_val = _invbat_ensure_scalar(nu, \"nu\")\n        lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n\n        if not (\n            np.isfinite(xi_val)\n            and np.isfinite(kappa_val)\n            and np.isfinite(nu_val)\n            and np.isfinite(lmbd_val)\n        ):\n            raise ValueError(\"`xi`, `kappa`, `nu`, and `lmbd` must be finite scalars.\")\n\n        if size is None:\n            shape = ()\n            total = 1\n        else:\n            if np.isscalar(size):\n                shape = (int(size),)\n            else:\n                shape = tuple(int(dim) for dim in np.atleast_1d(size))\n            total = int(np.prod(shape, dtype=int))\n            if total &lt; 0:\n                raise ValueError(\"`size` must describe a non-negative number of samples.\")\n\n        two_pi = 2.0 * np.pi\n\n        if total == 0:\n            empty = np.empty(shape, dtype=float)\n            return float(empty) if empty.ndim == 0 else empty\n\n        if kappa_val &lt;= _INVBAT_KAPPA_TOL:\n            samples = rng.uniform(0.0, two_pi, size=shape)\n            return float(samples) if samples.ndim == 0 else samples\n\n        table = self._get_invbat_table(kappa_val, nu_val, lmbd_val)\n        sampler = self._get_invbat_sampler_params(kappa_val, nu_val, lmbd_val)\n        kappa_env = sampler[\"kappa_env\"]\n        log_vm_norm = sampler[\"log_vm_norm\"]\n        log_multiplier = sampler[\"log_multiplier\"]\n        pdf_interp = table[\"pdf_interp\"]\n\n        samples = np.empty(total, dtype=float)\n        filled = 0\n        batch_base = max(8, min(4 * total, 4096))\n\n        while filled &lt; total:\n            batch = min(batch_base, total - filled) if filled &gt; 0 else batch_base\n            proposals = rng.vonmises(xi_val, kappa_env, size=batch)\n            phi = ((proposals - xi_val + np.pi) % two_pi) - np.pi\n\n            pdf_vals = np.clip(pdf_interp(phi), np.finfo(float).tiny, None)\n            log_target = np.log(pdf_vals)\n            log_env = kappa_env * np.cos(phi) - log_vm_norm\n            log_accept = log_target - log_env - log_multiplier\n\n            accept_mask = np.log(rng.random(size=batch)) &lt;= log_accept\n            if not np.any(accept_mask):\n                continue\n\n            accepted = proposals[accept_mask]\n            take = min(accepted.size, total - filled)\n            samples[filled : filled + take] = accepted[:take]\n            filled += take\n\n        samples = np.mod(samples, two_pi)\n        samples = samples.reshape(shape)\n        if samples.ndim == 0:\n            return float(samples)\n        return samples\n\n    def rvs(self, xi=None, kappa=None, nu=None, lmbd=None, size=None, random_state=None):\n        r\"\"\"\n        Draw random variates from the inverse Batschelet distribution.\n\n        Sampling proceeds by acceptance--rejection with a von Mises envelope whose\n        concentration is matched to the curvature of the inverse Batschelet kernel at\n        the mode. Envelope constants are calibrated on the cached spectral grid used\n        for `cdf`, so repeated sampling calls with the same parameters are fast\n        and stable across the entire parameter range.\n\n        Parameters\n        ----------\n        xi : float\n            Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n        kappa : float\n            Concentration parameter, $\\kappa \\geq 0$.\n        nu : float\n            Shape parameter, $-1 \\leq \\nu \\leq 1$.\n        lmbd : float\n            Skewness parameter, $-1 \\leq \\lambda \\leq 1$.\n        size : int or tuple of ints, optional\n            Desired output shape.\n        random_state : {None, int, np.random.Generator}, optional\n            Random number generator specification.\n\n        Returns\n        -------\n        rvs : array_like\n            Random variates on $[0, 2\\pi)$ sampled from the inverse Batschelet\n            distribution.\n        \"\"\"\n\n        xi_val = _invbat_ensure_scalar(xi, \"xi\")\n        kappa_val = _invbat_ensure_scalar(kappa, \"kappa\")\n        nu_val = _invbat_ensure_scalar(nu, \"nu\")\n        lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n        return super().rvs(xi_val, kappa_val, nu_val, lmbd_val, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        optimizer=\"L-BFGS-B\",\n        options=None,\n        nu_grid=None,\n        lmbd_grid=None,\n        kappa_bounds=(1e-6, _INVBAT_KAPPA_UPPER),\n        nu_bounds=(-0.99, 0.99),\n        lmbd_bounds=(-0.99, 0.99),\n        return_info=False,\n        **minimize_kwargs,\n    ):\n        r\"\"\"\n        Estimate $(\\xi, \\kappa, \\nu, \\lambda)$ from circular data.\n\n        ``method='mle'`` maximises the weighted log-likelihood using the cached\n        spectral tables for the pdf and normalising constant. ``method='moments'``\n        returns the circular mean, ``circ_kappa`` estimate, and sets $(\\nu, \\lambda)\n        = (0, 0)$.\n\n        Parameters\n        ----------\n        data : array_like\n            Sample of angles.\n        weights : array_like, optional\n            Non-negative weights broadcastable to ``data``.\n        method : {'mle', 'moments'}, default 'mle'\n            Estimation method.\n        optimizer : str, optional\n            SciPy optimiser for maximum likelihood.\n        options : dict, optional\n            Optimiser options forwarded to :func:`scipy.optimize.minimize`.\n        nu_grid : array_like, optional\n            Candidate $\n            u$ values for profiling the starting point.\n        lmbd_grid : array_like, optional\n            Candidate $\n            u$ values for $\n            lambda$ profiling.\n        kappa_bounds, nu_bounds, lmbd_bounds : tuple, optional\n            Parameter bounds enforced during optimisation.\n        return_info : bool, optional\n            If True, also return a dictionary with optimisation diagnostics.\n        **minimize_kwargs :\n            Additional keyword arguments forwarded to\n            :func:`scipy.optimize.minimize`.\n\n        Returns\n        -------\n        params : tuple\n            Estimated parameters ``(xi, kappa, nu, lmbd)``.\n        info : dict, optional\n            Returned when ``return_info=True`` with optimisation diagnostics.\n        \"\"\"\n\n        minimize_kwargs = self._sanitize_fit_kwargs(minimize_kwargs)\n        minimize_kwargs.pop(\"floc\", None)\n        minimize_kwargs.pop(\"fscale\", None)\n\n        data_arr = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if data_arr.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(data_arr, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, data_arr.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0.0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = float(w_sum**2 / np.sum(w**2))\n\n        xi_mom, r1 = circ_mean_and_r(alpha=data_arr, w=w)\n        if not np.isfinite(xi_mom):\n            xi_mom = 0.0\n        xi_mom = float(np.mod(xi_mom, 2.0 * np.pi))\n        r1 = float(np.clip(r1, 1e-12, 1.0 - 1e-12))\n        n_adjust = int(max(1, round(w_sum)))\n        kappa_mom = float(np.clip(circ_kappa(r=r1, n=n_adjust), kappa_bounds[0], kappa_bounds[1]))\n\n        if method == \"moments\":\n            estimates = (xi_mom, kappa_mom, 0.0, 0.0)\n            if return_info:\n                info = {\n                    \"method\": \"moments\",\n                    \"converged\": True,\n                    \"loglik\": float(-np.sum(w) * np.log(2.0 * np.pi)) if kappa_mom &lt;= _INVBAT_KAPPA_TOL else float(\"nan\"),\n                    \"n_effective\": n_eff,\n                }\n                return estimates, info\n            return estimates\n\n        method_key = str(method).lower()\n        if method_key != \"mle\":\n            raise ValueError(\"`method` must be one of {'mle', 'moments' }.\")\n\n        two_pi = 2.0 * np.pi\n\n        if nu_grid is None:\n            nu_grid = np.linspace(nu_bounds[0], nu_bounds[1], 5)\n        else:\n            nu_grid = np.asarray(nu_grid, dtype=float)\n\n        if lmbd_grid is None:\n            lmbd_grid = np.linspace(lmbd_bounds[0], lmbd_bounds[1], 5)\n        else:\n            lmbd_grid = np.asarray(lmbd_grid, dtype=float)\n\n        def nll(params):\n            xi_param, kappa_param, nu_param, lmbd_param = params\n            if not (0.0 &lt;= xi_param &lt;= two_pi):\n                return np.inf\n            if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n                return np.inf\n            if not (nu_bounds[0] &lt;= nu_param &lt;= nu_bounds[1]):\n                return np.inf\n            if not (lmbd_bounds[0] &lt;= lmbd_param &lt;= lmbd_bounds[1]):\n                return np.inf\n\n            xi_wrapped = float(np.mod(xi_param, two_pi))\n            if kappa_param &lt;= _INVBAT_KAPPA_TOL:\n                log_pdf = -np.log(two_pi)\n                return float(-np.sum(w * log_pdf))\n\n            table = self._get_invbat_table(float(kappa_param), float(nu_param), float(lmbd_param))\n            phi = ((data_arr - xi_wrapped + np.pi) % two_pi) - np.pi\n            pdf_vals = table[\"pdf_interp\"](phi)\n            if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n                return np.inf\n            return float(-np.sum(w * np.log(pdf_vals)))\n\n        best_nu = 0.0\n        best_lmbd = 0.0\n        best_score = nll((xi_mom, kappa_mom, best_nu, best_lmbd))\n        for nu_candidate in np.unique(np.concatenate(([0.0], nu_grid))):\n            for lmbd_candidate in np.unique(np.concatenate(([0.0], lmbd_grid))):\n                score = nll((xi_mom, kappa_mom, float(nu_candidate), float(lmbd_candidate)))\n                if score &lt; best_score:\n                    best_score = score\n                    best_nu = float(nu_candidate)\n                    best_lmbd = float(lmbd_candidate)\n\n        init = np.array([xi_mom, kappa_mom, best_nu, best_lmbd], dtype=float)\n        bounds = [\n            (0.0, two_pi),\n            (kappa_bounds[0], kappa_bounds[1]),\n            (nu_bounds[0], nu_bounds[1]),\n            (lmbd_bounds[0], lmbd_bounds[1]),\n        ]\n\n        options = {} if options is None else dict(options)\n\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            bounds=bounds,\n            options=options,\n            **minimize_kwargs,\n        )\n\n        optimizer_used = optimizer\n        if not result.success and optimizer != \"Powell\":\n            fallback = minimize(\n                nll,\n                init,\n                method=\"Powell\",\n                bounds=bounds,\n                options={},\n                **minimize_kwargs,\n            )\n            if fallback.success:\n                result = fallback\n                optimizer_used = \"Powell\"\n\n        if not result.success:\n            raise RuntimeError(f\"Maximum likelihood fit failed: {result.message}\")\n\n        xi_hat = self._wrap_direction(float(result.x[0]))\n        kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n        nu_hat = float(np.clip(result.x[2], nu_bounds[0], nu_bounds[1]))\n        lmbd_hat = float(np.clip(result.x[3], lmbd_bounds[0], lmbd_bounds[1]))\n\n        estimates = (xi_hat, kappa_hat, nu_hat, lmbd_hat)\n        if not return_info:\n            return estimates\n\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-result.fun),\n            \"n_effective\": n_eff,\n            \"converged\": bool(result.success),\n            \"optimizer\": optimizer_used,\n            \"nit\": getattr(result, \"nit\", np.nan),\n            \"nfev\": getattr(result, \"nfev\", np.nan),\n            \"message\": result.message,\n        }\n        return estimates, info\n\n    def _get_invbat_table(self, kappa, nu, lmbd, grid_size=None):\n        kappa_val = float(np.clip(kappa, 0.0, _INVBAT_KAPPA_UPPER))\n        nu_val = float(nu)\n        lmbd_val = float(lmbd)\n        if kappa_val &lt;= _INVBAT_KAPPA_TOL:\n            phi = np.array([-np.pi, np.pi], dtype=float)\n            pdf_vals = np.full(2, 1.0 / (2.0 * np.pi), dtype=float)\n            cdf_interp = PchipInterpolator(phi, [0.0, 1.0], extrapolate=True)\n            pdf_interp = PchipInterpolator(phi, pdf_vals, extrapolate=True)\n            return {\n                \"phi\": phi,\n                \"pdf\": pdf_vals,\n                \"cdf\": np.array([0.0, 1.0], dtype=float),\n                \"cdf_interp\": cdf_interp,\n                \"pdf_interp\": pdf_interp,\n                \"inv_cdf_interp\": PchipInterpolator([0.0, 1.0], phi, extrapolate=True),\n                \"log_normalizer\": -np.log(2.0 * np.pi),\n            }\n\n        if grid_size is None:\n            grid_size = _invbat_grid_size(kappa_val, nu_val, lmbd_val)\n        grid_int = int(grid_size)\n        key = (kappa_val, nu_val, lmbd_val, grid_int)\n        table = self._invbat_table_cache.get(key)\n        if table is None:\n            table = self._build_invbat_table(kappa_val, nu_val, lmbd_val, grid_int)\n            self._invbat_table_cache[key] = table\n        return table\n\n    def _build_invbat_table(self, kappa, nu, lmbd, grid_size):\n        phi = np.linspace(-np.pi, np.pi, grid_size + 1, dtype=float)\n        phi_star = _tnu(phi, nu, 0.0)\n        skew = _slmbdinv(phi_star, lmbd)\n\n        if np.isclose(lmbd, -1.0, atol=_INVBAT_LMBDA_TOL):\n            log_kernel = kappa * np.cos(phi_star - np.sin(phi_star))\n        else:\n            con1 = (1.0 - lmbd) / (1.0 + lmbd)\n            con2 = (2.0 * lmbd) / (1.0 + lmbd)\n            log_kernel = kappa * np.cos(con1 * phi_star + con2 * skew)\n\n        normalizer = self._get_cached_normalizer(\n            lambda: _c_invbatschelet(kappa, lmbd),\n            kappa,\n            lmbd,\n        )\n        if not np.isfinite(normalizer) or normalizer &lt;= 0.0:\n            normalizer = _c_invbatschelet_numeric(kappa, lmbd, grid_size=_INVBAT_NUMERIC_GRID)\n            cache = self._get_normalization_cache()\n            cache[(kappa, lmbd)] = normalizer\n\n        log_norm = np.log(normalizer)\n        log_pdf = log_norm + log_kernel\n        log_pdf = np.clip(log_pdf, -745.0, 700.0)\n        pdf = np.exp(log_pdf)\n\n        step = (2.0 * np.pi) / grid_size\n        avg = 0.5 * (pdf[:-1] + pdf[1:])\n        mass = float(np.sum(avg) * step)\n        if not np.isfinite(mass) or mass &lt;= 0.0:\n            pdf = np.full_like(pdf, 1.0 / (2.0 * np.pi), dtype=float)\n            log_norm = -np.log(2.0 * np.pi)\n            mass = 1.0\n        elif abs(mass - 1.0) &gt; 5e-10:\n            scale = 1.0 / mass\n            pdf *= scale\n            log_norm += np.log(scale)\n            mass = 1.0\n            cache = self._get_normalization_cache()\n            cache[(kappa, lmbd)] = np.exp(log_norm)\n\n        avg = 0.5 * (pdf[:-1] + pdf[1:])\n        cumulative = np.concatenate(([0.0], np.cumsum(avg))) * step\n        cumulative = np.maximum.accumulate(np.clip(cumulative, 0.0, 1.0))\n        cumulative[-1] = 1.0\n\n        cdf_interp = PchipInterpolator(phi, cumulative, extrapolate=True)\n\n        unique_vals, unique_idx = np.unique(cumulative, return_index=True)\n        inv_cdf_interp = (\n            PchipInterpolator(unique_vals, phi[unique_idx], extrapolate=True)\n            if unique_vals.size &gt;= 2\n            else None\n        )\n\n        pdf_interp = PchipInterpolator(phi, pdf, extrapolate=True)\n\n        return {\n            \"phi\": phi,\n            \"pdf\": pdf,\n            \"cdf\": cumulative,\n            \"cdf_interp\": cdf_interp,\n            \"pdf_interp\": pdf_interp,\n            \"inv_cdf_interp\": inv_cdf_interp,\n            \"log_normalizer\": log_norm,\n        }\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.inverse_batschelet_gen.pdf","title":"<code>pdf(x, xi, kappa, nu, lmbd, *args, **kwargs)</code>","text":"<p>Probability density function (PDF) of the inverse Batschelet distribution.</p> <p>Let</p> <ul> <li>\\(\\varphi = ((\\theta - \\xi + \\pi) \\bmod 2\\pi) - \\pi\\),</li> <li>\\(t_\\nu^{-1}(\\varphi)\\) solve \\(y - \\nu (1 + \\cos y) = \\varphi\\),</li> <li>\\(s_\\lambda^{-1}(\\cdot)\\) solve \\(u - \\tfrac{1 + \\lambda}{2} \\sin u = \\cdot\\),</li> </ul> <p>and set \\(\\phi^\\star = t_\\nu^{-1}(\\varphi)\\), \\(u^\\star = s_\\lambda^{-1}(\\phi^\\star)\\), \\(a = \\tfrac{1 - \\lambda}{1 + \\lambda}\\), \\(b = \\tfrac{2 \\lambda}{1 + \\lambda}\\). The inverse Batschelet density is</p> \\[ f(\\theta) = c(\\kappa, \\lambda) \\exp\\bigl[\\kappa \\cos\\bigl(a\\,\\phi^\\star + b\\,u^\\star\\bigr)\\bigr], \\] <p>where \\(c(\\kappa,\\lambda)\\) is the normalising constant (independent of \\(\\xi\\) and \\(\\nu\\)). For \\(\\kappa \\rightarrow 0\\) the distribution reduces to the circular uniform density \\(1/(2\\pi)\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the PDF, defined on the interval \\([0, 2\\pi)\\).</p> required <code>xi</code> <code>float</code> <p>Direction parameter, \\(0 \\leq \\xi \\leq 2\\pi\\).</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\geq 0\\). Higher values result in sharper peaks around \\(\\xi\\).</p> required <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\leq \\nu \\leq 1\\). Controls asymmetry through angular transformation.</p> required <code>lmbd</code> <code>float</code> <p>Skewness parameter, \\(-1 \\leq \\lambda \\leq 1\\). Controls the degree of skewness in the distribution.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Values of the probability density function at the specified points.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, xi, kappa, nu, lmbd, *args, **kwargs):\n    r\"\"\"\n    Probability density function (PDF) of the inverse Batschelet distribution.\n\n    Let\n\n    - $\\varphi = ((\\theta - \\xi + \\pi) \\bmod 2\\pi) - \\pi$,\n    - $t_\\nu^{-1}(\\varphi)$ solve $y - \\nu (1 + \\cos y) = \\varphi$,\n    - $s_\\lambda^{-1}(\\cdot)$ solve $u - \\tfrac{1 + \\lambda}{2} \\sin u = \\cdot$,\n\n    and set\n    $\\phi^\\star = t_\\nu^{-1}(\\varphi)$,\n    $u^\\star = s_\\lambda^{-1}(\\phi^\\star)$,\n    $a = \\tfrac{1 - \\lambda}{1 + \\lambda}$,\n    $b = \\tfrac{2 \\lambda}{1 + \\lambda}$.\n    The inverse Batschelet density is\n\n    $$\n    f(\\theta) = c(\\kappa, \\lambda)\n    \\exp\\bigl[\\kappa \\cos\\bigl(a\\,\\phi^\\star + b\\,u^\\star\\bigr)\\bigr],\n    $$\n\n    where $c(\\kappa,\\lambda)$ is the normalising constant (independent of\n    $\\xi$ and $\\nu$). For $\\kappa \\rightarrow 0$ the distribution reduces to\n    the circular uniform density $1/(2\\pi)$.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n    xi : float\n        Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n    kappa : float\n        Concentration parameter, $\\kappa \\geq 0$. Higher values result in sharper peaks around $\\xi$.\n    nu : float\n        Shape parameter, $-1 \\leq \\nu \\leq 1$. Controls asymmetry through angular transformation.\n    lmbd : float\n        Skewness parameter, $-1 \\leq \\lambda \\leq 1$. Controls the degree of skewness in the distribution.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Values of the probability density function at the specified points.\n    \"\"\"\n    return super().pdf(x, xi, kappa, nu, lmbd, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.inverse_batschelet_gen.cdf","title":"<code>cdf(x, xi, kappa, nu, lmbd, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the inverse Batschelet distribution.</p> <p>The implementation precomputes the normalised primitive on a periodic grid in the centred angle \\(\\varphi = (\\theta - \\xi) \\bmod 2\\pi - \\pi\\). For each grid node, the inverse peakedness transform \\(t_\\nu^{-1}\\) and inverse Batschelet skew \\(s_\\lambda^{-1}\\) are evaluated, and the resulting kernel is accumulated via a trapezoidal rule. The cumulative table is cached per parameter triple \\((\\kappa, \\nu, \\lambda)\\), enabling \\(O(1)\\) queries after the initial \\(O(N)\\) precomputation. The limit \\(\\kappa \\to 0\\) reduces to the circular uniform CDF \\(\\theta / (2\\pi)\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <code>xi</code> <code>float</code> <p>Direction parameter, \\(0 \\leq \\xi \\leq 2\\pi\\).</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\geq 0\\).</p> required <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\leq \\nu \\leq 1\\).</p> required <code>lmbd</code> <code>float</code> <p>Skewness parameter, \\(-1 \\leq \\lambda \\leq 1\\).</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative probabilities corresponding to <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, xi, kappa, nu, lmbd, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the inverse Batschelet distribution.\n\n    The implementation precomputes the normalised primitive on a periodic grid\n    in the centred angle $\\varphi = (\\theta - \\xi) \\bmod 2\\pi - \\pi$. For each\n    grid node, the inverse peakedness transform $t_\\nu^{-1}$ and inverse\n    Batschelet skew $s_\\lambda^{-1}$ are evaluated, and the resulting kernel is\n    accumulated via a trapezoidal rule. The cumulative table is cached per\n    parameter triple $(\\kappa, \\nu, \\lambda)$, enabling $O(1)$ queries after the\n    initial $O(N)$ precomputation. The limit $\\kappa \\to 0$ reduces to the\n    circular uniform CDF $\\theta / (2\\pi)$.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n    xi : float\n        Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n    kappa : float\n        Concentration parameter, $\\kappa \\geq 0$.\n    nu : float\n        Shape parameter, $-1 \\leq \\nu \\leq 1$.\n    lmbd : float\n        Skewness parameter, $-1 \\leq \\lambda \\leq 1$.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative probabilities corresponding to `x`.\n    \"\"\"\n    xi_val = _invbat_ensure_scalar(xi, \"xi\")\n    kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n    nu_val = _invbat_ensure_scalar(nu, \"nu\")\n    lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n    return super().cdf(x, xi_val, kappa_val, nu_val, lmbd_val, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.inverse_batschelet_gen.ppf","title":"<code>ppf(q, xi, kappa, nu, lmbd, *args, **kwargs)</code>","text":"<p>Percent-point function (quantile) of the inverse Batschelet distribution.</p> <p>Quantiles are obtained by inverting the cached cumulative table described in <code>cdf</code>. A monotone initial guess supplied by the table inverse is refined with safeguarded Newton steps that leverage the tabulated density, while preserving a bracketing interval. For \\(\\kappa \\rightarrow 0\\), the quantile reduces to the linear uniform mapping \\(2\\pi q\\).</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>array_like</code> <p>Quantiles to evaluate (0 &lt;= q &lt;= 1).</p> required <code>xi</code> <code>float</code> <p>Direction parameter, \\(0 \\leq \\xi \\leq 2\\pi\\).</p> required <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\geq 0\\).</p> required <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\leq \\nu \\leq 1\\).</p> required <code>lmbd</code> <code>float</code> <p>Skewness parameter, \\(-1 \\leq \\lambda \\leq 1\\).</p> required <p>Returns:</p> Name Type Description <code>ppf_values</code> <code>array_like</code> <p>Angles corresponding to the probabilities in <code>q</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def ppf(self, q, xi, kappa, nu, lmbd, *args, **kwargs):\n    r\"\"\"\n    Percent-point function (quantile) of the inverse Batschelet distribution.\n\n    Quantiles are obtained by inverting the cached cumulative table described in\n    `cdf`. A monotone initial guess supplied by the table inverse is refined\n    with safeguarded Newton steps that leverage the tabulated density, while\n    preserving a bracketing interval. For $\\kappa \\rightarrow 0$, the quantile\n    reduces to the linear uniform mapping $2\\pi q$.\n\n    Parameters\n    ----------\n    q : array_like\n        Quantiles to evaluate (0 &lt;= q &lt;= 1).\n    xi : float\n        Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n    kappa : float\n        Concentration parameter, $\\kappa \\geq 0$.\n    nu : float\n        Shape parameter, $-1 \\leq \\nu \\leq 1$.\n    lmbd : float\n        Skewness parameter, $-1 \\leq \\lambda \\leq 1$.\n\n    Returns\n    -------\n    ppf_values : array_like\n        Angles corresponding to the probabilities in `q`.\n    \"\"\"\n    xi_val = _invbat_ensure_scalar(xi, \"xi\")\n    kappa_val = float(np.clip(_invbat_ensure_scalar(kappa, \"kappa\"), 0.0, _INVBAT_KAPPA_UPPER))\n    nu_val = _invbat_ensure_scalar(nu, \"nu\")\n    lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n    return super().ppf(q, xi_val, kappa_val, nu_val, lmbd_val, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.inverse_batschelet_gen.rvs","title":"<code>rvs(xi=None, kappa=None, nu=None, lmbd=None, size=None, random_state=None)</code>","text":"<p>Draw random variates from the inverse Batschelet distribution.</p> <p>Sampling proceeds by acceptance--rejection with a von Mises envelope whose concentration is matched to the curvature of the inverse Batschelet kernel at the mode. Envelope constants are calibrated on the cached spectral grid used for <code>cdf</code>, so repeated sampling calls with the same parameters are fast and stable across the entire parameter range.</p> <p>Parameters:</p> Name Type Description Default <code>xi</code> <code>float</code> <p>Direction parameter, \\(0 \\leq \\xi \\leq 2\\pi\\).</p> <code>None</code> <code>kappa</code> <code>float</code> <p>Concentration parameter, \\(\\kappa \\geq 0\\).</p> <code>None</code> <code>nu</code> <code>float</code> <p>Shape parameter, \\(-1 \\leq \\nu \\leq 1\\).</p> <code>None</code> <code>lmbd</code> <code>float</code> <p>Skewness parameter, \\(-1 \\leq \\lambda \\leq 1\\).</p> <code>None</code> <code>size</code> <code>int or tuple of ints</code> <p>Desired output shape.</p> <code>None</code> <code>random_state</code> <code>(None, int, Generator)</code> <p>Random number generator specification.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rvs</code> <code>array_like</code> <p>Random variates on \\([0, 2\\pi)\\) sampled from the inverse Batschelet distribution.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, xi=None, kappa=None, nu=None, lmbd=None, size=None, random_state=None):\n    r\"\"\"\n    Draw random variates from the inverse Batschelet distribution.\n\n    Sampling proceeds by acceptance--rejection with a von Mises envelope whose\n    concentration is matched to the curvature of the inverse Batschelet kernel at\n    the mode. Envelope constants are calibrated on the cached spectral grid used\n    for `cdf`, so repeated sampling calls with the same parameters are fast\n    and stable across the entire parameter range.\n\n    Parameters\n    ----------\n    xi : float\n        Direction parameter, $0 \\leq \\xi \\leq 2\\pi$.\n    kappa : float\n        Concentration parameter, $\\kappa \\geq 0$.\n    nu : float\n        Shape parameter, $-1 \\leq \\nu \\leq 1$.\n    lmbd : float\n        Skewness parameter, $-1 \\leq \\lambda \\leq 1$.\n    size : int or tuple of ints, optional\n        Desired output shape.\n    random_state : {None, int, np.random.Generator}, optional\n        Random number generator specification.\n\n    Returns\n    -------\n    rvs : array_like\n        Random variates on $[0, 2\\pi)$ sampled from the inverse Batschelet\n        distribution.\n    \"\"\"\n\n    xi_val = _invbat_ensure_scalar(xi, \"xi\")\n    kappa_val = _invbat_ensure_scalar(kappa, \"kappa\")\n    nu_val = _invbat_ensure_scalar(nu, \"nu\")\n    lmbd_val = _invbat_ensure_scalar(lmbd, \"lmbd\")\n    return super().rvs(xi_val, kappa_val, nu_val, lmbd_val, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.inverse_batschelet_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', optimizer='L-BFGS-B', options=None, nu_grid=None, lmbd_grid=None, kappa_bounds=(1e-06, _INVBAT_KAPPA_UPPER), nu_bounds=(-0.99, 0.99), lmbd_bounds=(-0.99, 0.99), return_info=False, **minimize_kwargs)</code>","text":"<p>Estimate \\((\\xi, \\kappa, \\nu, \\lambda)\\) from circular data.</p> <p><code>method='mle'</code> maximises the weighted log-likelihood using the cached spectral tables for the pdf and normalising constant. <code>method='moments'</code> returns the circular mean, <code>circ_kappa</code> estimate, and sets \\((\\nu, \\lambda) = (0, 0)\\).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Sample of angles.</p> required <code>weights</code> <code>array_like</code> <p>Non-negative weights broadcastable to <code>data</code>.</p> <code>None</code> <code>method</code> <code>(mle, moments)</code> <p>Estimation method.</p> <code>'mle'</code> <code>optimizer</code> <code>str</code> <p>SciPy optimiser for maximum likelihood.</p> <code>'L-BFGS-B'</code> <code>options</code> <code>dict</code> <p>Optimiser options forwarded to :func:<code>scipy.optimize.minimize</code>.</p> <code>None</code> <code>nu_grid</code> <code>array_like</code> <p>Candidate $ u$ values for profiling the starting point.</p> <code>None</code> <code>lmbd_grid</code> <code>array_like</code> <p>Candidate $ u$ values for $ lambda$ profiling.</p> <code>None</code> <code>kappa_bounds</code> <code>tuple</code> <p>Parameter bounds enforced during optimisation.</p> <code>(1e-06, _INVBAT_KAPPA_UPPER)</code> <code>nu_bounds</code> <code>tuple</code> <p>Parameter bounds enforced during optimisation.</p> <code>(1e-06, _INVBAT_KAPPA_UPPER)</code> <code>lmbd_bounds</code> <code>tuple</code> <p>Parameter bounds enforced during optimisation.</p> <code>(1e-06, _INVBAT_KAPPA_UPPER)</code> <code>return_info</code> <code>bool</code> <p>If True, also return a dictionary with optimisation diagnostics.</p> <code>False</code> <code>**minimize_kwargs</code> <p>Additional keyword arguments forwarded to :func:<code>scipy.optimize.minimize</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>params</code> <code>tuple</code> <p>Estimated parameters <code>(xi, kappa, nu, lmbd)</code>.</p> <code>info</code> <code>(dict, optional)</code> <p>Returned when <code>return_info=True</code> with optimisation diagnostics.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    optimizer=\"L-BFGS-B\",\n    options=None,\n    nu_grid=None,\n    lmbd_grid=None,\n    kappa_bounds=(1e-6, _INVBAT_KAPPA_UPPER),\n    nu_bounds=(-0.99, 0.99),\n    lmbd_bounds=(-0.99, 0.99),\n    return_info=False,\n    **minimize_kwargs,\n):\n    r\"\"\"\n    Estimate $(\\xi, \\kappa, \\nu, \\lambda)$ from circular data.\n\n    ``method='mle'`` maximises the weighted log-likelihood using the cached\n    spectral tables for the pdf and normalising constant. ``method='moments'``\n    returns the circular mean, ``circ_kappa`` estimate, and sets $(\\nu, \\lambda)\n    = (0, 0)$.\n\n    Parameters\n    ----------\n    data : array_like\n        Sample of angles.\n    weights : array_like, optional\n        Non-negative weights broadcastable to ``data``.\n    method : {'mle', 'moments'}, default 'mle'\n        Estimation method.\n    optimizer : str, optional\n        SciPy optimiser for maximum likelihood.\n    options : dict, optional\n        Optimiser options forwarded to :func:`scipy.optimize.minimize`.\n    nu_grid : array_like, optional\n        Candidate $\n        u$ values for profiling the starting point.\n    lmbd_grid : array_like, optional\n        Candidate $\n        u$ values for $\n        lambda$ profiling.\n    kappa_bounds, nu_bounds, lmbd_bounds : tuple, optional\n        Parameter bounds enforced during optimisation.\n    return_info : bool, optional\n        If True, also return a dictionary with optimisation diagnostics.\n    **minimize_kwargs :\n        Additional keyword arguments forwarded to\n        :func:`scipy.optimize.minimize`.\n\n    Returns\n    -------\n    params : tuple\n        Estimated parameters ``(xi, kappa, nu, lmbd)``.\n    info : dict, optional\n        Returned when ``return_info=True`` with optimisation diagnostics.\n    \"\"\"\n\n    minimize_kwargs = self._sanitize_fit_kwargs(minimize_kwargs)\n    minimize_kwargs.pop(\"floc\", None)\n    minimize_kwargs.pop(\"fscale\", None)\n\n    data_arr = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if data_arr.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(data_arr, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, data_arr.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0.0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = float(w_sum**2 / np.sum(w**2))\n\n    xi_mom, r1 = circ_mean_and_r(alpha=data_arr, w=w)\n    if not np.isfinite(xi_mom):\n        xi_mom = 0.0\n    xi_mom = float(np.mod(xi_mom, 2.0 * np.pi))\n    r1 = float(np.clip(r1, 1e-12, 1.0 - 1e-12))\n    n_adjust = int(max(1, round(w_sum)))\n    kappa_mom = float(np.clip(circ_kappa(r=r1, n=n_adjust), kappa_bounds[0], kappa_bounds[1]))\n\n    if method == \"moments\":\n        estimates = (xi_mom, kappa_mom, 0.0, 0.0)\n        if return_info:\n            info = {\n                \"method\": \"moments\",\n                \"converged\": True,\n                \"loglik\": float(-np.sum(w) * np.log(2.0 * np.pi)) if kappa_mom &lt;= _INVBAT_KAPPA_TOL else float(\"nan\"),\n                \"n_effective\": n_eff,\n            }\n            return estimates, info\n        return estimates\n\n    method_key = str(method).lower()\n    if method_key != \"mle\":\n        raise ValueError(\"`method` must be one of {'mle', 'moments' }.\")\n\n    two_pi = 2.0 * np.pi\n\n    if nu_grid is None:\n        nu_grid = np.linspace(nu_bounds[0], nu_bounds[1], 5)\n    else:\n        nu_grid = np.asarray(nu_grid, dtype=float)\n\n    if lmbd_grid is None:\n        lmbd_grid = np.linspace(lmbd_bounds[0], lmbd_bounds[1], 5)\n    else:\n        lmbd_grid = np.asarray(lmbd_grid, dtype=float)\n\n    def nll(params):\n        xi_param, kappa_param, nu_param, lmbd_param = params\n        if not (0.0 &lt;= xi_param &lt;= two_pi):\n            return np.inf\n        if not (kappa_bounds[0] &lt;= kappa_param &lt;= kappa_bounds[1]):\n            return np.inf\n        if not (nu_bounds[0] &lt;= nu_param &lt;= nu_bounds[1]):\n            return np.inf\n        if not (lmbd_bounds[0] &lt;= lmbd_param &lt;= lmbd_bounds[1]):\n            return np.inf\n\n        xi_wrapped = float(np.mod(xi_param, two_pi))\n        if kappa_param &lt;= _INVBAT_KAPPA_TOL:\n            log_pdf = -np.log(two_pi)\n            return float(-np.sum(w * log_pdf))\n\n        table = self._get_invbat_table(float(kappa_param), float(nu_param), float(lmbd_param))\n        phi = ((data_arr - xi_wrapped + np.pi) % two_pi) - np.pi\n        pdf_vals = table[\"pdf_interp\"](phi)\n        if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n            return np.inf\n        return float(-np.sum(w * np.log(pdf_vals)))\n\n    best_nu = 0.0\n    best_lmbd = 0.0\n    best_score = nll((xi_mom, kappa_mom, best_nu, best_lmbd))\n    for nu_candidate in np.unique(np.concatenate(([0.0], nu_grid))):\n        for lmbd_candidate in np.unique(np.concatenate(([0.0], lmbd_grid))):\n            score = nll((xi_mom, kappa_mom, float(nu_candidate), float(lmbd_candidate)))\n            if score &lt; best_score:\n                best_score = score\n                best_nu = float(nu_candidate)\n                best_lmbd = float(lmbd_candidate)\n\n    init = np.array([xi_mom, kappa_mom, best_nu, best_lmbd], dtype=float)\n    bounds = [\n        (0.0, two_pi),\n        (kappa_bounds[0], kappa_bounds[1]),\n        (nu_bounds[0], nu_bounds[1]),\n        (lmbd_bounds[0], lmbd_bounds[1]),\n    ]\n\n    options = {} if options is None else dict(options)\n\n    result = minimize(\n        nll,\n        init,\n        method=optimizer,\n        bounds=bounds,\n        options=options,\n        **minimize_kwargs,\n    )\n\n    optimizer_used = optimizer\n    if not result.success and optimizer != \"Powell\":\n        fallback = minimize(\n            nll,\n            init,\n            method=\"Powell\",\n            bounds=bounds,\n            options={},\n            **minimize_kwargs,\n        )\n        if fallback.success:\n            result = fallback\n            optimizer_used = \"Powell\"\n\n    if not result.success:\n        raise RuntimeError(f\"Maximum likelihood fit failed: {result.message}\")\n\n    xi_hat = self._wrap_direction(float(result.x[0]))\n    kappa_hat = float(np.clip(result.x[1], kappa_bounds[0], kappa_bounds[1]))\n    nu_hat = float(np.clip(result.x[2], nu_bounds[0], nu_bounds[1]))\n    lmbd_hat = float(np.clip(result.x[3], lmbd_bounds[0], lmbd_bounds[1]))\n\n    estimates = (xi_hat, kappa_hat, nu_hat, lmbd_hat)\n    if not return_info:\n        return estimates\n\n    info = {\n        \"method\": \"mle\",\n        \"loglik\": float(-result.fun),\n        \"n_effective\": n_eff,\n        \"converged\": bool(result.success),\n        \"optimizer\": optimizer_used,\n        \"nit\": getattr(result, \"nit\", np.nan),\n        \"nfev\": getattr(result, \"nfev\", np.nan),\n        \"message\": result.message,\n    }\n    return estimates, info\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapstable_gen","title":"<code>wrapstable_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Wrapped Stable Distribution</p> <p></p> <p>The wrapped stable family results from wrapping a linear stable law onto <code>[0, 2\u03c0)</code>. Its trigonometric moments satisfy</p> \\[ \\mathbb{E}\\big[e^{ip\\Theta}\\big] = \\rho_p e^{i\\mu_p}, \\qquad \\rho_p = \\exp\\left[-(\\gamma p)^\\alpha\\right], \\] <p>with</p> \\[ \\mu_p = \\begin{cases}     \\delta p + \\beta \\tan\\left(\\tfrac{\\pi\\alpha}{2}\\right)\\bigl((\\gamma p)^\\alpha - \\gamma p\\bigr), &amp; \\alpha \\ne 1, \\\\[6pt]     \\delta p + \\tfrac{2}{\\pi}\\beta\\gamma p \\log(\\gamma p), &amp; \\alpha = 1. \\end{cases} \\] <p>Special cases include the wrapped normal (<code>\u03b1=2, \u03b2=0</code>), wrapped Cauchy (<code>\u03b1=1, \u03b2=0</code>), and wrapped L\u00e9vy (<code>\u03b1=1/2, \u03b2=1</code>).</p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function via adaptive Fourier series.</p> <code>cdf</code> <p>Analytic cumulative distribution function using integrated series.</p> <code>ppf</code> <p>Quantile function obtained by safeguarded Newton refinement.</p> <code>rvs</code> <p>Random variates by Chambers\u2013Mallows\u2013Stuck sampling and wrapping.</p> <code>fit</code> <p>Estimate parameters via moment starts with optional MLE refinement.</p> References <ul> <li>Pewsey (2008). Computational Statistics &amp; Data Analysis 52(3), 1516-1523.</li> </ul> <p>Parameters must be scalar; Fourier series coefficients are cached per parameter set.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class wrapstable_gen(CircularContinuous):\n    r\"\"\"Wrapped Stable Distribution\n\n    ![wrapstable](../images/circ-mod-wrapstable.png)\n\n    The wrapped stable family results from wrapping a linear stable law onto\n    ``[0, 2\u03c0)``. Its trigonometric moments satisfy\n\n    $$\n    \\mathbb{E}\\big[e^{ip\\Theta}\\big] = \\rho_p e^{i\\mu_p}, \\qquad\n    \\rho_p = \\exp\\left[-(\\gamma p)^\\alpha\\right],\n    $$\n\n    with\n\n    $$\n    \\mu_p =\n    \\begin{cases}\n        \\delta p + \\beta \\tan\\left(\\tfrac{\\pi\\alpha}{2}\\right)\\bigl((\\gamma p)^\\alpha - \\gamma p\\bigr), &amp; \\alpha \\ne 1, \\\\[6pt]\n        \\delta p + \\tfrac{2}{\\pi}\\beta\\gamma p \\log(\\gamma p), &amp; \\alpha = 1.\n    \\end{cases}\n    $$\n\n    Special cases include the wrapped normal (``\u03b1=2, \u03b2=0``), wrapped Cauchy\n    (``\u03b1=1, \u03b2=0``), and wrapped L\u00e9vy (``\u03b1=1/2, \u03b2=1``).\n\n    Methods\n    -------\n    pdf(x, delta, alpha, beta, gamma)\n        Probability density function via adaptive Fourier series.\n\n    cdf(x, delta, alpha, beta, gamma)\n        Analytic cumulative distribution function using integrated series.\n\n    ppf(q, delta, alpha, beta, gamma)\n        Quantile function obtained by safeguarded Newton refinement.\n\n    rvs(delta, alpha, beta, gamma, size=None, random_state=None)\n        Random variates by Chambers\u2013Mallows\u2013Stuck sampling and wrapping.\n\n    fit(data, *, method='mle' | 'moments', ...)\n        Estimate parameters via moment starts with optional MLE refinement.\n\n    References\n    ----------\n    - Pewsey (2008). *Computational Statistics &amp; Data Analysis* 52(3), 1516-1523.\n\n    Parameters must be scalar; Fourier series coefficients are cached per\n    parameter set.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._series_cache = {}\n\n    def _clear_normalization_cache(self):\n        super()._clear_normalization_cache()\n        self._series_cache = {}\n\n    def _validate_params(self, delta, alpha, beta, gamma):\n        delta_arr, alpha_arr, beta_arr, gamma_arr = np.broadcast_arrays(delta, alpha, beta, gamma)\n        return (\n            (delta_arr &gt;= 0.0)\n            &amp; (delta_arr &lt;= 2.0 * np.pi)\n            &amp; (alpha_arr &gt; 0.0)\n            &amp; (alpha_arr &lt;= 2.0)\n            &amp; (beta_arr &gt; -1.0)\n            &amp; (beta_arr &lt; 1.0)\n            &amp; (gamma_arr &gt; 0.0)\n        )\n\n    def _argcheck(self, delta, alpha, beta, gamma):\n        try:\n            return self._validate_params(delta, alpha, beta, gamma)\n        except ValueError:\n            return False\n\n    def _pdf(self, x, delta, alpha, beta, gamma):\n        x_arr = np.asarray(x, dtype=float)\n        rho_vals, mu_vals, p = self._get_series_terms(delta, alpha, beta, gamma)\n        cos_args = p[:, np.newaxis] * x_arr[np.newaxis, ...] - mu_vals[:, np.newaxis]\n        series_sum = np.sum(rho_vals[:, np.newaxis] * np.cos(cos_args), axis=0)\n        pdf_values = 1 / (2 * np.pi) * (1 + 2 * series_sum)\n        if np.isscalar(x):\n            return np.asarray(pdf_values, dtype=float).reshape(-1)[0]\n        return pdf_values\n\n    def pdf(self, x, delta, alpha, beta, gamma, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Wrapped Stable distribution.\n\n        $$\n        f(\\theta) = \\frac{1}{2\\pi} \\left[1 + 2 \\sum_{p=1}^{\\infty} \\rho_p \\cos\\left(p(\\theta - \\mu_p)\\right)\\right]\n        $$\n\n        , where $\\rho_p$ is the $p$th mean resultant length and $\\mu_p$ is the $p$th mean direction:\n\n        $$\n        \\rho_p = \\exp\\left(-(\\gamma p)^\\alpha\\right)\n        $$\n\n        $$\n        \\mu_p = \n        \\begin{cases}\n            \\delta p + \\beta \\tan\\left(\\frac{\\pi \\alpha}{2}\\right) \\left((\\gamma p)^\\alpha - \\gamma p\\right), &amp; \\alpha \\neq 1 \\\\\n            \\delta p - \\beta \\frac{2}{\\pi} \\log(\\gamma p), &amp; \\text{if } \\alpha = 1\n        \\end{cases}\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n        delta : float\n            Location parameter, $0 \\leq \\delta \\leq 2\\pi$. This is the mean direction of the distribution.\n        alpha : float\n            Stability parameter, $0 &lt; \\alpha \\leq 2$. Higher values indicate heavier tails.\n        beta : float\n            Skewness parameter, $-1 &lt; \\beta &lt; 1$. Controls the asymmetry of the distribution.\n        gamma : float\n            Scale parameter, $\\gamma &gt; 0$. Scales the distribution.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Values of the probability density function at the specified points.\n        \"\"\"\n        return super().pdf(x, delta, alpha, beta, gamma, *args, **kwargs)\n\n    def _cdf(self, x, delta, alpha, beta, gamma):\n        x_arr = np.asarray(x, dtype=float)\n        scalar_input = x_arr.ndim == 0\n        theta = np.atleast_1d(x_arr)\n\n        rho_vals, mu_vals, p = self._get_series_terms(delta, alpha, beta, gamma)\n        theta_flat = theta.reshape(1, -1)\n        p_vals = p.astype(float)\n\n        sin_args = p_vals[:, np.newaxis] * theta_flat - mu_vals[:, np.newaxis]\n        coeffs = (rho_vals / p_vals)[:, np.newaxis]\n        series_sum = np.sum(coeffs * np.sin(sin_args), axis=0)\n        cdf_vals = (theta_flat[0] / (2.0 * np.pi)) + (1.0 / np.pi) * series_sum\n\n        anchor = (1.0 / np.pi) * np.sum((rho_vals / p_vals) * np.sin(-mu_vals))\n        cdf_vals = cdf_vals - anchor\n        cdf_vals = np.where(cdf_vals &lt; 0.0, cdf_vals + 1.0, cdf_vals)\n        cdf_vals = np.clip(cdf_vals, 0.0, 1.0)\n\n        # Ensure exact endpoints\n        two_pi = 2.0 * np.pi\n        cdf_vals[np.isclose(theta_flat[0], 0.0, atol=1e-12)] = 0.0\n        cdf_vals[np.isclose(theta_flat[0], two_pi, atol=1e-12)] = 1.0\n\n        if scalar_input:\n            return float(cdf_vals.reshape(-1)[0])\n        return cdf_vals.reshape(x_arr.shape)\n\n    def _ppf(self, q, delta, alpha, beta, gamma):\n        q_arr = np.asarray(q, dtype=float)\n        flat = q_arr.reshape(-1)\n        if flat.size == 0:\n            return q_arr.astype(float)\n\n        delta_val = self._scalar_param(delta)\n        alpha_val = self._scalar_param(alpha)\n        beta_val = self._scalar_param(beta)\n        gamma_val = self._scalar_param(gamma)\n\n        result = np.full_like(flat, np.nan, dtype=float)\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if not np.any(valid):\n            shaped = result.reshape(q_arr.shape)\n            return float(shaped) if q_arr.ndim == 0 else shaped\n\n        q_valid = flat[valid]\n        close_zero = np.isclose(q_valid, 0.0, atol=1e-12, rtol=0.0)\n        close_one = np.isclose(q_valid, 1.0, atol=1e-12, rtol=0.0)\n\n        two_pi = 2.0 * np.pi\n\n        theta_vals = np.empty_like(q_valid)\n        for idx, q_val in enumerate(q_valid):\n            if close_zero[idx]:\n                theta_vals[idx] = 0.0\n                continue\n            if close_one[idx]:\n                theta_vals[idx] = two_pi\n                continue\n\n            lo, hi = 0.0, two_pi\n            theta = q_val * two_pi\n\n            for _ in range(_WRAPSTABLE_NEWTON_MAXITER):\n                cdf_theta = float(self._cdf(theta, delta_val, alpha_val, beta_val, gamma_val))\n                pdf_theta = float(self._pdf(theta, delta_val, alpha_val, beta_val, gamma_val))\n                residual = cdf_theta - q_val\n\n                if abs(residual) &lt;= _WRAPSTABLE_NEWTON_TOL and (hi - lo) &lt;= _WRAPSTABLE_NEWTON_WIDTH_TOL:\n                    break\n\n                if residual &gt; 0.0:\n                    hi = min(hi, theta)\n                else:\n                    lo = max(lo, theta)\n\n                if pdf_theta &lt;= 0.0 or not np.isfinite(pdf_theta):\n                    theta = 0.5 * (lo + hi)\n                    continue\n\n                step = residual / pdf_theta\n                theta_new = theta - step\n                if not np.isfinite(theta_new) or theta_new &lt;= lo or theta_new &gt;= hi:\n                    theta = 0.5 * (lo + hi)\n                else:\n                    theta = theta_new\n\n                if (hi - lo) &lt;= _WRAPSTABLE_NEWTON_WIDTH_TOL:\n                    break\n\n            else:  # pragma: no cover - fallback to bisection if Newton fails\n                for _ in range(30):\n                    theta_mid = 0.5 * (lo + hi)\n                    cdf_mid = float(self._cdf(theta_mid, delta_val, alpha_val, beta_val, gamma_val))\n                    if cdf_mid &gt; q_val:\n                        hi = theta_mid\n                    else:\n                        lo = theta_mid\n                theta = 0.5 * (lo + hi)\n\n            theta_vals[idx] = (theta + two_pi) % two_pi\n\n        result[valid] = theta_vals\n        shaped = result.reshape(q_arr.shape)\n        if q_arr.ndim == 0:\n            return float(shaped)\n        return shaped\n\n    def _rvs(self, delta, alpha, beta, gamma, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        delta_val = self._scalar_param(delta)\n        alpha_val = self._scalar_param(alpha)\n        beta_val = self._scalar_param(beta)\n        gamma_val = self._scalar_param(gamma)\n\n        if not (0.0 &lt; alpha_val &lt;= 2.0):\n            raise ValueError(\"`alpha` must lie in (0, 2].\")\n        if not (-1.0 &lt; beta_val &lt; 1.0):\n            raise ValueError(\"`beta` must lie in (-1, 1).\")\n        if not (gamma_val &gt; 0.0):\n            raise ValueError(\"`gamma` must be positive.\")\n\n        if size is None:\n            shape = ()\n            total = 1\n        else:\n            if np.isscalar(size):\n                shape = (int(size),)\n            else:\n                shape = tuple(int(dim) for dim in np.atleast_1d(size))\n            total = int(np.prod(shape, dtype=int))\n            if total &lt; 0:\n                raise ValueError(\"`size` must describe a non-negative number of samples.\")\n\n        if total == 0:\n            empty = np.empty(shape, dtype=float)\n            return float(empty) if empty.ndim == 0 else empty\n\n        linear_samples = _wrapstable_sample_linear(\n            alpha=alpha_val,\n            beta=beta_val,\n            gamma=gamma_val,\n            delta=delta_val,\n            size=total,\n            rng=rng,\n        )\n\n        samples = np.mod(linear_samples, 2.0 * np.pi).reshape(shape)\n        if samples.ndim == 0:\n            return float(samples)\n        return samples\n\n    def rvs(self, delta=None, alpha=None, beta=None, gamma=None, size=None, random_state=None):\n        r\"\"\"Draw random variates from the wrapped stable distribution.\"\"\"\n\n        delta_val = self._scalar_param(delta)\n        alpha_val = self._scalar_param(alpha)\n        beta_val = self._scalar_param(beta)\n        gamma_val = self._scalar_param(gamma)\n        return super().rvs(delta_val, alpha_val, beta_val, gamma_val, size=size, random_state=random_state)\n\n    def fit(\n        self,\n        data,\n        *,\n        weights=None,\n        method=\"mle\",\n        optimizer=\"L-BFGS-B\",\n        options=None,\n        alpha_bounds=(1e-3, 2.0),\n        beta_bounds=(-0.99, 0.99),\n        gamma_bounds=(1e-6, 10.0),\n        return_info=False,\n        **minimize_kwargs,\n    ):\n        r\"\"\"Estimate ``(delta, alpha, beta, gamma)`` from circular data.\"\"\"\n\n        minimize_kwargs = self._sanitize_fit_kwargs(minimize_kwargs)\n        minimize_kwargs.pop(\"floc\", None)\n        minimize_kwargs.pop(\"fscale\", None)\n\n        data_arr = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if data_arr.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(data_arr, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n            w = np.broadcast_to(w, data_arr.shape).astype(float, copy=False).ravel()\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0.0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = float(w_sum**2 / np.sum(w**2))\n\n        def weighted_moment(p):\n            return np.sum(w * np.exp(1j * p * data_arr)) / w_sum\n\n        m1 = weighted_moment(1)\n        m2 = weighted_moment(2)\n\n        r1 = float(np.clip(abs(m1), 1e-9, 1 - 1e-9))\n        r2 = float(np.clip(abs(m2), 1e-9, 1 - 1e-9))\n\n        if r1 &gt;= 1 - 1e-6 or r2 &gt;= 1 - 1e-6:\n            alpha_mom = 1.0\n            gamma_mom = 1e-3\n        else:\n            y1 = float(np.log(-np.log(r1)))\n            y2 = float(np.log(-np.log(r2)))\n            slope = (y2 - y1) / np.log(2.0)\n            alpha_mom = float(np.clip(slope, alpha_bounds[0], alpha_bounds[1]))\n            gamma_mom = float(np.exp(y1 / alpha_mom))\n            gamma_mom = float(np.clip(gamma_mom, gamma_bounds[0], gamma_bounds[1]))\n\n        phi1 = float(np.angle(m1))\n        phi2_raw = float(np.angle(m2))\n        phi2 = phi2_raw + 2.0 * np.pi * round((2.0 * phi1 - phi2_raw) / (2.0 * np.pi))\n\n        if abs(alpha_mom - 1.0) &lt;= _WRAPSTABLE_ALPHA_TOL:\n            B1 = (2.0 / np.pi) * gamma_mom * np.log(gamma_mom)\n            B2 = (2.0 / np.pi) * (gamma_mom * 2.0) * np.log(gamma_mom * 2.0)\n            denom = B2 - 2.0 * B1\n            if abs(denom) &lt; 1e-8:\n                beta_mom = 0.0\n                delta_mom = phi1\n            else:\n                beta_mom = (phi2 - 2.0 * phi1) / denom\n                beta_mom = float(np.clip(beta_mom, beta_bounds[0], beta_bounds[1]))\n                delta_mom = phi1 - beta_mom * B1\n        else:\n            A = np.tan(0.5 * np.pi * alpha_mom)\n            B1 = (gamma_mom) ** alpha_mom - gamma_mom\n            B2 = (gamma_mom * 2.0) ** alpha_mom - gamma_mom * 2.0\n            denom = A * (B2 - 2.0 * B1)\n            if abs(denom) &lt; 1e-8:\n                beta_mom = 0.0\n                delta_mom = phi1\n            else:\n                beta_mom = (phi2 - 2.0 * phi1) / denom\n                beta_mom = float(np.clip(beta_mom, beta_bounds[0], beta_bounds[1]))\n                delta_mom = phi1 - beta_mom * A * B1\n\n        delta_mom = float(np.mod(delta_mom, 2.0 * np.pi))\n\n        if method == \"moments\":\n            estimates = (delta_mom, alpha_mom, beta_mom, gamma_mom)\n            if return_info:\n                info = {\n                    \"method\": \"moments\",\n                    \"converged\": True,\n                    \"n_effective\": n_eff,\n                }\n                return estimates, info\n            return estimates\n\n        method_key = str(method).lower()\n        if method_key != \"mle\":\n            raise ValueError(\"`method` must be one of {'mle', 'moments' }.\")\n\n        def nll(params):\n            delta_param, alpha_param, beta_param, gamma_param = params\n            if not (0.0 &lt;= delta_param &lt;= 2.0 * np.pi):\n                return np.inf\n            if not (alpha_bounds[0] &lt;= alpha_param &lt;= alpha_bounds[1]):\n                return np.inf\n            if not (beta_bounds[0] &lt;= beta_param &lt;= beta_bounds[1]):\n                return np.inf\n            if not (gamma_bounds[0] &lt;= gamma_param &lt;= gamma_bounds[1]):\n                return np.inf\n\n            pdf_vals = self._pdf(data_arr, delta_param, alpha_param, beta_param, gamma_param)\n            if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n                return np.inf\n            return float(-np.sum(w * np.log(pdf_vals)))\n\n        delta_candidates = np.mod(\n            np.array([delta_mom, phi1, phi2 / 2.0]), 2.0 * np.pi\n        )\n        alpha_candidates = np.clip(\n            np.array([alpha_mom, 1.0, min(1.9, alpha_mom * 1.2)]), alpha_bounds[0], alpha_bounds[1]\n        )\n        beta_candidates = np.clip(\n            np.array([beta_mom, 0.0, np.sign(beta_mom) * 0.5]), beta_bounds[0], beta_bounds[1]\n        )\n        gamma_candidates = np.clip(\n            np.array([gamma_mom, max(gamma_bounds[0], gamma_mom * 0.8), min(gamma_bounds[1], gamma_mom * 1.2)]),\n            gamma_bounds[0],\n            gamma_bounds[1],\n        )\n\n        best_params = (delta_mom, alpha_mom, beta_mom, gamma_mom)\n        best_score = nll(best_params)\n        for d0 in delta_candidates:\n            for a0 in alpha_candidates:\n                for b0 in beta_candidates:\n                    for g0 in gamma_candidates:\n                        cand = (float(d0), float(a0), float(b0), float(g0))\n                        score = nll(cand)\n                        if score &lt; best_score:\n                            best_score = score\n                            best_params = cand\n\n        bounds = [\n            (0.0, 2.0 * np.pi),\n            tuple(alpha_bounds),\n            tuple(beta_bounds),\n            tuple(gamma_bounds),\n        ]\n\n        init = np.array(best_params, dtype=float)\n        options = {} if options is None else dict(options)\n\n        optimizer_used = optimizer\n        result = minimize(\n            nll,\n            init,\n            method=optimizer,\n            bounds=bounds,\n            options=options,\n            **minimize_kwargs,\n        )\n\n        if not result.success and optimizer != \"Powell\":\n            fallback = minimize(\n                nll,\n                init,\n                method=\"Powell\",\n                bounds=bounds,\n                options={},\n                **minimize_kwargs,\n            )\n            if fallback.success:\n                result = fallback\n                optimizer_used = \"Powell\"\n\n        if not result.success:\n            raise RuntimeError(f\"Maximum likelihood fit failed: {result.message}\")\n\n        delta_hat = float(np.mod(result.x[0], 2.0 * np.pi))\n        alpha_hat = float(np.clip(result.x[1], alpha_bounds[0], alpha_bounds[1]))\n        beta_hat = float(np.clip(result.x[2], beta_bounds[0], beta_bounds[1]))\n        gamma_hat = float(np.clip(result.x[3], gamma_bounds[0], gamma_bounds[1]))\n\n        estimates = (delta_hat, alpha_hat, beta_hat, gamma_hat)\n        if not return_info:\n            return estimates\n\n        info = {\n            \"method\": \"mle\",\n            \"loglik\": float(-result.fun),\n            \"n_effective\": n_eff,\n            \"converged\": bool(result.success),\n            \"optimizer\": optimizer_used,\n            \"nit\": getattr(result, \"nit\", np.nan),\n            \"nfev\": getattr(result, \"nfev\", np.nan),\n            \"message\": result.message,\n        }\n        return estimates, info\n\n    def _get_series_terms(self, delta, alpha, beta, gamma):\n        delta_s = self._scalar_param(delta)\n        alpha_s = self._scalar_param(alpha)\n        beta_s = self._scalar_param(beta)\n        gamma_s = self._scalar_param(gamma)\n        key = self._normalization_cache_key(delta_s, alpha_s, beta_s, gamma_s)\n        if key is None:\n            return self._compute_series_terms(delta_s, alpha_s, beta_s, gamma_s)\n        cache = self._series_cache\n        if key not in cache:\n            cache[key] = self._compute_series_terms(delta_s, alpha_s, beta_s, gamma_s)\n        return cache[key]\n\n    def _compute_series_terms(self, delta, alpha, beta, gamma):\n        if gamma &lt;= 0.0:\n            raise ValueError(\"`gamma` must be positive for wrapstable.\")\n\n        def _initial_order(tol):\n            if tol &lt;= 0.0:\n                return 1\n            log_term = -np.log(tol)\n            if log_term &lt;= 0.0:\n                return 1\n            if not np.isfinite(alpha) or alpha &lt;= 0.0:\n                return 1\n\n            exponent = (np.log(log_term) / alpha) - np.log(gamma)\n            if not np.isfinite(exponent):\n                return _WRAPSTABLE_MAX_TERMS\n            if exponent &gt; np.log(_WRAPSTABLE_MAX_TERMS):\n                return _WRAPSTABLE_MAX_TERMS\n\n            value = np.exp(exponent)\n            if not np.isfinite(value):\n                return _WRAPSTABLE_MAX_TERMS\n            value = max(1.0, value)\n            return int(min(_WRAPSTABLE_MAX_TERMS, np.ceil(value)))\n\n        p_pdf = _initial_order(_WRAPSTABLE_PDF_TOL)\n        p_cdf = _initial_order(_WRAPSTABLE_CDF_TOL)\n        P = max(1, p_pdf, p_cdf)\n\n        for _ in range(_WRAPSTABLE_MAX_TERMS):\n            rho_P = np.exp(-((gamma * P) ** alpha))\n            if rho_P &lt;= _WRAPSTABLE_PDF_TOL and rho_P / P &lt;= _WRAPSTABLE_CDF_TOL:\n                break\n            P += 1\n            if P &gt;= _WRAPSTABLE_MAX_TERMS:\n                break\n\n        p = np.arange(1, P + 1, dtype=float)\n        rho_vals = np.exp(-((gamma * p) ** alpha))\n\n        if abs(alpha - 1.0) &lt;= _WRAPSTABLE_ALPHA_TOL:\n            mu_vals = delta * p + (2.0 / np.pi) * beta * gamma * p * np.log(gamma * p)\n        else:\n            mu_vals = delta * p + beta * np.tan(0.5 * np.pi * alpha) * (\n                (gamma * p) ** alpha - gamma * p\n            )\n\n        return rho_vals, mu_vals, p\n\n    @staticmethod\n    def _scalar_param(value):\n        arr = np.asarray(value, dtype=float)\n        if arr.size == 1:\n            return float(np.asarray(arr, dtype=float).reshape(-1)[0])\n        first = float(arr.flat[0])\n        if not np.allclose(arr, first):\n            raise ValueError(\n                \"wrapstable parameters must be scalar; vectorised parameters are not supported \"\n                \"because Fourier series coefficients are cached per parameter set.\"\n            )\n        return first\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapstable_gen.pdf","title":"<code>pdf(x, delta, alpha, beta, gamma, *args, **kwargs)</code>","text":"<p>Probability density function of the Wrapped Stable distribution.</p> \\[ f(\\theta) = \\frac{1}{2\\pi} \\left[1 + 2 \\sum_{p=1}^{\\infty} \\rho_p \\cos\\left(p(\\theta - \\mu_p)\\right)\\right] \\] <p>, where \\(\\rho_p\\) is the \\(p\\)th mean resultant length and \\(\\mu_p\\) is the \\(p\\)th mean direction:</p> \\[ \\rho_p = \\exp\\left(-(\\gamma p)^\\alpha\\right) \\] \\[ \\mu_p =  \\begin{cases}     \\delta p + \\beta \\tan\\left(\\frac{\\pi \\alpha}{2}\\right) \\left((\\gamma p)^\\alpha - \\gamma p\\right), &amp; \\alpha \\neq 1 \\\\     \\delta p - \\beta \\frac{2}{\\pi} \\log(\\gamma p), &amp; \\text{if } \\alpha = 1 \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the PDF, defined on the interval \\([0, 2\\pi)\\).</p> required <code>delta</code> <code>float</code> <p>Location parameter, \\(0 \\leq \\delta \\leq 2\\pi\\). This is the mean direction of the distribution.</p> required <code>alpha</code> <code>float</code> <p>Stability parameter, \\(0 &lt; \\alpha \\leq 2\\). Higher values indicate heavier tails.</p> required <code>beta</code> <code>float</code> <p>Skewness parameter, \\(-1 &lt; \\beta &lt; 1\\). Controls the asymmetry of the distribution.</p> required <code>gamma</code> <code>float</code> <p>Scale parameter, \\(\\gamma &gt; 0\\). Scales the distribution.</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Values of the probability density function at the specified points.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, delta, alpha, beta, gamma, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Wrapped Stable distribution.\n\n    $$\n    f(\\theta) = \\frac{1}{2\\pi} \\left[1 + 2 \\sum_{p=1}^{\\infty} \\rho_p \\cos\\left(p(\\theta - \\mu_p)\\right)\\right]\n    $$\n\n    , where $\\rho_p$ is the $p$th mean resultant length and $\\mu_p$ is the $p$th mean direction:\n\n    $$\n    \\rho_p = \\exp\\left(-(\\gamma p)^\\alpha\\right)\n    $$\n\n    $$\n    \\mu_p = \n    \\begin{cases}\n        \\delta p + \\beta \\tan\\left(\\frac{\\pi \\alpha}{2}\\right) \\left((\\gamma p)^\\alpha - \\gamma p\\right), &amp; \\alpha \\neq 1 \\\\\n        \\delta p - \\beta \\frac{2}{\\pi} \\log(\\gamma p), &amp; \\text{if } \\alpha = 1\n    \\end{cases}\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the PDF, defined on the interval $[0, 2\\pi)$.\n    delta : float\n        Location parameter, $0 \\leq \\delta \\leq 2\\pi$. This is the mean direction of the distribution.\n    alpha : float\n        Stability parameter, $0 &lt; \\alpha \\leq 2$. Higher values indicate heavier tails.\n    beta : float\n        Skewness parameter, $-1 &lt; \\beta &lt; 1$. Controls the asymmetry of the distribution.\n    gamma : float\n        Scale parameter, $\\gamma &gt; 0$. Scales the distribution.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Values of the probability density function at the specified points.\n    \"\"\"\n    return super().pdf(x, delta, alpha, beta, gamma, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapstable_gen.rvs","title":"<code>rvs(delta=None, alpha=None, beta=None, gamma=None, size=None, random_state=None)</code>","text":"<p>Draw random variates from the wrapped stable distribution.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def rvs(self, delta=None, alpha=None, beta=None, gamma=None, size=None, random_state=None):\n    r\"\"\"Draw random variates from the wrapped stable distribution.\"\"\"\n\n    delta_val = self._scalar_param(delta)\n    alpha_val = self._scalar_param(alpha)\n    beta_val = self._scalar_param(beta)\n    gamma_val = self._scalar_param(gamma)\n    return super().rvs(delta_val, alpha_val, beta_val, gamma_val, size=size, random_state=random_state)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.wrapstable_gen.fit","title":"<code>fit(data, *, weights=None, method='mle', optimizer='L-BFGS-B', options=None, alpha_bounds=(0.001, 2.0), beta_bounds=(-0.99, 0.99), gamma_bounds=(1e-06, 10.0), return_info=False, **minimize_kwargs)</code>","text":"<p>Estimate <code>(delta, alpha, beta, gamma)</code> from circular data.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def fit(\n    self,\n    data,\n    *,\n    weights=None,\n    method=\"mle\",\n    optimizer=\"L-BFGS-B\",\n    options=None,\n    alpha_bounds=(1e-3, 2.0),\n    beta_bounds=(-0.99, 0.99),\n    gamma_bounds=(1e-6, 10.0),\n    return_info=False,\n    **minimize_kwargs,\n):\n    r\"\"\"Estimate ``(delta, alpha, beta, gamma)`` from circular data.\"\"\"\n\n    minimize_kwargs = self._sanitize_fit_kwargs(minimize_kwargs)\n    minimize_kwargs.pop(\"floc\", None)\n    minimize_kwargs.pop(\"fscale\", None)\n\n    data_arr = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n    if data_arr.size == 0:\n        raise ValueError(\"`data` must contain at least one observation.\")\n\n    if weights is None:\n        w = np.ones_like(data_arr, dtype=float)\n    else:\n        w = np.asarray(weights, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`weights` must be non-negative.\")\n        w = np.broadcast_to(w, data_arr.shape).astype(float, copy=False).ravel()\n\n    w_sum = float(np.sum(w))\n    if not np.isfinite(w_sum) or w_sum &lt;= 0.0:\n        raise ValueError(\"Sum of weights must be positive.\")\n    n_eff = float(w_sum**2 / np.sum(w**2))\n\n    def weighted_moment(p):\n        return np.sum(w * np.exp(1j * p * data_arr)) / w_sum\n\n    m1 = weighted_moment(1)\n    m2 = weighted_moment(2)\n\n    r1 = float(np.clip(abs(m1), 1e-9, 1 - 1e-9))\n    r2 = float(np.clip(abs(m2), 1e-9, 1 - 1e-9))\n\n    if r1 &gt;= 1 - 1e-6 or r2 &gt;= 1 - 1e-6:\n        alpha_mom = 1.0\n        gamma_mom = 1e-3\n    else:\n        y1 = float(np.log(-np.log(r1)))\n        y2 = float(np.log(-np.log(r2)))\n        slope = (y2 - y1) / np.log(2.0)\n        alpha_mom = float(np.clip(slope, alpha_bounds[0], alpha_bounds[1]))\n        gamma_mom = float(np.exp(y1 / alpha_mom))\n        gamma_mom = float(np.clip(gamma_mom, gamma_bounds[0], gamma_bounds[1]))\n\n    phi1 = float(np.angle(m1))\n    phi2_raw = float(np.angle(m2))\n    phi2 = phi2_raw + 2.0 * np.pi * round((2.0 * phi1 - phi2_raw) / (2.0 * np.pi))\n\n    if abs(alpha_mom - 1.0) &lt;= _WRAPSTABLE_ALPHA_TOL:\n        B1 = (2.0 / np.pi) * gamma_mom * np.log(gamma_mom)\n        B2 = (2.0 / np.pi) * (gamma_mom * 2.0) * np.log(gamma_mom * 2.0)\n        denom = B2 - 2.0 * B1\n        if abs(denom) &lt; 1e-8:\n            beta_mom = 0.0\n            delta_mom = phi1\n        else:\n            beta_mom = (phi2 - 2.0 * phi1) / denom\n            beta_mom = float(np.clip(beta_mom, beta_bounds[0], beta_bounds[1]))\n            delta_mom = phi1 - beta_mom * B1\n    else:\n        A = np.tan(0.5 * np.pi * alpha_mom)\n        B1 = (gamma_mom) ** alpha_mom - gamma_mom\n        B2 = (gamma_mom * 2.0) ** alpha_mom - gamma_mom * 2.0\n        denom = A * (B2 - 2.0 * B1)\n        if abs(denom) &lt; 1e-8:\n            beta_mom = 0.0\n            delta_mom = phi1\n        else:\n            beta_mom = (phi2 - 2.0 * phi1) / denom\n            beta_mom = float(np.clip(beta_mom, beta_bounds[0], beta_bounds[1]))\n            delta_mom = phi1 - beta_mom * A * B1\n\n    delta_mom = float(np.mod(delta_mom, 2.0 * np.pi))\n\n    if method == \"moments\":\n        estimates = (delta_mom, alpha_mom, beta_mom, gamma_mom)\n        if return_info:\n            info = {\n                \"method\": \"moments\",\n                \"converged\": True,\n                \"n_effective\": n_eff,\n            }\n            return estimates, info\n        return estimates\n\n    method_key = str(method).lower()\n    if method_key != \"mle\":\n        raise ValueError(\"`method` must be one of {'mle', 'moments' }.\")\n\n    def nll(params):\n        delta_param, alpha_param, beta_param, gamma_param = params\n        if not (0.0 &lt;= delta_param &lt;= 2.0 * np.pi):\n            return np.inf\n        if not (alpha_bounds[0] &lt;= alpha_param &lt;= alpha_bounds[1]):\n            return np.inf\n        if not (beta_bounds[0] &lt;= beta_param &lt;= beta_bounds[1]):\n            return np.inf\n        if not (gamma_bounds[0] &lt;= gamma_param &lt;= gamma_bounds[1]):\n            return np.inf\n\n        pdf_vals = self._pdf(data_arr, delta_param, alpha_param, beta_param, gamma_param)\n        if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n            return np.inf\n        return float(-np.sum(w * np.log(pdf_vals)))\n\n    delta_candidates = np.mod(\n        np.array([delta_mom, phi1, phi2 / 2.0]), 2.0 * np.pi\n    )\n    alpha_candidates = np.clip(\n        np.array([alpha_mom, 1.0, min(1.9, alpha_mom * 1.2)]), alpha_bounds[0], alpha_bounds[1]\n    )\n    beta_candidates = np.clip(\n        np.array([beta_mom, 0.0, np.sign(beta_mom) * 0.5]), beta_bounds[0], beta_bounds[1]\n    )\n    gamma_candidates = np.clip(\n        np.array([gamma_mom, max(gamma_bounds[0], gamma_mom * 0.8), min(gamma_bounds[1], gamma_mom * 1.2)]),\n        gamma_bounds[0],\n        gamma_bounds[1],\n    )\n\n    best_params = (delta_mom, alpha_mom, beta_mom, gamma_mom)\n    best_score = nll(best_params)\n    for d0 in delta_candidates:\n        for a0 in alpha_candidates:\n            for b0 in beta_candidates:\n                for g0 in gamma_candidates:\n                    cand = (float(d0), float(a0), float(b0), float(g0))\n                    score = nll(cand)\n                    if score &lt; best_score:\n                        best_score = score\n                        best_params = cand\n\n    bounds = [\n        (0.0, 2.0 * np.pi),\n        tuple(alpha_bounds),\n        tuple(beta_bounds),\n        tuple(gamma_bounds),\n    ]\n\n    init = np.array(best_params, dtype=float)\n    options = {} if options is None else dict(options)\n\n    optimizer_used = optimizer\n    result = minimize(\n        nll,\n        init,\n        method=optimizer,\n        bounds=bounds,\n        options=options,\n        **minimize_kwargs,\n    )\n\n    if not result.success and optimizer != \"Powell\":\n        fallback = minimize(\n            nll,\n            init,\n            method=\"Powell\",\n            bounds=bounds,\n            options={},\n            **minimize_kwargs,\n        )\n        if fallback.success:\n            result = fallback\n            optimizer_used = \"Powell\"\n\n    if not result.success:\n        raise RuntimeError(f\"Maximum likelihood fit failed: {result.message}\")\n\n    delta_hat = float(np.mod(result.x[0], 2.0 * np.pi))\n    alpha_hat = float(np.clip(result.x[1], alpha_bounds[0], alpha_bounds[1]))\n    beta_hat = float(np.clip(result.x[2], beta_bounds[0], beta_bounds[1]))\n    gamma_hat = float(np.clip(result.x[3], gamma_bounds[0], gamma_bounds[1]))\n\n    estimates = (delta_hat, alpha_hat, beta_hat, gamma_hat)\n    if not return_info:\n        return estimates\n\n    info = {\n        \"method\": \"mle\",\n        \"loglik\": float(-result.fun),\n        \"n_effective\": n_eff,\n        \"converged\": bool(result.success),\n        \"optimizer\": optimizer_used,\n        \"nit\": getattr(result, \"nit\", np.nan),\n        \"nfev\": getattr(result, \"nfev\", np.nan),\n        \"message\": result.message,\n    }\n    return estimates, info\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.katojones_gen","title":"<code>katojones_gen</code>","text":"<p>               Bases: <code>CircularContinuous</code></p> <p>Kato--Jones (2015) Distribution</p> <p></p> <p>Methods:</p> Name Description <code>pdf</code> <p>Probability density function.</p> <code>cdf</code> <p>Cumulative distribution function via adaptive Fourier series.</p> <code>rvs</code> <p>Random variates obtained by inverting the CDF.</p> <code>fit</code> <p>Method-of-moments or maximum-likelihood parameter estimation.</p> Notes <p>Implements the tractable four-parameter unimodal family proposed by Kato and Jones (2015). Parameters control the first two trigonometric moments: <code>mu</code> sets the mean direction, <code>gamma</code> the mean resultant length, and <code>rho</code>/<code>lam</code> encode the magnitude/phase of the second-order moment. Feasible parameter tuples satisfy <code>0 &lt;= mu &lt; 2*pi</code>, <code>0 &lt;= gamma &lt; 1</code>, <code>0 &lt;= rho &lt; 1</code>, <code>0 &lt;= lam &lt; 2*pi</code> together with the constraint enforced in <code>_argcheck</code>.</p> <p>Special cases include the uniform distribution (<code>gamma = 0</code>), the cardioid (<code>rho = 0</code>) and the wrapped Cauchy (<code>lambda = 0</code> with <code>gamma = rho</code>).</p> References <ul> <li>Kato, S., &amp; Jones, M. C. (2015). A tractable and interpretable   four-parameter family of unimodal distributions on the circle. Biometrika,   102(1), 181-190.</li> </ul> Source code in <code>pycircstat2/distributions.py</code> <pre><code>class katojones_gen(CircularContinuous):\n    \"\"\"\n    Kato--Jones (2015) Distribution\n\n    ![katojones](../images/circ-mod-katojones.png)\n\n    Methods\n    -------\n    pdf(x, mu, gamma, rho, lam)\n        Probability density function.\n    cdf(x, mu, gamma, rho, lam)\n        Cumulative distribution function via adaptive Fourier series.\n\n    rvs(mu, gamma, rho, lam, size=None, random_state=None)\n        Random variates obtained by inverting the CDF.\n    fit(data, method=\\\"moments\\\" | \\\"mle\\\", ...)\n        Method-of-moments or maximum-likelihood parameter estimation.\n    Notes\n    -----\n    Implements the tractable four-parameter unimodal family proposed by Kato and\n    Jones (2015). Parameters control the first two trigonometric moments:\n    ``mu`` sets the mean direction, ``gamma`` the mean resultant length, and\n    ``rho``/``lam`` encode the magnitude/phase of the second-order moment.\n    Feasible parameter tuples satisfy ``0 &lt;= mu &lt; 2*pi``, ``0 &lt;= gamma &lt; 1``,\n    ``0 &lt;= rho &lt; 1``, ``0 &lt;= lam &lt; 2*pi`` together with the constraint enforced\n    in `_argcheck`.\n\n    Special cases include the uniform distribution (``gamma = 0``), the cardioid\n    (``rho = 0``) and the wrapped Cauchy (``lambda = 0`` with ``gamma = rho``).\n\n    References\n    ----------\n    - Kato, S., &amp; Jones, M. C. (2015). *A tractable and interpretable\n      four-parameter family of unimodal distributions on the circle*. Biometrika,\n      102(1), 181-190.\n    \"\"\"\n\n    _moment_tolerance = 1e-12\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._series_cache = {}\n\n    def _clear_normalization_cache(self):\n        super()._clear_normalization_cache()\n        self._series_cache = {}\n\n    @staticmethod\n    def _scalar_param(value):\n        arr = np.asarray(value, dtype=float)\n        if arr.size == 1:\n            return float(np.asarray(arr, dtype=float).reshape(-1)[0])\n        first = float(arr.flat[0])\n        if not np.allclose(arr, first):\n            raise ValueError(\n                \"katojones parameters must be scalar; vectorised parameters are not supported \"\n                \"because series expansions are cached per parameter set.\"\n            )\n        return first\n\n    def _argcheck(self, mu, gamma, rho, lam):\n        try:\n            mu_arr, gamma_arr, rho_arr, lam_arr = np.broadcast_arrays(mu, gamma, rho, lam)\n        except ValueError:\n            return False\n\n        base = (\n            (mu_arr &gt;= 0.0)\n            &amp; (mu_arr &lt; 2.0 * np.pi)\n            &amp; (gamma_arr &gt;= 0.0)\n            &amp; (gamma_arr &lt; 1.0)\n            &amp; (rho_arr &gt;= 0.0)\n            &amp; (rho_arr &lt; 1.0)\n            &amp; (lam_arr &gt;= 0.0)\n            &amp; (lam_arr &lt; 2.0 * np.pi)\n        )\n\n        cos_lam = np.cos(lam_arr)\n        sin_lam = np.sin(lam_arr)\n        constraint_val = (rho_arr * cos_lam - gamma_arr) ** 2 + (rho_arr * sin_lam) ** 2\n        constraint_limit = (1.0 - gamma_arr) ** 2 + 1e-12\n        admissible = constraint_val &lt;= constraint_limit\n        return base &amp; admissible\n\n    def _pdf(self, x, mu, gamma, rho, lam):\n        x_arr = np.asarray(x, dtype=float)\n        delta = x_arr - mu\n        denom = 1.0 + rho**2 - 2.0 * rho * np.cos(delta - lam)\n        denom = np.clip(denom, 1e-15, None)\n        numerator = 1.0 + (2.0 * gamma * (np.cos(delta) - rho * np.cos(lam))) / denom\n        pdf = numerator / (2.0 * np.pi)\n        pdf = np.clip(pdf, 0.0, None)\n        if np.isscalar(x):\n            return np.asarray(pdf, dtype=float).reshape(-1)[0]\n        return pdf\n\n    def pdf(self, x, mu, gamma, rho, lam, *args, **kwargs):\n        r\"\"\"\n        Probability density function of the Kato--Jones (2015) distribution.\n\n        $$\n        g(\\theta) = \\frac{1}{2\\pi}\\left[1 + \\frac{2\\gamma\\,(\\cos(\\theta-\\mu) - \\rho\\cos\\lambda)}\n        {1 + \\rho^2 - 2\\rho\\cos(\\theta-\\mu-\\lambda)}\\right]\n        $$\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the probability density function.\n        mu : float\n            Mean direction, $0 \\leq \\mu &lt; 2\\pi$.\n        gamma : float\n            Mean resultant length, $0 \\leq \\gamma &lt; 1$.\n        rho : float\n            Second-order magnitude, $0 \\leq \\rho &lt; 1$.\n        lam : float\n            Second-order phase, $0 \\leq \\lambda &lt; 2\\pi$.\n\n        Returns\n        -------\n        pdf_values : array_like\n            Probability density function evaluated at `x`.\n        \"\"\"\n        return super().pdf(x, mu, gamma, rho, lam, *args, **kwargs)\n\n    def _cdf(self, x, mu, gamma, rho, lam):\n        x_arr = np.asarray(x, dtype=float)\n        scalar_input = x_arr.ndim == 0\n        flat = x_arr.reshape(-1)\n\n        mu_val = float(np.mod(self._scalar_param(mu), 2.0 * np.pi))\n        gamma_val = float(np.clip(self._scalar_param(gamma), 0.0, 1.0 - 1e-12))\n        rho_val = float(np.clip(self._scalar_param(rho), 0.0, 1.0 - 1e-12))\n        lam_val = float(np.mod(self._scalar_param(lam), 2.0 * np.pi))\n\n        if gamma_val &lt;= _KJ_GAMMA_TOL:\n            cdf_flat = flat / (2.0 * np.pi)\n        else:\n            series = self._get_series_terms(mu_val, gamma_val, rho_val, lam_val)\n            cdf_raw = self._evaluate_cdf_series(flat, mu_val, gamma_val, rho_val, lam_val, series=series)\n            cdf_flat = np.mod(cdf_raw, 1.0)\n\n        cdf_flat = np.clip(cdf_flat, 0.0, 1.0)\n        cdf_flat[np.isclose(flat, 0.0, atol=1e-12)] = 0.0\n        cdf_flat[np.isclose(flat, 2.0 * np.pi, atol=1e-12)] = 1.0\n\n        if scalar_input:\n            return float(cdf_flat[0])\n        return cdf_flat.reshape(x_arr.shape)\n\n    def cdf(self, x, mu, gamma, rho, lam, *args, **kwargs):\n        r\"\"\"\n        Cumulative distribution function of the Kato--Jones (2015) distribution.\n\n        The CDF has the closed-form Fourier expansion\n\n        $$\n        G(\\theta) = \\frac{\\theta}{2\\pi}\n        + \\frac{1}{\\pi}\\sum_{p=1}^{\\infty} \\frac{\\gamma \\rho^{p-1}}{p}\n        \\sin\\!\\bigl(p\\theta - [p\\mu + (p-1)\\lambda]\\bigr),\n        $$\n\n        which is evaluated adaptively by truncating the series once the tail\n        contribution drops below a specified tolerance. No numerical quadrature\n        is required.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the cumulative distribution function.\n        mu : float\n            Mean direction, $0 \\leq \\mu &lt; 2\\pi$.\n        gamma : float\n            Mean resultant length, $0 \\leq \\gamma &lt; 1$.\n        rho : float\n            Second-order magnitude, $0 \\leq \\rho &lt; 1$.\n        lam : float\n            Second-order phase, $0 \\leq \\lambda &lt; 2\\pi$.\n\n        Returns\n        -------\n        cdf_values : array_like\n            Cumulative distribution function evaluated at `x`.\n        \"\"\"\n        return super().cdf(x, mu, gamma, rho, lam, *args, **kwargs)\n\n    def _logpdf(self, x, mu, gamma, rho, lam):\n        pdf_vals = self._pdf(x, mu, gamma, rho, lam)\n        return np.log(np.clip(pdf_vals, np.finfo(float).tiny, None))\n\n    def logpdf(self, x, mu, gamma, rho, lam, *args, **kwargs):\n        r\"\"\"\n        Logarithm of the probability density function of the Kato--Jones (2015)\n        distribution.\n\n        Parameters\n        ----------\n        x : array_like\n            Points at which to evaluate the log-PDF.\n        mu : float\n            Mean direction, $0 \\leq \\mu &lt; 2\\pi$.\n        gamma : float\n            Mean resultant length, $0 \\leq \\gamma &lt; 1$.\n        rho : float\n            Second-order magnitude, $0 \\leq \\rho &lt; 1$.\n        lam : float\n            Second-order phase, $0 \\leq \\lambda &lt; 2\\pi$.\n\n        Returns\n        -------\n        logpdf_values : array_like\n            Logarithm of the probability density function evaluated at `x`.\n        \"\"\"\n        return super().logpdf(x, mu, gamma, rho, lam, *args, **kwargs)\n\n    def _ppf(self, q, mu, gamma, rho, lam):\n        mu_val = float(np.mod(self._scalar_param(mu), 2.0 * np.pi))\n        gamma_val = float(np.clip(self._scalar_param(gamma), 0.0, 1.0 - 1e-12))\n        rho_val = float(np.clip(self._scalar_param(rho), 0.0, 1.0 - 1e-12))\n        lam_val = float(np.mod(self._scalar_param(lam), 2.0 * np.pi))\n\n        q_arr = np.asarray(q, dtype=float)\n        if q_arr.size == 0:\n            return q_arr.astype(float)\n\n        if gamma_val &lt;= _KJ_GAMMA_TOL:\n            return (2.0 * np.pi * q_arr).astype(float)\n\n        scalar_input = q_arr.ndim == 0\n        flat = q_arr.reshape(-1)\n        result = np.full_like(flat, np.nan, dtype=float)\n\n        valid = np.isfinite(flat) &amp; (flat &gt;= 0.0) &amp; (flat &lt;= 1.0)\n        if not np.any(valid):\n            return float(result) if scalar_input else result.reshape(q_arr.shape)\n\n        series = self._get_series_terms(mu_val, gamma_val, rho_val, lam_val)\n        two_pi = 2.0 * np.pi\n\n        def cdf_single(theta):\n            value = self._evaluate_cdf_series(theta, mu_val, gamma_val, rho_val, lam_val, series=series)\n            value = np.mod(value, 1.0)\n            return float(np.clip(value, 0.0, 1.0))\n\n        for idx, q_val in enumerate(flat):\n            if not valid[idx]:\n                continue\n            if np.isclose(q_val, 0.0, atol=1e-12):\n                result[idx] = 0.0\n                continue\n            if np.isclose(q_val, 1.0, atol=1e-12):\n                result[idx] = two_pi\n                continue\n\n            lo, hi = 0.0, two_pi\n            theta = q_val * two_pi\n\n            for _ in range(_KJ_NEWTON_MAXITER):\n                cdf_theta = cdf_single(theta)\n                pdf_theta = float(self._pdf(theta, mu_val, gamma_val, rho_val, lam_val))\n                residual = cdf_theta - q_val\n\n                if abs(residual) &lt;= _KJ_NEWTON_TOL and (hi - lo) &lt;= _KJ_NEWTON_WIDTH_TOL:\n                    break\n\n                if residual &gt; 0.0:\n                    hi = min(hi, theta)\n                else:\n                    lo = max(lo, theta)\n\n                if pdf_theta &lt;= 0.0 or not np.isfinite(pdf_theta):\n                    theta = 0.5 * (lo + hi)\n                    continue\n\n                step = residual / pdf_theta\n                theta_candidate = theta - step\n                if not np.isfinite(theta_candidate) or theta_candidate &lt;= lo or theta_candidate &gt;= hi:\n                    theta = 0.5 * (lo + hi)\n                else:\n                    theta = theta_candidate\n\n                if (hi - lo) &lt;= _KJ_NEWTON_WIDTH_TOL:\n                    break\n            else:\n                for _ in range(30):\n                    mid = 0.5 * (lo + hi)\n                    if cdf_single(mid) &gt; q_val:\n                        hi = mid\n                    else:\n                        lo = mid\n                theta = 0.5 * (lo + hi)\n\n            result[idx] = theta % two_pi\n\n        if scalar_input:\n            return float(result[0])\n        return result.reshape(q_arr.shape)\n\n    def _rvs(self, mu, gamma, rho, lam, size=None, random_state=None):\n        rng = self._init_rng(random_state)\n\n        if size is None:\n            u = rng.random()\n            return float(self._ppf(u, mu, gamma, rho, lam))\n\n        if np.isscalar(size):\n            shape = (int(size),)\n        else:\n            shape = tuple(int(dim) for dim in np.atleast_1d(size))\n\n        total = int(np.prod(shape, dtype=int))\n        if total &lt; 0:\n            raise ValueError(\"`size` must describe a non-negative number of samples.\")\n        if total == 0:\n            return np.empty(shape, dtype=float)\n\n        u = rng.random(size=shape)\n        return self._ppf(u, mu, gamma, rho, lam)\n\n    def rvs(self, mu=None, gamma=None, rho=None, lam=None, size=None, random_state=None):\n        mu_val = self._scalar_param(mu)\n        gamma_val = self._scalar_param(gamma)\n        rho_val = self._scalar_param(rho)\n        lam_val = self._scalar_param(lam)\n        return super().rvs(mu_val, gamma_val, rho_val, lam_val, size=size, random_state=random_state)\n\n    def trig_moment(self, p: int = 1, *args, **kwargs) -&gt; complex:\n        shape_args, non_shape_kwargs = self._separate_shape_parameters(\n            args, kwargs, \"trig_moment\"\n        )\n        call_kwargs = self._prepare_call_kwargs(non_shape_kwargs, \"trig_moment\")\n        params = self._parse_args(*shape_args, **call_kwargs)[0]\n        if len(params) != 4:\n            raise ValueError(\"Expected parameters (mu, gamma, rho, lam).\")\n        mu, gamma, rho, lam = [float(np.asarray(val, dtype=float)) for val in params]\n\n        if not np.isscalar(p):\n            raise ValueError(\"`p` must be an integer scalar.\")\n        if int(round(p)) != p:\n            raise ValueError(\"`p` must be an integer.\")\n\n        k = int(round(p))\n        if k == 0:\n            return complex(1.0, 0.0)\n\n        abs_k = abs(k)\n        mag = float(gamma) if abs_k == 1 else float(gamma * (rho ** (abs_k - 1)))\n        angle = abs_k * mu + (abs_k - 1) * lam\n        value = mag * np.exp(1j * angle)\n\n        if k &lt; 0:\n            return np.conjugate(value)\n        return complex(value)\n\n    def _prepare_data_weights(self, data, weights=None):\n        data_arr = self._wrap_angles(np.asarray(data, dtype=float)).ravel()\n        if data_arr.size == 0:\n            raise ValueError(\"`data` must contain at least one observation.\")\n\n        if weights is None:\n            w = np.ones_like(data_arr, dtype=float)\n        else:\n            w = np.asarray(weights, dtype=float)\n            try:\n                w = np.broadcast_to(w, data_arr.shape).astype(float, copy=False).ravel()\n            except ValueError as exc:\n                raise ValueError(\"`weights` must be broadcastable to the data shape.\") from exc\n            if np.any(w &lt; 0):\n                raise ValueError(\"`weights` must be non-negative.\")\n\n        w_sum = float(np.sum(w))\n        if not np.isfinite(w_sum) or w_sum &lt;= 0.0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        n_eff = float(w_sum**2 / np.sum(w**2))\n        return data_arr, w, w_sum, n_eff\n\n    def _fit_moments(self, data, *, weights=None, return_info=False):\n        data_arr, w, w_sum, n_eff = self._prepare_data_weights(data, weights=weights)\n\n        mu_hat, r1 = circ_mean_and_r(alpha=data_arr, w=w)\n        centered = angmod(data_arr - mu_hat)\n        cos2 = np.cos(2.0 * centered)\n        sin2 = np.sin(2.0 * centered)\n        alpha2 = float(np.sum(w * cos2) / w_sum)\n        beta2 = float(np.sum(w * sin2) / w_sum)\n        mu_hat = self._wrap_direction(float(mu_hat))\n        gamma_hat = float(np.clip(r1, 0.0, 1.0 - 1e-9))\n\n        alpha2_proj, beta2_proj = self._project_second_order(gamma_hat, alpha2, beta2)\n\n        if gamma_hat &lt; self._moment_tolerance:\n            rho_hat = 0.0\n            lam_hat = 0.0\n        else:\n            r2 = np.hypot(alpha2_proj, beta2_proj)\n            rho_hat = float(np.clip(r2 / max(gamma_hat, 1e-12), 0.0, 1.0 - 1e-9))\n            lam_hat = float(np.mod(np.arctan2(beta2_proj, alpha2_proj), 2.0 * np.pi))\n            if rho_hat &lt; self._moment_tolerance:\n                lam_hat = 0.0\n\n        estimates = (mu_hat, gamma_hat, rho_hat, lam_hat)\n        if return_info:\n            info = {\n                \"method\": \"moments\",\n                \"converged\": True,\n                \"n_effective\": n_eff,\n            }\n            return estimates, info\n        return estimates\n\n    @staticmethod\n    def _project_second_order(gamma, alpha2, beta2):\n        gamma = float(gamma)\n        radius = gamma * (1.0 - gamma)\n        center_alpha = gamma * gamma\n        vec_alpha = alpha2 - center_alpha\n        vec_beta = beta2\n        distance = np.hypot(vec_alpha, vec_beta)\n        if radius &lt;= 0.0:\n            return center_alpha, 0.0\n        if distance &lt;= radius:\n            return alpha2, beta2\n        if distance == 0.0:\n            return center_alpha + radius, 0.0\n        scale = radius / distance\n        alpha_proj = center_alpha + vec_alpha * scale\n        beta_proj = vec_beta * scale\n        return alpha_proj, beta_proj\n\n    @staticmethod\n    def convert_alpha2_beta2(gamma, alpha2, beta2, *, verify=True):\n        \"\"\"\n        Convert second-order moment parameters to (rho, lambda).\n\n        Parameters\n        ----------\n        gamma : float\n            Mean resultant length, 0 &lt;= gamma &lt; 1.\n        alpha2 : float\n            Second-order cosine moment around mu.\n        beta2 : float\n            Second-order sine moment around mu.\n        verify : bool, optional\n            If True (default), check that (alpha2, beta2) lies within the feasible\n            disk for the supplied gamma and raise a ValueError if not.\n\n        Returns\n        -------\n        rho : float\n            Second-order magnitude parameter.\n        lam : float\n            Second-order phase parameter in [0, 2 pi).\n        \"\"\"\n        gamma = float(gamma)\n        alpha2 = float(alpha2)\n        beta2 = float(beta2)\n\n        if not (0.0 &lt;= gamma &lt; 1.0):\n            raise ValueError(\"`gamma` must lie in [0, 1).\")\n\n        radius_sq = (gamma * (1.0 - gamma)) ** 2\n        center_alpha = gamma * gamma\n        dist_sq = (alpha2 - center_alpha) ** 2 + beta2**2\n\n        tol = 1e-12\n        if verify and dist_sq &gt; radius_sq + tol:\n            raise ValueError(\n                f\"(alpha2, beta2) = ({alpha2}, {beta2}) is outside the feasible disk \"\n                f\"for gamma={gamma}.\"\n            )\n\n        r2 = np.hypot(alpha2, beta2)\n        if gamma &lt;= katojones_gen._moment_tolerance:\n            if verify and r2 &gt; tol:\n                raise ValueError(\n                    \"When gamma is approximately zero, alpha2 and beta2 must also be near zero.\"\n                )\n            return 0.0, 0.0\n\n        rho = float(np.clip(r2 / gamma, 0.0, 1.0 - 1e-12))\n        if r2 &lt;= tol:\n            lam = 0.0\n        else:\n            lam = float(np.mod(np.arctan2(beta2, alpha2), 2.0 * np.pi))\n        return rho, lam\n\n    @staticmethod\n    def convert_rho_lambda(gamma, rho, lam, *, verify=True):\n        \"\"\"\n        Convert (rho, lambda) parameters to second-order moments (alpha2, beta2).\n\n        Parameters\n        ----------\n        gamma : float\n            Mean resultant length, 0 &lt;= gamma &lt; 1.\n        rho : float\n            Second-order magnitude, 0 &lt;= rho &lt; 1.\n        lam : float\n            Second-order phase, 0 &lt;= lam &lt; 2*pi.\n        verify : bool, optional\n            If True (default), ensure (gamma, rho, lam) satisfies the feasibility\n            constraint and raise a ValueError otherwise.\n\n        Returns\n        -------\n        alpha2 : float\n            Second-order cosine moment around mu.\n        beta2 : float\n            Second-order sine moment around mu.\n        \"\"\"\n        gamma = float(gamma)\n        rho = float(rho)\n        lam = float(lam)\n\n        if not (0.0 &lt;= gamma &lt; 1.0):\n            raise ValueError(\"`gamma` must lie in [0, 1).\")\n        if not (0.0 &lt;= rho &lt; 1.0):\n            raise ValueError(\"`rho` must lie in [0, 1).\")\n\n        if verify:\n            constraint = (rho * np.cos(lam) - gamma) ** 2 + (rho * np.sin(lam)) ** 2\n            if constraint &gt; (1.0 - gamma) ** 2 + 1e-12:\n                raise ValueError(\n                    f\"(gamma, rho, lam)=({gamma}, {rho}, {lam}) violates the feasibility constraint.\"\n                )\n\n        alpha2 = float(gamma * rho * np.cos(lam))\n        beta2 = float(gamma * rho * np.sin(lam))\n        return alpha2, beta2\n\n    @staticmethod\n    def _aux_from_rho_lam(gamma, rho, lam):\n        gamma = float(gamma)\n        rho = float(rho)\n        lam = float(lam)\n        gamma = np.clip(gamma, 0.0, 1.0 - 1e-12)\n        rho = np.clip(rho, 0.0, 1.0 - 1e-12)\n        lam = float(np.mod(lam, 2.0 * np.pi))\n\n        if gamma &gt;= 1.0 - 1e-12:\n            return 0.0, 0.0\n\n        denom = max(1e-12, 1.0 - gamma)\n        delta_cos = rho * np.cos(lam) - gamma\n        delta_sin = rho * np.sin(lam)\n        s = float(np.clip(np.hypot(delta_cos, delta_sin) / denom, 0.0, 1.0 - 1e-9))\n        phi = float(np.mod(np.arctan2(delta_sin, delta_cos), 2.0 * np.pi))\n        if s &lt; katojones_gen._moment_tolerance:\n            phi = 0.0\n        return s, phi\n\n    @staticmethod\n    def _rho_lam_from_aux(gamma, s, phi):\n        gamma = float(np.clip(gamma, 0.0, 1.0 - 1e-9))\n        s = float(np.clip(s, 0.0, 1.0 - 1e-9))\n        phi = float(np.mod(phi, 2.0 * np.pi))\n\n        cos_phi = np.cos(phi)\n        sin_phi = np.sin(phi)\n        delta_cos = (1.0 - gamma) * s * cos_phi\n        delta_sin = (1.0 - gamma) * s * sin_phi\n        rho_cos = gamma + delta_cos\n        rho_sin = delta_sin\n        rho = float(np.clip(np.hypot(rho_cos, rho_sin), 0.0, 1.0 - 1e-9))\n        lam = float(np.mod(np.arctan2(rho_sin, rho_cos), 2.0 * np.pi))\n        return rho, lam\n\n    def _get_series_terms(self, mu, gamma, rho, lam):\n        mu_val = float(np.mod(self._scalar_param(mu), 2.0 * np.pi))\n        gamma_val = float(np.clip(self._scalar_param(gamma), 0.0, 1.0 - 1e-12))\n        rho_val = float(np.clip(self._scalar_param(rho), 0.0, 1.0 - 1e-12))\n        lam_val = float(np.mod(self._scalar_param(lam), 2.0 * np.pi))\n\n        key = self._normalization_cache_key(mu_val, gamma_val, rho_val, lam_val)\n        if key is None:\n            return self._compute_series_terms(mu_val, gamma_val, rho_val, lam_val)\n\n        cache = self._series_cache\n        if key not in cache:\n            cache[key] = self._compute_series_terms(mu_val, gamma_val, rho_val, lam_val)\n        return cache[key]\n\n    def _compute_series_terms(self, mu, gamma, rho, lam):\n        if gamma &lt;= _KJ_GAMMA_TOL or rho &lt;= _KJ_GAMMA_TOL:\n            return {\n                \"coeffs\": np.empty(0, dtype=float),\n                \"phases\": np.empty(0, dtype=float),\n                \"p\": np.empty(0, dtype=float),\n                \"anchor\": 0.0,\n            }\n\n        rho_val = float(np.clip(rho, 0.0, 1.0 - 1e-12))\n        gamma_val = float(np.clip(gamma, 0.0, 1.0 - 1e-12))\n\n        if rho_val == 0.0:\n            P = 1\n        else:\n            P = 1\n            for _ in range(_KJ_MAX_TERMS):\n                tail = (gamma_val / max(P, 1)) * (rho_val ** max(P - 1, 0)) / max(1e-12, 1.0 - rho_val)\n                if tail &lt;= _KJ_CDF_TOL:\n                    break\n                P += 1\n            P = min(P, _KJ_MAX_TERMS)\n\n        p = np.arange(1, P + 1, dtype=float)\n        rho_pows = rho_val ** (p - 1.0)\n        coeffs = gamma_val * rho_pows / p\n        phases = np.mod(p * mu + (p - 1.0) * lam, 2.0 * np.pi)\n        anchor = -(1.0 / np.pi) * np.sum(coeffs * np.sin(phases))\n\n        return {\n            \"coeffs\": coeffs,\n            \"phases\": phases,\n            \"p\": p,\n            \"anchor\": float(anchor),\n        }\n\n    def _evaluate_cdf_series(self, theta, mu, gamma, rho, lam, *, series=None):\n        theta_arr = np.asarray(theta, dtype=float)\n        flat = theta_arr.reshape(-1)\n\n        if series is None:\n            series = self._get_series_terms(mu, gamma, rho, lam)\n\n        coeffs = series[\"coeffs\"]\n        if coeffs.size == 0:\n            return (flat / (2.0 * np.pi)).reshape(theta_arr.shape)\n\n        phases = series[\"phases\"]\n        p = series[\"p\"][:, np.newaxis]\n        theta_col = flat[np.newaxis, :]\n        sin_terms = np.sin(p * theta_col - phases[:, np.newaxis])\n        series_sum = np.sum(coeffs[:, np.newaxis] * sin_terms, axis=0)\n        base = flat / (2.0 * np.pi)\n        values = base + (1.0 / np.pi) * series_sum - series[\"anchor\"]\n        return values.reshape(theta_arr.shape)\n\n    def _fit_mle(\n        self,\n        data,\n        *,\n        weights=None,\n        initial,\n        optimizer,\n        options,\n        return_info=False,\n        **minimize_kwargs,\n    ):\n        data_arr, w, w_sum, n_eff = self._prepare_data_weights(data, weights=weights)\n\n        if initial is None:\n            initial = self._fit_moments(data_arr, weights=w)\n\n        mu0, gamma0, rho0, lam0 = initial\n        mu0 = self._wrap_direction(float(mu0))\n        gamma0 = float(np.clip(gamma0, 1e-6, 1.0 - 1e-6))\n        lam0 = float(np.mod(lam0, 2.0 * np.pi))\n        rho0 = float(np.clip(rho0, 0.0, 1.0 - 1e-6))\n        if rho0 &lt; self._moment_tolerance:\n            rho0 = 0.0\n            lam0 = 0.0\n        s0, phi0 = self._aux_from_rho_lam(gamma0, rho0, lam0)\n        x0 = np.array([mu0, gamma0, s0, phi0], dtype=float)\n\n        def objective(params):\n            mu, gamma, s, phi = params\n            mu = self._wrap_direction(float(mu))\n            gamma = float(np.clip(gamma, 1e-6, 1.0 - 1e-9))\n            s = float(np.clip(s, 0.0, 1.0 - 1e-9))\n            phi = float(np.mod(phi, 2.0 * np.pi))\n            rho, lam = self._rho_lam_from_aux(gamma, s, phi)\n            if not self._argcheck(mu, gamma, rho, lam):\n                return 1e12\n            pdf_vals = self._pdf(data_arr, mu, gamma, rho, lam)\n            if np.any(pdf_vals &lt;= 0.0) or not np.all(np.isfinite(pdf_vals)):\n                return 1e12\n            return -np.sum(w * np.log(pdf_vals))\n\n        bounds = [\n            (0.0, 2.0 * np.pi),\n            (1e-6, 1.0 - 1e-6),\n            (0.0, 1.0 - 1e-6),\n            (0.0, 2.0 * np.pi),\n        ]\n\n        result = minimize(\n            objective,\n            x0,\n            method=optimizer,\n            bounds=bounds,\n            options=options,\n            **minimize_kwargs,\n        )\n\n        if not result.success:\n            fallback_method = \"Powell\" if optimizer != \"Powell\" else None\n            if fallback_method is not None:\n                fallback_result = minimize(\n                    objective,\n                    x0,\n                    method=fallback_method,\n                    bounds=bounds,\n                    options={},\n                    **minimize_kwargs,\n                )\n                if fallback_result.success:\n                    result = fallback_result\n            if not result.success:\n                raise RuntimeError(f\"Maximum likelihood fit failed: {result.message}\")\n\n        mu_hat, gamma_hat, s_hat, phi_hat = result.x\n        mu_hat = self._wrap_direction(float(mu_hat))\n        gamma_hat = float(np.clip(gamma_hat, 0.0, 1.0 - 1e-9))\n        s_hat = float(np.clip(s_hat, 0.0, 1.0 - 1e-9))\n        phi_hat = float(np.mod(phi_hat, 2.0 * np.pi))\n        rho_hat, lam_hat = self._rho_lam_from_aux(gamma_hat, s_hat, phi_hat)\n\n        estimates = (mu_hat, gamma_hat, rho_hat, lam_hat)\n        if return_info:\n            final_nll = objective(result.x)\n            info = {\n                \"method\": \"mle\",\n                \"converged\": bool(result.success),\n                \"loglik\": float(-final_nll),\n                \"n_effective\": n_eff,\n                \"nit\": getattr(result, \"nit\", None),\n                \"optimizer\": optimizer,\n                \"initial\": initial,\n            }\n            return estimates, info\n        return estimates\n\n    def fit(\n        self,\n        data,\n        method=\"moments\",\n        *,\n        weights=None,\n        initial=None,\n        optimizer=\"L-BFGS-B\",\n        options=None,\n        return_info=False,\n        **kwargs,\n    ):\n        kwargs = self._clean_loc_scale_kwargs(kwargs, caller=\"fit\")\n        kwargs.pop(\"floc\", None)\n        kwargs.pop(\"fscale\", None)\n\n        if method == \"moments\":\n            if kwargs:\n                raise TypeError(\"Unexpected optimizer arguments for method='moments'.\")\n            estimates, info = self._fit_moments(\n                data,\n                weights=weights,\n                return_info=True,\n            )\n            return (estimates, info) if return_info else estimates\n\n        if method != \"mle\":\n            raise ValueError(\"method must be either 'moments' or 'mle'.\")\n\n        options = {} if options is None else dict(options)\n        estimates, info = self._fit_mle(\n            data,\n            weights=weights,\n            initial=initial,\n            optimizer=optimizer,\n            options=options,\n            return_info=True,\n            **kwargs,\n        )\n        return (estimates, info) if return_info else estimates\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.katojones_gen.pdf","title":"<code>pdf(x, mu, gamma, rho, lam, *args, **kwargs)</code>","text":"<p>Probability density function of the Kato--Jones (2015) distribution.</p> \\[ g(\\theta) = \\frac{1}{2\\pi}\\left[1 + \\frac{2\\gamma\\,(\\cos(\\theta-\\mu) - \\rho\\cos\\lambda)} {1 + \\rho^2 - 2\\rho\\cos(\\theta-\\mu-\\lambda)}\\right] \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the probability density function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, \\(0 \\leq \\mu &lt; 2\\pi\\).</p> required <code>gamma</code> <code>float</code> <p>Mean resultant length, \\(0 \\leq \\gamma &lt; 1\\).</p> required <code>rho</code> <code>float</code> <p>Second-order magnitude, \\(0 \\leq \\rho &lt; 1\\).</p> required <code>lam</code> <code>float</code> <p>Second-order phase, \\(0 \\leq \\lambda &lt; 2\\pi\\).</p> required <p>Returns:</p> Name Type Description <code>pdf_values</code> <code>array_like</code> <p>Probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def pdf(self, x, mu, gamma, rho, lam, *args, **kwargs):\n    r\"\"\"\n    Probability density function of the Kato--Jones (2015) distribution.\n\n    $$\n    g(\\theta) = \\frac{1}{2\\pi}\\left[1 + \\frac{2\\gamma\\,(\\cos(\\theta-\\mu) - \\rho\\cos\\lambda)}\n    {1 + \\rho^2 - 2\\rho\\cos(\\theta-\\mu-\\lambda)}\\right]\n    $$\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the probability density function.\n    mu : float\n        Mean direction, $0 \\leq \\mu &lt; 2\\pi$.\n    gamma : float\n        Mean resultant length, $0 \\leq \\gamma &lt; 1$.\n    rho : float\n        Second-order magnitude, $0 \\leq \\rho &lt; 1$.\n    lam : float\n        Second-order phase, $0 \\leq \\lambda &lt; 2\\pi$.\n\n    Returns\n    -------\n    pdf_values : array_like\n        Probability density function evaluated at `x`.\n    \"\"\"\n    return super().pdf(x, mu, gamma, rho, lam, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.katojones_gen.cdf","title":"<code>cdf(x, mu, gamma, rho, lam, *args, **kwargs)</code>","text":"<p>Cumulative distribution function of the Kato--Jones (2015) distribution.</p> <p>The CDF has the closed-form Fourier expansion</p> \\[ G(\\theta) = \\frac{\\theta}{2\\pi} + \\frac{1}{\\pi}\\sum_{p=1}^{\\infty} \\frac{\\gamma \\rho^{p-1}}{p} \\sin\\!\\bigl(p\\theta - [p\\mu + (p-1)\\lambda]\\bigr), \\] <p>which is evaluated adaptively by truncating the series once the tail contribution drops below a specified tolerance. No numerical quadrature is required.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the cumulative distribution function.</p> required <code>mu</code> <code>float</code> <p>Mean direction, \\(0 \\leq \\mu &lt; 2\\pi\\).</p> required <code>gamma</code> <code>float</code> <p>Mean resultant length, \\(0 \\leq \\gamma &lt; 1\\).</p> required <code>rho</code> <code>float</code> <p>Second-order magnitude, \\(0 \\leq \\rho &lt; 1\\).</p> required <code>lam</code> <code>float</code> <p>Second-order phase, \\(0 \\leq \\lambda &lt; 2\\pi\\).</p> required <p>Returns:</p> Name Type Description <code>cdf_values</code> <code>array_like</code> <p>Cumulative distribution function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def cdf(self, x, mu, gamma, rho, lam, *args, **kwargs):\n    r\"\"\"\n    Cumulative distribution function of the Kato--Jones (2015) distribution.\n\n    The CDF has the closed-form Fourier expansion\n\n    $$\n    G(\\theta) = \\frac{\\theta}{2\\pi}\n    + \\frac{1}{\\pi}\\sum_{p=1}^{\\infty} \\frac{\\gamma \\rho^{p-1}}{p}\n    \\sin\\!\\bigl(p\\theta - [p\\mu + (p-1)\\lambda]\\bigr),\n    $$\n\n    which is evaluated adaptively by truncating the series once the tail\n    contribution drops below a specified tolerance. No numerical quadrature\n    is required.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the cumulative distribution function.\n    mu : float\n        Mean direction, $0 \\leq \\mu &lt; 2\\pi$.\n    gamma : float\n        Mean resultant length, $0 \\leq \\gamma &lt; 1$.\n    rho : float\n        Second-order magnitude, $0 \\leq \\rho &lt; 1$.\n    lam : float\n        Second-order phase, $0 \\leq \\lambda &lt; 2\\pi$.\n\n    Returns\n    -------\n    cdf_values : array_like\n        Cumulative distribution function evaluated at `x`.\n    \"\"\"\n    return super().cdf(x, mu, gamma, rho, lam, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.katojones_gen.logpdf","title":"<code>logpdf(x, mu, gamma, rho, lam, *args, **kwargs)</code>","text":"<p>Logarithm of the probability density function of the Kato--Jones (2015) distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Points at which to evaluate the log-PDF.</p> required <code>mu</code> <code>float</code> <p>Mean direction, \\(0 \\leq \\mu &lt; 2\\pi\\).</p> required <code>gamma</code> <code>float</code> <p>Mean resultant length, \\(0 \\leq \\gamma &lt; 1\\).</p> required <code>rho</code> <code>float</code> <p>Second-order magnitude, \\(0 \\leq \\rho &lt; 1\\).</p> required <code>lam</code> <code>float</code> <p>Second-order phase, \\(0 \\leq \\lambda &lt; 2\\pi\\).</p> required <p>Returns:</p> Name Type Description <code>logpdf_values</code> <code>array_like</code> <p>Logarithm of the probability density function evaluated at <code>x</code>.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>def logpdf(self, x, mu, gamma, rho, lam, *args, **kwargs):\n    r\"\"\"\n    Logarithm of the probability density function of the Kato--Jones (2015)\n    distribution.\n\n    Parameters\n    ----------\n    x : array_like\n        Points at which to evaluate the log-PDF.\n    mu : float\n        Mean direction, $0 \\leq \\mu &lt; 2\\pi$.\n    gamma : float\n        Mean resultant length, $0 \\leq \\gamma &lt; 1$.\n    rho : float\n        Second-order magnitude, $0 \\leq \\rho &lt; 1$.\n    lam : float\n        Second-order phase, $0 \\leq \\lambda &lt; 2\\pi$.\n\n    Returns\n    -------\n    logpdf_values : array_like\n        Logarithm of the probability density function evaluated at `x`.\n    \"\"\"\n    return super().logpdf(x, mu, gamma, rho, lam, *args, **kwargs)\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.katojones_gen.convert_alpha2_beta2","title":"<code>convert_alpha2_beta2(gamma, alpha2, beta2, *, verify=True)</code>  <code>staticmethod</code>","text":"<p>Convert second-order moment parameters to (rho, lambda).</p> <p>Parameters:</p> Name Type Description Default <code>gamma</code> <code>float</code> <p>Mean resultant length, 0 &lt;= gamma &lt; 1.</p> required <code>alpha2</code> <code>float</code> <p>Second-order cosine moment around mu.</p> required <code>beta2</code> <code>float</code> <p>Second-order sine moment around mu.</p> required <code>verify</code> <code>bool</code> <p>If True (default), check that (alpha2, beta2) lies within the feasible disk for the supplied gamma and raise a ValueError if not.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>float</code> <p>Second-order magnitude parameter.</p> <code>lam</code> <code>float</code> <p>Second-order phase parameter in [0, 2 pi).</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>@staticmethod\ndef convert_alpha2_beta2(gamma, alpha2, beta2, *, verify=True):\n    \"\"\"\n    Convert second-order moment parameters to (rho, lambda).\n\n    Parameters\n    ----------\n    gamma : float\n        Mean resultant length, 0 &lt;= gamma &lt; 1.\n    alpha2 : float\n        Second-order cosine moment around mu.\n    beta2 : float\n        Second-order sine moment around mu.\n    verify : bool, optional\n        If True (default), check that (alpha2, beta2) lies within the feasible\n        disk for the supplied gamma and raise a ValueError if not.\n\n    Returns\n    -------\n    rho : float\n        Second-order magnitude parameter.\n    lam : float\n        Second-order phase parameter in [0, 2 pi).\n    \"\"\"\n    gamma = float(gamma)\n    alpha2 = float(alpha2)\n    beta2 = float(beta2)\n\n    if not (0.0 &lt;= gamma &lt; 1.0):\n        raise ValueError(\"`gamma` must lie in [0, 1).\")\n\n    radius_sq = (gamma * (1.0 - gamma)) ** 2\n    center_alpha = gamma * gamma\n    dist_sq = (alpha2 - center_alpha) ** 2 + beta2**2\n\n    tol = 1e-12\n    if verify and dist_sq &gt; radius_sq + tol:\n        raise ValueError(\n            f\"(alpha2, beta2) = ({alpha2}, {beta2}) is outside the feasible disk \"\n            f\"for gamma={gamma}.\"\n        )\n\n    r2 = np.hypot(alpha2, beta2)\n    if gamma &lt;= katojones_gen._moment_tolerance:\n        if verify and r2 &gt; tol:\n            raise ValueError(\n                \"When gamma is approximately zero, alpha2 and beta2 must also be near zero.\"\n            )\n        return 0.0, 0.0\n\n    rho = float(np.clip(r2 / gamma, 0.0, 1.0 - 1e-12))\n    if r2 &lt;= tol:\n        lam = 0.0\n    else:\n        lam = float(np.mod(np.arctan2(beta2, alpha2), 2.0 * np.pi))\n    return rho, lam\n</code></pre>"},{"location":"reference/distributions/#pycircstat2.distributions.katojones_gen.convert_rho_lambda","title":"<code>convert_rho_lambda(gamma, rho, lam, *, verify=True)</code>  <code>staticmethod</code>","text":"<p>Convert (rho, lambda) parameters to second-order moments (alpha2, beta2).</p> <p>Parameters:</p> Name Type Description Default <code>gamma</code> <code>float</code> <p>Mean resultant length, 0 &lt;= gamma &lt; 1.</p> required <code>rho</code> <code>float</code> <p>Second-order magnitude, 0 &lt;= rho &lt; 1.</p> required <code>lam</code> <code>float</code> <p>Second-order phase, 0 &lt;= lam &lt; 2*pi.</p> required <code>verify</code> <code>bool</code> <p>If True (default), ensure (gamma, rho, lam) satisfies the feasibility constraint and raise a ValueError otherwise.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>alpha2</code> <code>float</code> <p>Second-order cosine moment around mu.</p> <code>beta2</code> <code>float</code> <p>Second-order sine moment around mu.</p> Source code in <code>pycircstat2/distributions.py</code> <pre><code>@staticmethod\ndef convert_rho_lambda(gamma, rho, lam, *, verify=True):\n    \"\"\"\n    Convert (rho, lambda) parameters to second-order moments (alpha2, beta2).\n\n    Parameters\n    ----------\n    gamma : float\n        Mean resultant length, 0 &lt;= gamma &lt; 1.\n    rho : float\n        Second-order magnitude, 0 &lt;= rho &lt; 1.\n    lam : float\n        Second-order phase, 0 &lt;= lam &lt; 2*pi.\n    verify : bool, optional\n        If True (default), ensure (gamma, rho, lam) satisfies the feasibility\n        constraint and raise a ValueError otherwise.\n\n    Returns\n    -------\n    alpha2 : float\n        Second-order cosine moment around mu.\n    beta2 : float\n        Second-order sine moment around mu.\n    \"\"\"\n    gamma = float(gamma)\n    rho = float(rho)\n    lam = float(lam)\n\n    if not (0.0 &lt;= gamma &lt; 1.0):\n        raise ValueError(\"`gamma` must lie in [0, 1).\")\n    if not (0.0 &lt;= rho &lt; 1.0):\n        raise ValueError(\"`rho` must lie in [0, 1).\")\n\n    if verify:\n        constraint = (rho * np.cos(lam) - gamma) ** 2 + (rho * np.sin(lam)) ** 2\n        if constraint &gt; (1.0 - gamma) ** 2 + 1e-12:\n            raise ValueError(\n                f\"(gamma, rho, lam)=({gamma}, {rho}, {lam}) violates the feasibility constraint.\"\n            )\n\n    alpha2 = float(gamma * rho * np.cos(lam))\n    beta2 = float(gamma * rho * np.sin(lam))\n    return alpha2, beta2\n</code></pre>"},{"location":"reference/hypothesis/","title":"Hypothesis Testing","text":""},{"location":"reference/hypothesis/#pycircstat2.hypothesis.TestResult","title":"<code>TestResult</code>  <code>dataclass</code>","text":"<p>Base class for hypothesis test results.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>@dataclass(frozen=True)\nclass TestResult:\n    \"\"\"Base class for hypothesis test results.\"\"\"\n\n    def asdict(self) -&gt; dict[str, Any]:\n        \"\"\"Return result data as a dictionary.\"\"\"\n        from dataclasses import asdict\n\n        return asdict(self)\n\n    def significance(self, attr: str = \"pval\") -&gt; Optional[str]:\n        \"\"\"Return significance stars for the requested p-value attribute.\"\"\"\n\n        if not hasattr(self, attr):\n            return None\n\n        value = getattr(self, attr)\n        if value is None:\n            return None\n\n        try:\n            return significance_code(float(value))\n        except (TypeError, ValueError):\n            return None\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.TestResult.asdict","title":"<code>asdict()</code>","text":"<p>Return result data as a dictionary.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def asdict(self) -&gt; dict[str, Any]:\n    \"\"\"Return result data as a dictionary.\"\"\"\n    from dataclasses import asdict\n\n    return asdict(self)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.TestResult.significance","title":"<code>significance(attr='pval')</code>","text":"<p>Return significance stars for the requested p-value attribute.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def significance(self, attr: str = \"pval\") -&gt; Optional[str]:\n    \"\"\"Return significance stars for the requested p-value attribute.\"\"\"\n\n    if not hasattr(self, attr):\n        return None\n\n    value = getattr(self, attr)\n    if value is None:\n        return None\n\n    try:\n        return significance_code(float(value))\n    except (TypeError, ValueError):\n        return None\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.rayleigh_test","title":"<code>rayleigh_test(alpha=None, w=None, r=None, n=None, B=1, seed=2046, verbose=False)</code>","text":"<p>Rayleigh's Test for Circular Uniformity.</p> <ul> <li>H0: The data in the population are distributed uniformly around the circle.</li> <li>H1: The data in the population are not disbutrited uniformly around the circle.</li> </ul> \\[ z = n \\cdot r^2 \\] <p>and</p> \\[ p = \\exp(\\sqrt{1 + 4n + 4(n^2 - R^2)} - (1 + 2n)) \\] <p>This method is for ungrouped data. For testing uniformity with grouped data, use <code>chisquare_test()</code> or <code>scipy.stats.chisquare()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies of angles.</p> <code>None</code> <code>r</code> <code>Optional[float]</code> <p>Resultant vector length from <code>descriptive.circ_mean()</code>.</p> <code>None</code> <code>n</code> <code>Optional[int]</code> <p>Sample size.</p> <code>None</code> <code>B</code> <code>int</code> <p>Number of bootstrap samples for p-value estimation.</p> <code>1</code> <code>seed</code> <code>SeedLike</code> <p>Seed used to initialize the random number generator for bootstrap resampling when <code>B &gt; 1</code>. Accepts integers, sequences of integers, <code>numpy.random.Generator</code>, <code>numpy.random.BitGenerator</code>, <code>numpy.random.SeedSequence</code> or <code>None</code>. Defaults to 2046.</p> <code>2046</code> <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>RayleighTestResult</code> <p>A dataclass containing:</p> <ul> <li>r: float<ul> <li>Resultant vector length.</li> </ul> </li> <li>z: float<ul> <li>Test statistic (Rayleigh's Z).</li> </ul> </li> <li>pval: float<ul> <li>Classical p-value based on the asymptotic formula.</li> </ul> </li> <li>bootstrap_pval: float or None<ul> <li>Bootstrap p-value (if computed, i.e., B &gt; 1); otherwise, None.</li> </ul> </li> </ul> Reference <p>P625, Section 27.1, Example 27.1 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def rayleigh_test(\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    r: Optional[float] = None,\n    n: Optional[int] = None,\n    B: int = 1,\n    seed: SeedLike = 2046,\n    verbose: bool = False,\n) -&gt; RayleighTestResult:\n    r\"\"\"\n    Rayleigh's Test for Circular Uniformity.\n\n    - H0: The data in the population are distributed uniformly around the circle.\n    - H1: The data in the population are not disbutrited uniformly around the circle.\n\n    $$ z = n \\cdot r^2 $$\n\n    and\n\n    $$ p = \\exp(\\sqrt{1 + 4n + 4(n^2 - R^2)} - (1 + 2n)) $$\n\n    This method is for ungrouped data. For testing uniformity with\n    grouped data, use `chisquare_test()` or `scipy.stats.chisquare()`.\n\n    Parameters\n    ----------\n\n    alpha: np.array or None\n        Angles in radian.\n\n    w: np.array or None.\n        Frequencies of angles.\n\n    r: float or None\n        Resultant vector length from `descriptive.circ_mean()`.\n\n    n: int or None\n        Sample size.\n\n    B: int\n        Number of bootstrap samples for p-value estimation.\n\n    seed: SeedLike\n        Seed used to initialize the random number generator for bootstrap resampling\n        when ``B &gt; 1``. Accepts integers, sequences of integers, ``numpy.random.Generator``,\n        ``numpy.random.BitGenerator``, ``numpy.random.SeedSequence`` or ``None``.\n        Defaults to 2046.\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    RayleighTestResult\n        A dataclass containing:\n\n        - r: float\n            - Resultant vector length.\n        - z: float\n            - Test statistic (Rayleigh's Z).\n        - pval: float\n            - Classical p-value based on the asymptotic formula.\n        - bootstrap_pval: float or None\n            - Bootstrap p-value (if computed, i.e., B &gt; 1); otherwise, None.\n\n    Reference\n    ---------\n    P625, Section 27.1, Example 27.1 of Zar, 2010\n    \"\"\"\n\n    if B &lt;= 0:\n        raise ValueError(\"`B` must be a positive integer.\")\n\n    if r is None:\n        if alpha is None:\n            raise ValueError(\"If `r` is None, then `alpha` (and optionally `w`) is required.\")\n        alpha = np.asarray(alpha, dtype=float)\n        if alpha.size == 0:\n            raise ValueError(\"`alpha` must contain at least one angle.\")\n        if w is None:\n            w = np.ones_like(alpha, dtype=float)\n        else:\n            w = np.asarray(w, dtype=float)\n            if w.shape != alpha.shape:\n                raise ValueError(\"`w` must have the same shape as `alpha`.\")\n        n_total = float(np.sum(w))\n        if n_total &lt;= 0:\n            raise ValueError(\"Sample size inferred from `w` must be positive.\")\n        if not np.isclose(n_total, round(n_total)):\n            raise ValueError(\"Rayleigh's test requires integer sample sizes when weights are used.\")\n        n = int(round(n_total))\n        r = circ_r(alpha, w)\n    else:\n        r = float(r)\n\n    if n is None or n &lt;= 0:\n        raise ValueError(\"Sample size `n` must be provided and positive when `r` is given.\")\n\n    if not (0.0 &lt;= r &lt;= 1.0):\n        raise ValueError(\"`r` must lie in the interval [0, 1].\")\n\n    R = n * r\n    z = n * r**2  # eq(27.2)\n\n    pval = np.exp(np.sqrt(1 + 4 * n + 4 * (n**2 - R**2)) - (1 + 2 * n))  # eq(27.4)\n\n    bootstrap_pval: Optional[float]\n    if seed is True and verbose is False:\n        warnings.warn(\n            \"Passing `verbose` as a positional argument is deprecated; use keyword arguments.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        verbose = bool(seed)\n        seed = 2046\n\n    if B &gt; 1:\n        rng = _init_rng(seed)\n        uniforms = rng.uniform(0.0, 2 * np.pi, size=(B, n))\n        unit_vectors = np.exp(1j * uniforms)\n        resultant_lengths = np.abs(np.sum(unit_vectors, axis=1))\n        bootstrap_stats = (resultant_lengths**2) / n\n        bootstrap_pval = float((np.count_nonzero(bootstrap_stats &gt;= z) + 1) / (B + 1))\n    else:\n        bootstrap_pval = None\n\n    if verbose:\n        print(\"Rayleigh's Test of Uniformity\")\n        print(\"-----------------------------\")\n        print(\"H0: \u03c1 = 0\")\n        print(\"HA: \u03c1 \u2260 0\")\n        print(\"\")\n        print(f\"Test Statistics  (\u03c1 | z-score): {r:.5f} | {z:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n        if B &gt; 1 and bootstrap_pval is not None:\n            print(\n                f\"Bootstrap P-value: {bootstrap_pval:.5f} {significance_code(bootstrap_pval)}\"\n            )\n\n    return RayleighTestResult(r=r, z=z, pval=pval, bootstrap_pval=bootstrap_pval)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.chisquare_test","title":"<code>chisquare_test(w, verbose=False)</code>","text":"<p>Chi-Square Goodness of Fit for Circular data.</p> <ul> <li>H0: The data in the population are distributed uniformly around the circle.</li> <li>H1: THe data in the population are not disbutrited uniformly around the circle.</li> </ul> <p>For method is for grouped data.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>ndarray</code> <p>Frequencies of angles</p> required <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>ChiSquareTestResult</code> <p>A dataclass containing:</p> <ul> <li>chi2: float<ul> <li>The chi-squared test statistic.</li> </ul> </li> <li>pval: float<ul> <li>The p-value of the test.</li> </ul> </li> </ul> Note <p>It's a wrapper of scipy.stats.chisquare()</p> Reference <p>P662-663, Section 27.17, Example 27.23 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def chisquare_test(w: np.ndarray, verbose: bool = False) -&gt; ChiSquareTestResult:\n    \"\"\"Chi-Square Goodness of Fit for Circular data.\n\n    - H0: The data in the population are distributed uniformly around the circle.\n    - H1: THe data in the population are not disbutrited uniformly around the circle.\n\n    For method is for grouped data.\n\n    Parameters\n    ----------\n    w: np.ndarray\n        Frequencies of angles\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    ChiSquareTestResult\n        A dataclass containing:\n\n        - chi2: float\n            - The chi-squared test statistic.\n        - pval: float\n            - The p-value of the test.\n\n    Note\n    ----\n    It's a wrapper of scipy.stats.chisquare()\n\n    Reference\n    ---------\n    P662-663, Section 27.17, Example 27.23 of Zar, 2010\n    \"\"\"\n    from scipy.stats import chisquare\n\n    frequencies = np.asarray(w, dtype=float)\n    if frequencies.ndim != 1 or frequencies.size == 0:\n        raise ValueError(\"`w` must be a one-dimensional array with at least one element.\")\n    if np.any(frequencies &lt; 0):\n        raise ValueError(\"`w` must contain non-negative frequencies.\")\n\n    res = chisquare(frequencies)\n    chi2 = res.statistic\n    pval = res.pvalue\n\n    if verbose:\n        print(\"Chi-Square Test of Uniformity\")\n        print(\"-----------------------------\")\n        print(\"H0: uniform\")\n        print(\"HA: not uniform\")\n        print(\"\")\n        print(f\"Test Statistics (\u03c7\u00b2): {chi2:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n\n    return ChiSquareTestResult(chi2=chi2, pval=pval)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.V_test","title":"<code>V_test(angle, alpha=None, w=None, mean=None, r=None, n=None, verbose=False)</code>","text":"<p>Modified Rayleigh Test for Uniformity versus a Specified Angle.</p> <ul> <li>H0: The population is uniformly distributed around the circle (i.e., H0: \u03c1=0)</li> <li>H1: The population is not uniformly distributed around the circle (i.e., H1: \u03c1!=0),     but has a mean of certain degree.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>angle</code> <code>Union[int, float]</code> <p>Angle in radian to be compared with mean angle.</p> required <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies of angles.</p> <code>None</code> <code>mean</code> <code>Optional[float]</code> <p>Circular mean from <code>descriptive.circ_mean()</code>. Needed if <code>alpha</code> is None.</p> <code>None</code> <code>r</code> <code>Optional[float]</code> <p>Resultant vector length from <code>descriptive.circ_mean()</code>. Needed if <code>alpha</code> is None.</p> <code>None</code> <code>n</code> <code>Optional[int]</code> <p>Sample size. Needed if <code>alpha</code> is None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>VTestResult</code> <p>Dataclass containing the test statistic <code>V</code>, the normalized statistic <code>u</code>, and the p-value.</p> Reference <p>P627, Section 27.1, Example 27.2 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def V_test(\n    angle: Union[int, float],\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    mean: Optional[float] = None,\n    r: Optional[float] = None,\n    n: Optional[int] = None,\n    verbose: bool = False,\n) -&gt; VTestResult:\n    \"\"\"\n    Modified Rayleigh Test for Uniformity versus a Specified Angle.\n\n    - H0: The population is uniformly distributed around the circle (i.e., H0: \u03c1=0)\n    - H1: The population is not uniformly distributed around the circle (i.e., H1: \u03c1!=0),\n        but has a mean of certain degree.\n\n    Parameters\n    ----------\n    angle: float or int\n        Angle in radian to be compared with mean angle.\n\n    alpha: np.array or None\n        Angles in radian.\n\n    w: np.array or None.\n        Frequencies of angles.\n\n    mean: float or None\n        Circular mean from `descriptive.circ_mean()`. Needed if `alpha` is None.\n\n    r: float or None\n        Resultant vector length from `descriptive.circ_mean()`. Needed if `alpha` is None.\n\n    n: int or None\n        Sample size. Needed if `alpha` is None.\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    VTestResult\n        Dataclass containing the test statistic `V`, the normalized statistic `u`,\n        and the p-value.\n\n    Reference\n    ---------\n    P627, Section 27.1, Example 27.2 of Zar, 2010\n    \"\"\"\n\n    angle = float(angle)\n\n    if mean is None or r is None or n is None:\n        if alpha is None:\n            raise ValueError(\"If `mean`, `r`, or `n` is None, then `alpha` (and optionally `w`) is required.\")\n        alpha = np.asarray(alpha, dtype=float)\n        if alpha.size == 0:\n            raise ValueError(\"`alpha` must contain at least one angle.\")\n        if w is None:\n            w = np.ones_like(alpha, dtype=float)\n        else:\n            w = np.asarray(w, dtype=float)\n            if w.shape != alpha.shape:\n                raise ValueError(\"`w` must have the same shape as `alpha`.\")\n        n = int(np.sum(w))\n        if n &lt;= 0:\n            raise ValueError(\"Sample size inferred from `w` must be positive.\")\n        mean, r = circ_mean_and_r(alpha, w)\n    else:\n        mean = float(mean)\n        r = float(r)\n        if n &lt;= 0:\n            raise ValueError(\"`n` must be positive.\")\n\n    if not (0.0 &lt;= r &lt;= 1.0):\n        raise ValueError(\"`r` must lie in the interval [0, 1].\")\n\n    R = n * r\n    V = R * np.cos(angmod(mean - angle, bounds=[-np.pi, np.pi]))  # eq(27.5)\n    u = V * np.sqrt(2.0 / n)  # eq(27.6)\n    pval = float(norm.sf(u))\n\n    if verbose:\n        print(\"Modified Rayleigh's Test of Uniformity\")\n        print(\"--------------------------------------\")\n        print(\"H0: \u03c1 = 0\")\n        print(f\"HA: \u03c1 \u2260 0 and \u03bc = {angle:.5f} rad\")\n        print(\"\")\n        print(f\"Test Statistics: {V:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n\n    return VTestResult(V=V, u=u, pval=pval)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.one_sample_test","title":"<code>one_sample_test(angle, alpha=None, w=None, lb=None, ub=None, verbose=False)</code>","text":"<p>To test whether the population mean angle is equal to a specified value, which is achieved by observing whether the angle lies within the 95% CI.</p> <ul> <li>H0: The population has a mean of \u03bc (\u03bc_a = \u03bc_0)</li> <li>H1: The population mean is not \u03bc (\u03bc_a \u2260 \u03bc_0)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>angle</code> <code>Union[int, float]</code> <p>Angle in radian to be compared with mean angle.</p> required <code>alpha</code> <code>Optional[ndarray]</code> <p>Angles in radian.</p> <code>None</code> <code>w</code> <code>Optional[ndarray]</code> <p>Frequencies of angles</p> <code>None</code> <code>lb</code> <code>Optional[float]</code> <p>Lower bound of circular mean from <code>descriptive.circ_mean_ci()</code>.</p> <code>None</code> <code>ub</code> <code>Optional[float]</code> <p>Upper bound of circular mean from <code>descriptive.circ_mean_ci()</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> Reference <p>P628, Section 27.1, Example 27.3 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def one_sample_test(\n    angle: Union[int, float],\n    alpha: Optional[np.ndarray] = None,\n    w: Optional[np.ndarray] = None,\n    lb: Optional[float] = None,\n    ub: Optional[float] = None,\n    verbose: bool = False,\n) -&gt; OneSampleTestResult:\n    \"\"\"\n    To test whether the population mean angle is equal to a specified value,\n    which is achieved by observing whether the angle lies within the 95% CI.\n\n    - H0: The population has a mean of \u03bc (\u03bc_a = \u03bc_0)\n    - H1: The population mean is not \u03bc (\u03bc_a \u2260 \u03bc_0)\n\n    Parameters\n    ----------\n\n    angle: float or int\n        Angle in radian to be compared with mean angle.\n\n    alpha: np.array or None\n        Angles in radian.\n\n    w: np.array or None.\n        Frequencies of angles\n\n    lb: float\n        Lower bound of circular mean from `descriptive.circ_mean_ci()`.\n\n    ub: float\n        Upper bound of circular mean from `descriptive.circ_mean_ci()`.\n\n    verbose: bool\n        Print formatted results.\n\n    Reference\n    ---------\n    P628, Section 27.1, Example 27.3 of Zar, 2010\n    \"\"\"\n\n    angle = float(angle)\n\n    if lb is None or ub is None:\n        if alpha is None:\n            raise ValueError(\"If `lb` or `ub` is None, then `alpha` (and optionally `w`) is required.\")\n        alpha = np.asarray(alpha, dtype=float)\n        if alpha.size == 0:\n            raise ValueError(\"`alpha` must contain at least one angle.\")\n        if w is None:\n            w = np.ones_like(alpha, dtype=float)\n        else:\n            w = np.asarray(w, dtype=float)\n            if w.shape != alpha.shape:\n                raise ValueError(\"`w` must have the same shape as `alpha`.\")\n        lb, ub = circ_mean_ci(alpha=alpha, w=w)\n\n    lb = float(lb)\n    ub = float(ub)\n\n    reject = not is_within_circular_range(angle, lb, ub)\n\n    if verbose:\n        print(\"One-Sample Test for the Mean Angle\")\n        print(\"----------------------------------\")\n        print(\"H0: \u03bc = \u03bc0\")\n        print(f\"HA: \u03bc \u2260 \u03bc0 and \u03bc0 = {angle:.5f} rad\")\n        print(\"\")\n        if reject:\n            print(\n                f\"Reject H0:\\n\u03bc0 = {angle:.5f} lies outside the 95% CI of \u03bc ({np.array([lb, ub]).round(5)})\"\n            )\n        else:\n            print(\n                f\"Failed to reject H0:\\n\u03bc0 = {angle:.5f} lies within the 95% CI of \u03bc ({np.array([lb, ub]).round(5)})\"\n            )\n\n    return OneSampleTestResult(reject=reject, angle=angle, ci=(lb, ub))\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.omnibus_test","title":"<code>omnibus_test(alpha, scale=1, verbose=False)</code>","text":"<p>Hodges\u2013Ajne omnibus test for circular uniformity.</p> <ul> <li>H0: The population is uniformly distributed around the circle</li> <li>H1: The population is not uniformly distributed.</li> </ul> <p>This test is distribution-free and handles uni-, bi-, and multimodal alternatives.  The classical p-value involves factorials and overflows for large n.  We therefore compute it in log-space (<code>math.lgamma</code>) and exponentiate at the very end.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>scale</code> <code>int</code> <p>Scale factor for the number of lines to be tested.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>OmnibusTestResult</code> <p>Dataclass containing the test statistic <code>A</code>, the corresponding p-value, and the minimum count <code>m</code>.</p> Reference <p>P629-630, Section 27.2, Example 27.4 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def omnibus_test(\n    alpha: np.ndarray,\n    scale: int = 1,\n    verbose: bool = False,\n) -&gt; OmnibusTestResult:\n    \"\"\"\n    Hodges\u2013Ajne omnibus test for circular uniformity.\n\n    - H0: The population is uniformly distributed around the circle\n    - H1: The population is not uniformly distributed.\n\n    This test is distribution-free and handles uni-, bi-, and multimodal\n    alternatives.  The classical p-value involves factorials and\n    overflows for large *n*.  We therefore compute it in log-space\n    (``math.lgamma``) and exponentiate at the very end.\n\n    Parameters\n    ----------\n    alpha: np.array or None\n        Angles in radian.\n\n    scale: int\n        Scale factor for the number of lines to be tested.\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    OmnibusTestResult\n        Dataclass containing the test statistic `A`, the corresponding p-value,\n        and the minimum count `m`.\n\n    Reference\n    ---------\n    P629-630, Section 27.2, Example 27.4 of Zar, 2010\n    \"\"\"\n\n    if scale &lt;= 0:\n        raise ValueError(\"`scale` must be a positive integer.\")\n\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    lines = np.linspace(0.0, np.pi, scale * 360, endpoint=False)\n    n = alpha.size\n\n    lines_rotated = angmod(lines[:, None] - alpha)\n\n    # # count number of points on the right half circle, excluding the boundaries\n    right = n - np.logical_and(\n        lines_rotated &gt; 0.0, lines_rotated &lt; np.pi\n    ).sum(axis=1)\n    m = int(np.min(right))\n\n    # ------------------------------------------------------------------\n    # 2. p-value   \u2014\u2014\u2014  analytical formula and its log form\n    # ------------------------------------------------------------------\n    #     Classical (Zar 2010, eq. 27-4):\n    #\n    #         p  =  (n \u2212 2m) \u00b7 n! / [ m! \u00b7 (n \u2212 m)! \u00b7 2^(n\u22121) ]            \u2026(1)\n    #       # pval = (\n    #       #    (n - 2 * m)\n    #       #    * math.factorial(n)\n    #       #    / (math.factorial(m) * math.factorial(n - m))\n    #       #    / 2 ** (n - 1)\n    #       # ) # eq(27.7)\n\n    #     Taking natural logs and using  \u0393(k+1) = k!  with  log \u0393 = lgamma:\n    #\n    #         ln p  =  ln(n \u2212 2m)\n    #                 + lgamma(n + 1)\n    #                 \u2212 lgamma(m + 1)\n    #                 \u2212 lgamma(n \u2212 m + 1)\n    #                 \u2212 (n \u2212 1)\u00b7ln 2                                        \u2026(2)\n    #\n    #     Eq. (2) is numerically safe for very large n; we exponentiate at\n    #     the end, knowing the result may under-flow to 0.0 in double precision.\n    # ------------------------------------------------------------------\n\n    denom = n - 2 * m\n    if denom &lt;= 0:\n        logp = -np.inf\n        pval = 0.0\n        A = np.inf\n    else:\n        logp = (\n            math.log(denom)\n            + math.lgamma(n + 1)\n            - math.lgamma(m + 1)\n            - math.lgamma(n - m + 1)\n            - (n - 1) * math.log(2.0)\n        )\n        pval = float(np.exp(logp))\n        A = np.pi * np.sqrt(n) / (2 * denom)\n\n    if verbose:\n        print('Hodges-Ajne (\"omnibus\") Test for Uniformity')\n        print(\"-------------------------------------------\")\n        print(\"H0: uniform\")\n        print(\"HA: not unifrom\")\n        print(\"\")\n        print(f\"Test Statistics: {A:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n    return OmnibusTestResult(A=float(A), pval=float(pval), m=int(m))\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.batschelet_test","title":"<code>batschelet_test(angle, alpha, verbose=False)</code>","text":"<p>Modified Hodges-Ajne Test for Uniformity versus a specified Angle (for ungrouped data).</p> <ul> <li>H0: The population is uniformly distributed around the circle.</li> <li>H1: The population is not uniformly distributed around the circle, but     is concentrated around a specified angle.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>angle</code> <code>Union[int, float]</code> <p>A specified angle.</p> required <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> Reference <p>P630-631, Section 27.2, Example 27.5 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def batschelet_test(\n    angle: Union[int, float],\n    alpha: np.ndarray,\n    verbose: bool = False,\n) -&gt; BatscheletTestResult:\n    \"\"\"Modified Hodges-Ajne Test for Uniformity versus a specified Angle\n    (for ungrouped data).\n\n    - H0: The population is uniformly distributed around the circle.\n    - H1: The population is not uniformly distributed around the circle, but\n        is concentrated around a specified angle.\n\n    Parameters\n    ----------\n    angle: np.array\n        A specified angle.\n\n    alpha: np.array or None\n        Angles in radian.\n\n    verbose: bool\n        Print formatted results.\n\n    Reference\n    ---------\n    P630-631, Section 27.2, Example 27.5 of Zar, 2010\n    \"\"\"\n\n    from scipy.stats import binomtest\n\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    angle = float(angle)\n\n    n = alpha.size\n    angle_diff = angmod((angle + 0.5 * np.pi) - alpha)\n    m = np.logical_and(angle_diff &gt; 0.0, angle_diff &lt; np.pi).sum()\n    C = int(n - m)\n    pval = float(binomtest(C, n=n, p=0.5).pvalue)\n\n    if verbose:\n        print(\"Batschelet Test for Uniformity\")\n        print(\"------------------------------\")\n        print(\"H0: uniform\")\n        print(f\"HA: not unifrom but concentrated around \u03b8 = {angle:.5f} rad\")\n        print(\"\")\n        print(f\"Test Statistics: {C}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n\n    return BatscheletTestResult(C=C, pval=pval)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.symmetry_test","title":"<code>symmetry_test(alpha, median=None, verbose=False)</code>","text":"<p>Non-parametric test for symmetry around the median. Works by performing a Wilcoxon sign rank test on the differences to the median. Also known as Wilcoxon paired-sample test.</p> <ul> <li>H0: the population is symmetrical around the median</li> <li>HA: the population is not symmetrical around the median</li> </ul> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>median</code> <code>Optional[float]</code> <p>Median computed by <code>descriptive.median()</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> Reference <p>P631-632, Section 27.3, Example 27.6 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def symmetry_test(\n    alpha: np.ndarray,\n    median: Optional[float] = None,\n    verbose: bool = False,\n) -&gt; SymmetryTestResult:\n    \"\"\"Non-parametric test for symmetry around the median. Works by performing a\n    Wilcoxon sign rank test on the differences to the median. Also known as\n    Wilcoxon paired-sample test.\n\n    - H0: the population is symmetrical around the median\n    - HA: the population is not symmetrical around the median\n\n    Parameters\n    ----------\n    alpha: np.array\n        Angles in radian.\n\n    median: float or None.\n        Median computed by `descriptive.median()`.\n\n    verbose: bool\n        Print formatted results.\n\n    Reference\n    ---------\n    P631-632, Section 27.3, Example 27.6 of Zar, 2010\n    \"\"\"\n\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    if median is None:\n        median = float(circ_median(alpha=alpha))\n    else:\n        median = float(median)\n\n    d = angmod(alpha - median, bounds=[-np.pi, np.pi])\n\n    res = wilcoxon(d, alternative=\"two-sided\")\n    test_statistic = float(res.statistic)\n    pval = float(res.pvalue)\n\n    if verbose:\n        print(\"Symmetry Test\")\n        print(\"------------------------------\")\n        print(\"H0: symmetrical around median\")\n        print(\"HA: not symmetrical around median\")\n        print(\"\")\n        print(f\"Test Statistics: {test_statistic:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n\n    return SymmetryTestResult(statistic=test_statistic, pval=pval)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.watson_williams_test","title":"<code>watson_williams_test(samples, verbose=False)</code>","text":"<p>The Watson-Williams Test for multiple samples.</p> <ul> <li>H0: All samples are from populations with the same mean angle</li> <li>H1: All samples are not from populations with the same mean angle</li> </ul> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Sequence[Any]</code> <p>A sequence of <code>Circular</code> objects or one-dimensional array-like radian samples.</p> required <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>WatsonWilliamsTestResult</code> <p>Dataclass containing the F statistic, p-value, and associated degrees of freedom.</p> Reference <p>P632-636, Section 27.4, Example 27.7/8 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def watson_williams_test(\n    samples: Sequence[Any],\n    verbose: bool = False,\n) -&gt; WatsonWilliamsTestResult:\n    \"\"\"The Watson-Williams Test for multiple samples.\n\n    - H0: All samples are from populations with the same mean angle\n    - H1: All samples are not from populations with the same mean angle\n\n    Parameters\n    ----------\n    samples: sequence\n        A sequence of `Circular` objects or one-dimensional array-like radian samples.\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    WatsonWilliamsTestResult\n        Dataclass containing the F statistic, p-value, and associated degrees of freedom.\n\n    Reference\n    ---------\n    P632-636, Section 27.4, Example 27.7/8 of Zar, 2010\n    \"\"\"\n\n    normalized = _coerce_circular_samples(samples)\n    if len(normalized) &lt; 2:\n        raise ValueError(\"At least two samples are required for the Watson-Williams test.\")\n\n    k = len(normalized)\n    N = sum(sample.n for sample in normalized)\n    if N &lt;= k:\n        raise ValueError(\"Combined sample size must exceed the number of groups.\")\n\n    Rs = np.array([sample.R for sample in normalized], dtype=float)\n    rw = float(np.sum(Rs) / N)\n\n    kappa_hat = float(circ_kappa(rw))\n    if not np.isfinite(kappa_hat):\n        kappa_hat = 0.0\n    if kappa_hat &lt;= 0.0:\n        K = 1.0\n        warnings.warn(\n            (\n                \"Watson-Williams test assumes common, high concentration; \"\n                \"estimated \u03ba\u22480. Results may be unreliable.\"\n            ),\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    else:\n        K = 1.0 + 3.0 / (8.0 * kappa_hat)\n        if kappa_hat &lt; 1.0:\n            warnings.warn(\n                (\n                    \"Watson-Williams test assumes common, high concentration; \"\n                    f\"estimated \u03ba\u2248{kappa_hat:.3f}. Results may be unreliable.\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n\n    all_alpha = np.hstack([sample.alpha for sample in normalized])\n    all_weights = np.hstack([sample.w for sample in normalized])\n    R = N * circ_r(alpha=all_alpha, w=all_weights)\n    F = K * (N - k) * (np.sum(Rs) - R) / (N - np.sum(Rs)) / (k - 1)\n    df_between = k - 1\n    df_within = N - k\n    pval = float(f.sf(F, df_between, df_within))\n\n    result = WatsonWilliamsTestResult(\n        F=float(F),\n        pval=pval,\n        df_between=df_between,\n        df_within=df_within,\n        k=k,\n        N=N,\n    )\n\n    if verbose:\n        print(\"The Watson-Williams Test for multiple samples\")\n        print(\"---------------------------------------------\")\n        print(\"H0: all samples are from populations with the same angle.\")\n        print(\"HA: all samples are not from populations with the same angle.\")\n        print(\"\")\n        print(f\"Test Statistics: {result.F:.5f}\")\n        print(f\"P-value: {result.pval:.5f} {significance_code(result.pval)}\")\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.watson_u2_test","title":"<code>watson_u2_test(samples, verbose=False)</code>","text":"<p>Watson's U2 Test for nonparametric two-sample testing (with or without ties).</p> <ul> <li>H0: The two samples came from the same population,     or from two populations having the same direction.</li> <li>H1: The two samples did not come from the same population,     or from two populations having the same directions.</li> </ul> <p>Use this instead of Watson-Williams two-sample test when at least one of the sampled populations is not unimodal or when there are other considerable departures from the assumptions of the latter test. It may be used on grouped data if the grouping interval is no greater than 5 degree.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Sequence[Any]</code> <p>A sequence of <code>Circular</code> objects or one-dimensional array-like radian samples.</p> required <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>WatsonU2TestResult</code> <p>Dataclass containing the U\u00b2 statistic and the associated p-value.</p> Reference <p>P637-638, Section 27.5, Example 27.9 of Zar, 2010 P639-640, Section 27.5, Example 27.10 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def watson_u2_test(\n    samples: Sequence[Any],\n    verbose: bool = False,\n) -&gt; WatsonU2TestResult:\n    \"\"\"Watson's U2 Test for nonparametric two-sample testing\n    (with or without ties).\n\n    - H0: The two samples came from the same population,\n        or from two populations having the same direction.\n    - H1: The two samples did not come from the same population,\n        or from two populations having the same directions.\n\n    Use this instead of Watson-Williams two-sample test when at\n    least one of the sampled populations is not unimodal or when\n    there are other considerable departures from the assumptions\n    of the latter test. It may be used on grouped data if the\n    grouping interval is no greater than 5 degree.\n\n    Parameters\n    ----------\n    samples: sequence\n        A sequence of `Circular` objects or one-dimensional array-like radian samples.\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    WatsonU2TestResult\n        Dataclass containing the U\u00b2 statistic and the associated p-value.\n\n    Reference\n    ---------\n    P637-638, Section 27.5, Example 27.9 of Zar, 2010\n    P639-640, Section 27.5, Example 27.10 of Zar, 2010\n    \"\"\"\n\n    from scipy.stats import rankdata\n\n    normalized = _coerce_circular_samples(samples)\n    if len(normalized) != 2:\n        raise ValueError(\"`watson_u2_test` requires exactly two samples.\")\n\n    def cumfreq(alpha_unique: np.ndarray, sample: _CircularSample) -&gt; np.ndarray:\n        expanded = sample.expand()\n        if expanded.size == 0:\n            raise ValueError(\"Each sample must contain at least one observation.\")\n\n        idx = [np.where(np.isclose(alpha_unique, val, atol=1e-10))[0] for val in expanded]\n        idx = np.concatenate(idx)\n        idx = np.hstack([0, idx, alpha_unique.size])\n\n        freq_cumsum = rankdata(expanded, method=\"max\") / sample.n\n        freq_cumsum = np.hstack([0, freq_cumsum])\n\n        tiles = np.diff(idx)\n        return np.repeat(freq_cumsum, tiles)\n\n    expanded_samples = [sample.expand() for sample in normalized]\n    a, t = np.unique(np.hstack(expanded_samples), return_counts=True)\n    cfs = [cumfreq(a, sample) for sample in normalized]\n    d = np.diff(cfs, axis=0)\n\n    N = sum(sample.n for sample in normalized)\n    U2 = (\n        np.prod([sample.n for sample in normalized])\n        / N**2\n        * (np.sum(t * d**2) - np.sum(t * d) ** 2 / N)\n    )\n    pval = 2 * np.exp(-19.74 * U2)\n    # Approximated P-value from Watson (1961)\n    # https://github.com/pierremegevand/watsons_u2/blob/master/watsons_U2_approx_p.m\n\n    if verbose:\n        print(\"Watson's U2 Test for two samples\")\n        print(\"---------------------------------------------\")\n        print(\"H0: The two samples are from populations with the same angle.\")\n        print(\"HA: The two samples are not from populations with the same angle.\")\n        print(\"\")\n        print(f\"Test Statistics: {U2:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n\n    return WatsonU2TestResult(U2=float(U2), pval=float(pval))\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.wheeler_watson_test","title":"<code>wheeler_watson_test(samples, verbose=False)</code>","text":"<p>The Wheeler and Watson Two/Multi-Sample Test.</p> <ul> <li>H0: The two samples came from the same population,     or from two populations having the same direction.</li> <li>H1: The two samples did not come from the same population,     or not from two populations having the same directions.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Sequence[Any]</code> <p>A sequence of <code>Circular</code> objects or one-dimensional array-like radian samples.</p> required <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>WheelerWatsonTestResult</code> <p>Dataclass containing the W statistic, degrees of freedom, and p-value.</p> Reference <p>P640-642, Section 27.5, Example 27.11 of Zar, 2010</p> Note <p>The current implementation doesn't consider ties in the data. Can be improved with P144, Pewsey et al. (2013)</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def wheeler_watson_test(\n    samples: Sequence[Any],\n    verbose: bool = False,\n) -&gt; WheelerWatsonTestResult:\n    \"\"\"The Wheeler and Watson Two/Multi-Sample Test.\n\n    - H0: The two samples came from the same population,\n        or from two populations having the same direction.\n    - H1: The two samples did not come from the same population,\n        or not from two populations having the same directions.\n\n    Parameters\n    ----------\n    samples: sequence\n        A sequence of `Circular` objects or one-dimensional array-like radian samples.\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    WheelerWatsonTestResult\n        Dataclass containing the W statistic, degrees of freedom, and p-value.\n\n    Reference\n    ---------\n    P640-642, Section 27.5, Example 27.11 of Zar, 2010\n\n    Note\n    ----\n    The current implementation doesn't consider ties in the data.\n    Can be improved with P144, Pewsey et al. (2013)\n    \"\"\"\n    from scipy.stats import chi2\n\n    normalized = _coerce_circular_samples(samples)\n\n    def get_circrank(alpha: np.ndarray, sample: _CircularSample, N: int) -&gt; np.ndarray:\n        expanded = sample.expand()\n        rank_of_direction = (\n            np.squeeze([np.where(np.isclose(alpha, value))[0] for value in expanded]) + 1\n        )\n        return 2 * np.pi / N * rank_of_direction\n\n    N = sum(sample.n for sample in normalized)\n    expanded_samples = [sample.expand() for sample in normalized]\n    a, _ = np.unique(np.hstack(expanded_samples), return_counts=True)\n\n    circ_ranks = [get_circrank(a, sample, N) for sample in normalized]\n\n    k = len(circ_ranks)\n\n    if k == 2:\n        C = np.sum(np.cos(circ_ranks[0]))\n        S = np.sum(np.sin(circ_ranks[0]))\n        W = 2 * (N - 1) * (C**2 + S**2) / np.prod([sample.n for sample in normalized])\n    elif k &gt;= 3:\n        W = 0.0\n        for i in range(k):\n            circ_rank = circ_ranks[i]\n            C = np.sum(np.cos(circ_rank))\n            S = np.sum(np.sin(circ_rank))\n            W += (C**2 + S**2) / normalized[i].n\n        W *= 2.0\n    else:\n        raise ValueError(\"At least two samples are required for the Wheeler-Watson test.\")\n\n    df = 2 * (k - 1)\n    pval = float(chi2.sf(W, df=df))\n\n    if verbose:\n        print(\"The Wheeler and Watson Two/Multi-Sample Test\")\n        print(\"---------------------------------------------\")\n        print(\"H0: All samples are from populations with the same angle.\")\n        print(\"HA: All samples are not from populations with the same angle.\")\n        print(\"\")\n        print(f\"Test Statistics: {W:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n\n    return WheelerWatsonTestResult(W=float(W), pval=pval, df=df)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.wallraff_test","title":"<code>wallraff_test(samples, angle=0.0, verbose=False)</code>","text":"<p>Wallraff test of angular distances / dispersion against a specified angle.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Sequence[Any]</code> <p>A sequence of <code>Circular</code> objects or one-dimensional array-like radian samples.</p> required <code>angle</code> <code>float</code> <p>A specified angle in radian.</p> <code>0.0</code> <code>verbose</code> <code>bool</code> <p>Print formatted results.</p> <code>False</code> <p>Returns:</p> Type Description <code>WallraffTestResult</code> <p>Dataclass containing the U statistic and p-value.</p> Reference <p>P637-638, Section 27.8, Example 27.13 of Zar, 2010</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def wallraff_test(\n    samples: Sequence[Any],\n    angle: float = 0.0,\n    verbose: bool = False,\n) -&gt; WallraffTestResult:\n    \"\"\"Wallraff test of angular distances / dispersion against a specified angle.\n\n    Parameters\n    ----------\n    samples: sequence\n        A sequence of `Circular` objects or one-dimensional array-like radian samples.\n\n    angle: float\n        A specified angle in radian.\n\n    verbose: bool\n        Print formatted results.\n\n    Returns\n    -------\n    WallraffTestResult\n        Dataclass containing the U statistic and p-value.\n\n    Reference\n    ---------\n    P637-638, Section 27.8, Example 27.13 of Zar, 2010\n    \"\"\"\n\n    normalized = _coerce_circular_samples(samples)\n\n    if len(normalized) != 2:\n        raise ValueError(\"Current implementation only supports two-sample comparison.\")\n\n    angle_arr = np.asarray(angle, dtype=float)\n    if angle_arr.ndim == 0:\n        angles = np.repeat(angle_arr, len(normalized))\n    else:\n        if angle_arr.size != len(normalized):\n            raise ValueError(\"`angle` must be a scalar or have the same length as `samples`.\")\n        angles = angle_arr\n\n    ns = [sample.n for sample in normalized]\n    distances = [angular_distance(normalized[i].alpha, angles[i]) for i in range(len(normalized))]\n\n    rs = rankdata(np.hstack(distances))\n\n    N = np.sum(ns)\n\n    # mann-whitney\n    R1 = np.sum(rs[: ns[0]])\n    U1 = np.prod(ns) + ns[0] * (ns[0] + 1) / 2 - R1\n    U2 = np.prod(ns) - U1\n    U = np.min([U1, U2])\n\n    z = (U - np.prod(ns) / 2 + 0.5) / np.sqrt(np.prod(ns) * (N + 1) / 12)\n    pval = float(2 * norm.sf(abs(z)))\n\n    if verbose:\n        print(\"Wallraff test of angular distances / dispersion\")\n        print(\"-----------------------------------------------\")\n        print(\"H0: The groups have equal dispersion around the specified reference angle.\")\n        print(\"HA: At least one group differs in dispersion around the specified angle.\")\n        print(\"\")\n        print(f\"Test Statistics: {U:.5f}\")\n        print(f\"P-value: {pval:.5f} {significance_code(pval)}\")\n\n    return WallraffTestResult(U=float(U), pval=pval)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.circ_anova","title":"<code>circ_anova(samples, method='F-test', kappa=None, f_mod=True, verbose=False)</code>","text":"<p>Circular Analysis of Variance (ANOVA) for multi-sample comparison of mean directions.</p> <ul> <li>H\u2080: All groups have the same mean direction.</li> <li>H\u2081: At least one group has a different mean direction.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of np.ndarray</code> <p>List of arrays, where each array contains circular data (angles in radians) for a group.</p> required <code>method</code> <code>str</code> <p>The test statistic to use. Options: - <code>\"F-test\"</code> (default): High-concentration F-test (Stephens 1972). - <code>\"LRT\"</code>: Likelihood Ratio Test (Cordeiro et al. 1994).</p> <code>'F-test'</code> <code>kappa</code> <code>float</code> <p>The common concentration parameter (\u03ba). If not specified, it is estimated using MLE.</p> <code>None</code> <code>f_mod</code> <code>bool</code> <p>If <code>True</code>, applies a correction factor <code>(1 + 3/8\u03ba)</code> to the F-statistic.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints the test summary.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <code>CircularAnovaResult</code> <p>Dataclass containing the selected statistic, p-value, and supporting metrics.</p> References <ul> <li>Stephens (1972). Multi-sample tests for the von Mises distribution.</li> <li>Cordeiro, Paula, &amp; Botter (1994). Improved likelihood ratio tests for dispersion models.</li> <li>Jammalamadaka &amp; SenGupta (2001). Topics in Circular Statistics, Section 5.3.</li> </ul> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def circ_anova(\n    samples: list[np.ndarray],\n    method: str = \"F-test\",\n    kappa: Optional[float] = None,\n    f_mod: bool = True,\n    verbose: bool = False,\n) -&gt; CircularAnovaResult:\n    \"\"\"\n    Circular Analysis of Variance (ANOVA) for multi-sample comparison of mean directions.\n\n    - **H\u2080**: All groups have the same mean direction.\n    - **H\u2081**: At least one group has a different mean direction.\n\n    Parameters\n    ----------\n    samples : list of np.ndarray\n        List of arrays, where each array contains circular data (angles in radians) for a group.\n    method : str, optional\n        The test statistic to use. Options:\n        - `\"F-test\"` (default): High-concentration F-test (Stephens 1972).\n        - `\"LRT\"`: Likelihood Ratio Test (Cordeiro et al. 1994).\n    kappa : float, optional\n        The common concentration parameter (\u03ba). If not specified, it is estimated using MLE.\n    f_mod : bool, optional\n        If `True`, applies a correction factor `(1 + 3/8\u03ba)` to the F-statistic.\n    verbose : bool, optional\n        If `True`, prints the test summary.\n\n    Returns\n    -------\n    result : CircularAnovaResult\n        Dataclass containing the selected statistic, p-value, and supporting metrics.\n\n    References\n    ----------\n    - Stephens (1972). Multi-sample tests for the von Mises distribution.\n    - Cordeiro, Paula, &amp; Botter (1994). Improved likelihood ratio tests for dispersion models.\n    - Jammalamadaka &amp; SenGupta (2001). Topics in Circular Statistics, Section 5.3.\n    \"\"\"\n\n    # Number of groups\n    k = len(samples)\n    if k &lt; 2:\n        raise ValueError(\"At least two groups are required for ANOVA.\")\n\n    # Sample sizes, mean directions, and resultants\n    ns = np.array([len(group) for group in samples])\n    Rs = np.array(\n        [circ_r(group) * len(group) for group in samples]\n    )  # Sum of resultant vectors\n    mus = np.array([circ_mean(group) for group in samples])  # Mean directions\n\n    # Overall resultant and mean direction\n    all_samples = np.hstack(samples)\n    N = len(all_samples)\n    R_all = circ_r(all_samples) * N\n    mu_all = circ_mean(all_samples)\n\n    # Estimate \u03ba if not provided\n    if kappa is None:\n        kappa = circ_kappa(R_all / N)\n    kappa_value = float(kappa)\n\n    # **F-test**\n    if method == \"F-test\":\n        # Between-group and within-group sum of squares\n        SS_between = np.sum(Rs) - R_all\n        SS_within = N - np.sum(Rs)\n        SS_total = N - R_all\n\n        df_between = k - 1\n        df_within = N - k\n        df_total = N - 1\n\n        MS_between = SS_between / df_between\n        MS_within = SS_within / df_within\n\n        # Apply correction factor (Stephens 1972)\n        if f_mod:\n            F_stat = (1 + 3 / (8 * kappa)) * (MS_between / MS_within)\n        else:\n            F_stat = MS_between / MS_within\n\n        p_value = 1 - f.cdf(F_stat, df_between, df_within)\n\n        result = CircularAnovaResult(\n            method=\"F-test\",\n            mu=mus,\n            mu_all=float(mu_all),\n            kappa=kappa_value,\n            kappa_all=kappa_value,\n            R=Rs,\n            R_all=float(R_all),\n            df=(df_between, df_within, df_total),\n            statistic=float(F_stat),\n            pval=float(p_value),\n            SS=(float(SS_between), float(SS_within), float(SS_total)),\n            MS=(float(MS_between), float(MS_within)),\n        )\n\n    # **Likelihood Ratio Test (LRT)**\n    elif method == \"LRT\":\n        # Compute test statistic\n        term1 = 1 - (1 / (4 * kappa_value)) * (sum(1 / ns) - 1 / N)\n        term2 = 2 * kappa_value * np.sum(Rs * (1 - np.cos(mus - mu_all)))\n        chi_square_stat = term1 * term2\n\n        df = k - 1\n        p_value = 1 - chi2.cdf(chi_square_stat, df)\n\n        result = CircularAnovaResult(\n            method=\"LRT\",\n            mu=mus,\n            mu_all=float(mu_all),\n            kappa=kappa_value,\n            kappa_all=kappa_value,\n            R=Rs,\n            R_all=float(R_all),\n            df=int(df),\n            statistic=float(chi_square_stat),\n            pval=float(p_value),\n        )\n\n    else:\n        raise ValueError(\"Invalid method. Choose 'F-test' or 'LRT'.\")\n\n    # Print results if verbose is enabled\n    if verbose:\n        print(\"\\nCircular Analysis of Variance (ANOVA)\")\n        print(\"--------------------------------------\")\n        print(f\"Method: {result.method}\")\n        print(f\"Mean Directions (radians): {result.mu}\")\n        print(f\"Overall Mean Direction (radians): {result.mu_all}\")\n        print(f\"Kappa: {result.kappa}\")\n        print(f\"Kappa (overall): {result.kappa_all}\")\n        print(f\"Degrees of Freedom: {result.df}\")\n        print(f\"Test Statistic: {result.statistic:.5f}\")\n        print(f\"P-value: {result.pval:.5f}\")\n        if method == \"F-test\":\n            print(f\"Sum of Squares (Between, Within, Total): {result.SS}\")\n            print(f\"Mean Squares (Between, Within): {result.MS}\")\n        print(\"--------------------------------------\\n\")\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.angular_randomisation_test","title":"<code>angular_randomisation_test(samples, n_simulation=1000, seed=2046, verbose=False)</code>","text":"<p>The Angular Randomization Test (ART) for homogeneity.</p> <ul> <li>H0: The two samples come from the same population.</li> <li>H1: The two samples do not come from the same population.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Sequence[Any]</code> <p>A sequence of <code>Circular</code> objects or one-dimensional array-like radian samples.</p> required <code>n_simulation</code> <code>int</code> <p>Number of permutations for the test. Defaults to 1000.</p> <code>1000</code> <code>seed</code> <code>SeedLike</code> <p>Seed used to initialize the random number generator for the permutation test. Accepts integers, sequences of integers, <code>numpy.random.Generator</code>, <code>numpy.random.BitGenerator</code>, <code>numpy.random.SeedSequence</code> or <code>None</code>. Defaults to 2046.</p> <code>2046</code> <p>Returns:</p> Type Description <code>AngularRandomisationTestResult</code> <p>Dataclass containing the observed statistic and permutation p-value.</p> Reference <p>Jebur, A. J., &amp; Abushilah, S. F. (2022). Distribution-free two-sample homogeneity test for circular data based on geodesic distance. International Journal of Nonlinear Analysis and Applications, 13(1), 2703-2711.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def angular_randomisation_test(\n    samples: Sequence[Any],\n    n_simulation: int = 1000,\n    seed: SeedLike = 2046,\n    verbose: bool = False,\n) -&gt; AngularRandomisationTestResult:\n    \"\"\"The Angular Randomization Test (ART) for homogeneity.\n\n    - H0: The two samples come from the same population.\n    - H1: The two samples do not come from the same population.\n\n    Parameters\n    ----------\n    samples: sequence\n        A sequence of `Circular` objects or one-dimensional array-like radian samples.\n    n_simulation: int, optional\n        Number of permutations for the test. Defaults to 1000.\n    seed: SeedLike\n        Seed used to initialize the random number generator for the permutation test.\n        Accepts integers, sequences of integers, ``numpy.random.Generator``,\n        ``numpy.random.BitGenerator``, ``numpy.random.SeedSequence`` or ``None``.\n        Defaults to 2046.\n\n    Returns\n    -------\n    AngularRandomisationTestResult\n        Dataclass containing the observed statistic and permutation p-value.\n\n    Reference\n    ---------\n    Jebur, A. J., &amp; Abushilah, S. F. (2022).\n    Distribution-free two-sample homogeneity test for circular data based on geodesic distance.\n    International Journal of Nonlinear Analysis and Applications, 13(1), 2703-2711.\n    \"\"\"\n\n    normalized = _coerce_circular_samples(samples)\n\n    if len(normalized) != 2:\n        raise ValueError(\"The Angular Randomization Test requires exactly two samples.\")\n    if n_simulation &lt;= 0:\n        raise ValueError(\"`n_simulation` must be a positive integer.\")\n\n    sample_arrays = [np.asarray(sample.alpha, dtype=float) for sample in normalized]\n    if any(arr.size == 0 for arr in sample_arrays):\n        raise ValueError(\"Each sample must contain at least one observation.\")\n\n    def art_statistic(S1: np.ndarray, S2: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute the Angular Randomisation Test (ART) statistic for two groups of circular data.\n        Following equations (3.1) and (4.2) from Jebur &amp; Abushilah (2022) .\n\n        Args:\n            S1 (np.ndarray): First group of angles in radians (\u03c6 values)\n            S2 (np.ndarray): Second group of angles in radians (\u03c8 values)\n\n        Returns:\n            float: The ART test statistic\n        \"\"\"\n        n = len(S1)\n        m = len(S2)\n\n        # Compute the scaling factor ((n+m)/(nm))^(-1/2)\n        scaling_factor = np.sqrt(n * m / (n + m))\n\n        # Compute sum of all pairwise geodesic distances\n        total_distance = circ_pairdist(S1, S2, metric=\"geodesic\", return_sum=True)\n\n        # Scale the total distance and return\n        return scaling_factor * total_distance\n\n    # 1. Compute observed test statistic T*\u2080\n    observed_stat = art_statistic(sample_arrays[0], sample_arrays[1])\n\n    # Initialize counter for permutations more extreme than observed\n    n_extreme = 1  # Start at 1 to count the observed statistic\n\n    # Combine samples for permutation\n    combined_data = np.concatenate(sample_arrays)\n    n1 = sample_arrays[0].size\n\n    # Perform permutation test\n    if seed is True and verbose is False:\n        warnings.warn(\n            \"Passing `verbose` as a positional argument is deprecated; use keyword arguments.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        verbose = bool(seed)\n        seed = 2046\n\n    rng = _init_rng(seed)\n\n    for _ in range(n_simulation):\n        # Randomly permute the combined data\n        permuted_data = rng.permutation(combined_data)\n\n        # Split into two groups of original sizes\n        perm_S1 = permuted_data[:n1]\n        perm_S2 = permuted_data[n1:]\n\n        # Compute test statistic for this permutation\n        perm_stat = art_statistic(perm_S1, perm_S2)\n\n        # Count if permuted statistic is &gt;= observed (one-sided test)\n        if perm_stat &gt;= observed_stat:\n            n_extreme += 1\n\n    # Compute p-value as in equation (4.3)\n    p_value = n_extreme / (n_simulation + 1)\n\n    if verbose:\n        print(\"Angular Randomization Test (ART) for Homogeneity\")\n        print(\"-------------------------------------------------\")\n        print(\"H0: The two samples come from the same population.\")\n        print(\"HA: The two samples do not come from the same population.\")\n        print(\"\")\n        print(f\"Observed Test Statistic: {observed_stat:.5f}\")\n        print(f\"P-value: {p_value:.5f} {significance_code(p_value)}\")\n\n    return AngularRandomisationTestResult(statistic=float(observed_stat), pval=float(p_value), n_simulation=n_simulation)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.kuiper_test","title":"<code>kuiper_test(alpha, n_simulation=9999, seed=2046, verbose=False)</code>","text":"<p>Kuiper's test for Circular Uniformity.</p> <ul> <li>H0: The data in the population are distributed uniformly around the circle.</li> <li>H1: THe data in the population are not disbutrited uniformly around the circle.</li> </ul> <p>This method is for ungrouped data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>n_simulation</code> <code>int</code> <p>Number of simulation for the p-value. If n_simulation=1, the p-value is asymptotically approximated. If n_simulation&gt;1, the p-value is simulated. Default is 9999.</p> <code>9999</code> <code>seed</code> <code>SeedLike</code> <p>Seed used to initialize the random number generator for the simulation-based p-value. Accepts integers, sequences of integers, <code>numpy.random.Generator</code>, <code>numpy.random.BitGenerator</code>, <code>numpy.random.SeedSequence</code> or <code>None</code>. Defaults to 2046.</p> <code>2046</code> <p>Returns:</p> Type Description <code>KuiperTestResult</code> <p>Dataclass containing the Kuiper statistic, p-value, simulation mode, and count.</p> Note <p>Implementation from R package <code>Directional</code> https://rdrr.io/cran/Directional/src/R/kuiper.R</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def kuiper_test(\n    alpha: np.ndarray,\n    n_simulation: int = 9999,\n    seed: SeedLike = 2046,\n    verbose: bool = False,\n) -&gt; KuiperTestResult:\n    \"\"\"\n    Kuiper's test for Circular Uniformity.\n\n    - H0: The data in the population are distributed uniformly around the circle.\n    - H1: THe data in the population are not disbutrited uniformly around the circle.\n\n    This method is for ungrouped data.\n\n    Parameters\n    ----------\n\n    alpha: np.array\n        Angles in radian.\n\n    n_simulation: int\n        Number of simulation for the p-value.\n        If n_simulation=1, the p-value is asymptotically approximated.\n        If n_simulation&gt;1, the p-value is simulated.\n        Default is 9999.\n\n    seed: SeedLike\n        Seed used to initialize the random number generator for the simulation-based\n        p-value. Accepts integers, sequences of integers, ``numpy.random.Generator``,\n        ``numpy.random.BitGenerator``, ``numpy.random.SeedSequence`` or ``None``.\n        Defaults to 2046.\n\n    Returns\n    -------\n    KuiperTestResult\n        Dataclass containing the Kuiper statistic, p-value, simulation mode, and count.\n\n    Note\n    ----\n    Implementation from R package `Directional`\n    https://rdrr.io/cran/Directional/src/R/kuiper.R\n    \"\"\"\n\n    if n_simulation &lt;= 0:\n        raise ValueError(\"`n_simulation` must be a positive integer.\")\n\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    def compute_V(sample):\n        ordered = np.sort(sample) / (2 * np.pi)\n        n = ordered.size\n        indices = np.arange(1, n + 1, dtype=float)\n\n        D_plus = np.max(indices / n - ordered)\n        D_minus = np.max(ordered - (indices - 1) / n)\n        f = np.sqrt(n) + 0.155 + 0.24 / np.sqrt(n)\n        V = f * (D_plus + D_minus)\n        return float(V), float(f)\n\n    n = alpha.size\n    Vo, f = compute_V(alpha)\n\n    if seed is True and verbose is False:\n        warnings.warn(\n            \"Passing `verbose` as a positional argument is deprecated; use keyword arguments.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        verbose = bool(seed)\n        seed = 2046\n\n    if n_simulation == 1:\n        # asymptotic p-value\n        mode = \"asymptotic\"\n        m = (np.arange(1, 50, dtype=float)) ** 2\n        a1 = 4 * m * Vo**2\n        a2 = np.exp(-2 * m * Vo**2)\n        b1 = 2 * (a1 - 1) * a2\n        b2 = 8 * Vo / (3 * f) * m * (a1 - 3) * a2\n        pval = float(np.sum(b1 - b2))\n    else:\n        mode = \"simulation\"\n        rng = _init_rng(seed)\n        uniforms = rng.uniform(low=0.0, high=2 * np.pi, size=(n, n_simulation))\n        x = np.sort(uniforms, axis=0)\n        Vs = np.array([compute_V(x[:, i])[0] for i in range(n_simulation)])\n        pval = float((np.count_nonzero(Vs &gt;= Vo) + 1) / (n_simulation + 1))\n\n    if verbose:\n        print(\"Kuiper's Test of Circular Uniformity\")\n        print(\"------------------------------------\")\n        print(\"H0: The sample is drawn from a circularly uniform distribution.\")\n        print(\"HA: The sample is not drawn from a circularly uniform distribution.\")\n        print(\"\")\n        print(f\"Test Statistic: {Vo:.4f}\")\n        print(f\"P-value = {pval} {significance_code(pval)}\")\n\n    return KuiperTestResult(V=float(Vo), pval=float(pval), mode=mode, n_simulation=n_simulation)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.watson_test","title":"<code>watson_test(alpha, n_simulation=9999, seed=2046, verbose=False)</code>","text":"<p>Watson's Goodness-of-Fit Testing, aka Watson one-sample U2 test.</p> <ul> <li>H0: The sample data come from a population distributed uniformly around the circle.</li> <li>H1: The sample data do not come from a population distributed uniformly around the circle.</li> </ul> <p>This method is for ungrouped data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>n_simulation</code> <code>int</code> <p>Number of simulation for the p-value. If n_simulation=1, the p-value is asymptotically approximated. If n_simulation&gt;1, the p-value is simulated.</p> <code>9999</code> <code>seed</code> <code>SeedLike</code> <p>Seed used to initialize the random number generator for the simulation-based p-value. Accepts integers, sequences of integers, <code>numpy.random.Generator</code>, <code>numpy.random.BitGenerator</code>, <code>numpy.random.SeedSequence</code> or <code>None</code>. Defaults to 2046.</p> <code>2046</code> <p>Returns:</p> Type Description <code>WatsonTestResult</code> <p>Dataclass containing the Watson U\u00b2 statistic, p-value, and simulation details.</p> Note <p>Implementation from R package <code>Directional</code> https://rdrr.io/cran/Directional/src/R/watson.R</p> <p>The code for simulated p-value in Directional (v5.7) seems to be just copied from kuiper(), thus yield in wrong results.</p> See Also <p>kuiper_test(); rao_spacing_test()</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def watson_test(\n    alpha: np.ndarray,\n    n_simulation: int = 9999,\n    seed: SeedLike = 2046,\n    verbose: bool = False,\n) -&gt; WatsonTestResult:\n    \"\"\"\n    Watson's Goodness-of-Fit Testing, aka Watson one-sample U2 test.\n\n    - H0: The sample data come from a population distributed uniformly around the circle.\n    - H1: The sample data do not come from a population distributed uniformly around the circle.\n\n    This method is for ungrouped data.\n\n    Parameters\n    ----------\n\n    alpha: np.array\n        Angles in radian.\n\n    n_simulation: int\n        Number of simulation for the p-value.\n        If n_simulation=1, the p-value is asymptotically approximated.\n        If n_simulation&gt;1, the p-value is simulated.\n\n    seed: SeedLike\n        Seed used to initialize the random number generator for the simulation-based\n        p-value. Accepts integers, sequences of integers, ``numpy.random.Generator``,\n        ``numpy.random.BitGenerator``, ``numpy.random.SeedSequence`` or ``None``.\n        Defaults to 2046.\n\n    Returns\n    -------\n    WatsonTestResult\n        Dataclass containing the Watson U\u00b2 statistic, p-value, and simulation details.\n\n    Note\n    ----\n    Implementation from R package `Directional`\n    https://rdrr.io/cran/Directional/src/R/watson.R\n\n    The code for simulated p-value in Directional (v5.7) seems to be just copied from\n    kuiper(), thus yield in wrong results.\n\n    See Also\n    --------\n    kuiper_test(); rao_spacing_test()\n    \"\"\"\n\n    if n_simulation &lt;= 0:\n        raise ValueError(\"`n_simulation` must be a positive integer.\")\n\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    def compute_U2(sample):\n        ordered = np.sort(sample)\n        n = ordered.size\n        indices = np.arange(1, n + 1, dtype=float)\n\n        u = ordered / (2 * np.pi)\n        U2 = np.sum(((u - (indices - 0.5) / n) - (np.sum(u) / n - 0.5)) ** 2) + 1 / (12 * n)\n        return float(U2)\n\n    n = alpha.size\n    U2o = compute_U2(alpha)\n\n    if seed is True and verbose is False:\n        warnings.warn(\n            \"Passing `verbose` as a positional argument is deprecated; use keyword arguments.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        verbose = bool(seed)\n        seed = 2046\n\n    if n_simulation == 1:\n        mode = \"asymptotic\"\n        m = np.arange(1, 51)\n        pval = float(2 * sum((-1) ** (m - 1) * np.exp(-2 * m**2 * np.pi**2 * U2o)))\n    else:\n        mode = \"simulation\"\n        rng = _init_rng(seed)\n        uniforms = rng.uniform(low=0.0, high=2 * np.pi, size=(n, n_simulation))\n        x = np.sort(uniforms, axis=0)\n        U2s = np.array([compute_U2(x[:, i]) for i in range(n_simulation)])\n        pval = float((np.count_nonzero(U2s &gt;= U2o) + 1) / (n_simulation + 1))\n\n    if verbose:\n        print(\"Watson's One-Sample U2 Test of Circular Uniformity\")\n        print(\"--------------------------------------------------\")\n        print(\"H0: The sample is drawn from a circularly uniform distribution.\")\n        print(\"HA: The sample is not drawn from a circularly uniform distribution.\")\n        print(\"\")\n        print(f\"Test Statistic: {U2o:.4f}\")\n        print(f\"P-value = {pval} {significance_code(pval)}\")\n\n    return WatsonTestResult(U2=float(U2o), pval=float(pval), mode=mode, n_simulation=n_simulation)\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.rao_spacing_test","title":"<code>rao_spacing_test(alpha, w=None, kappa=1000.0, n_simulation=9999, seed=2046, verbose=False)</code>","text":"<p>Simulation based Rao's spacing test.</p> <ul> <li>H0: The sample data come from a population distributed uniformly around the circle.</li> <li>H1: The sample data do not come from a population distributed uniformly around the circle.</li> </ul> <p>This method is for both grouped and ungrouped data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radian.</p> required <code>w</code> <code>Union[ndarray, None]</code> <p>Frequencies</p> <code>None</code> <code>kappa</code> <code>float</code> <p>Concentration parameter. Only use for grouped data.</p> <code>1000.0</code> <code>n_simulation</code> <code>int</code> <p>Number of simulations.</p> <code>9999</code> <code>seed</code> <code>SeedLike</code> <p>Seed used to initialize the random number generator for the simulation-based p-value. Accepts integers, sequences of integers, <code>numpy.random.Generator</code>, <code>numpy.random.BitGenerator</code>, <code>numpy.random.SeedSequence</code> or <code>None</code>. Defaults to 2046.</p> <code>2046</code> <p>Returns:</p> Type Description <code>RaoSpacingTestResult</code> <p>Dataclass containing the Rao spacing statistic (degrees), p-value, method, and simulation count.</p> Reference <p>Landler et al. (2019) https://movementecologyjournal.biomedcentral.com/articles/10.1186/s40462-019-0160-x</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def rao_spacing_test(\n    alpha: np.ndarray,\n    w: Union[np.ndarray, None] = None,\n    kappa: float = 1000.0,\n    n_simulation: int = 9999,\n    seed: SeedLike = 2046,\n    verbose: bool = False,\n) -&gt; RaoSpacingTestResult:\n    \"\"\"Simulation based Rao's spacing test.\n\n    - H0: The sample data come from a population distributed uniformly around the circle.\n    - H1: The sample data do not come from a population distributed uniformly around the circle.\n\n    This method is for both grouped and ungrouped data.\n\n    Parameters\n    ----------\n    alpha: np.ndarray\n        Angles in radian.\n\n    w: np.ndarray or None\n        Frequencies\n\n    kappa: float\n        Concentration parameter. Only use for grouped data.\n\n    n_simulation: int\n        Number of simulations.\n\n    seed: SeedLike\n        Seed used to initialize the random number generator for the simulation-based\n        p-value. Accepts integers, sequences of integers, ``numpy.random.Generator``,\n        ``numpy.random.BitGenerator``, ``numpy.random.SeedSequence`` or ``None``.\n        Defaults to 2046.\n\n    Returns\n    -------\n    RaoSpacingTestResult\n        Dataclass containing the Rao spacing statistic (degrees), p-value, method, and simulation count.\n\n    Reference\n    ---------\n    Landler et al. (2019)\n    https://movementecologyjournal.biomedcentral.com/articles/10.1186/s40462-019-0160-x\n    \"\"\"\n\n    if n_simulation &lt;= 0:\n        raise ValueError(\"`n_simulation` must be a positive integer.\")\n\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    def compute_U(sample):\n        ordered = np.sort(sample)\n        n_local = ordered.size\n        spacings = np.hstack([ordered[1:] - ordered[:-1], 2 * np.pi - ordered[-1] + ordered[0]])\n        return 0.5 * np.sum(np.abs(spacings - (2 * np.pi / n_local)))\n\n    if w is not None:\n        w = np.asarray(w, dtype=float)\n        if np.any(w &lt; 0):\n            raise ValueError(\"`w` must contain non-negative frequencies.\")\n        if not np.all(np.isclose(w, np.round(w))):\n            raise ValueError(\"`w` must contain integer frequencies.\")\n        w = w.astype(int)\n        if w.shape != alpha.shape:\n            raise ValueError(\"`w` must have the same shape as `alpha`.\")\n        n = int(np.sum(w))\n        if n &lt;= 0:\n            raise ValueError(\"Sum of weights must be positive.\")\n        m = alpha.size\n        expanded_alpha = np.repeat(alpha, w)\n        mode = \"grouped\"\n    else:\n        expanded_alpha = alpha\n        n = expanded_alpha.size\n        mode = \"ungrouped\"\n\n    if seed is True and verbose is False:\n        warnings.warn(\n            \"Passing `verbose` as a positional argument is deprecated; use keyword arguments.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        verbose = bool(seed)\n        seed = 2046\n\n    rng = _init_rng(seed)\n\n    Uo = compute_U(expanded_alpha)\n    if w is not None:  # noncontinuous / grouped data\n        vm_dist = vonmises(kappa=kappa)\n        uniforms = rng.uniform(low=0.0, high=2 * np.pi, size=(n_simulation, n))\n        snapped = np.floor(uniforms * m / (2 * np.pi)) * (2 * np.pi / m)\n        noise = vm_dist.rvs(size=(n_simulation, n), random_state=rng)\n        samples = angmod(snapped + noise)\n        Us = np.array([compute_U(sample) for sample in samples])\n    else:\n        samples = rng.uniform(low=0.0, high=2 * np.pi, size=(n_simulation, n))\n        Us = np.array([compute_U(sample) for sample in samples])\n\n    counter = np.count_nonzero(Us &gt;= Uo)\n    pval = float((counter + 1) / (n_simulation + 1))\n\n    if verbose:\n        print(\"Rao's Spacing Test of Circular Uniformity\")\n        print(\"-----------------------------------------\")\n        print(\"H0: The sample is drawn from a circularly uniform distribution.\")\n        print(\"HA: The sample is not drawn from a circularly uniform distribution.\")\n        print(\"\")\n        print(f\"Test Statistic: {Uo:.4f}\")\n        print(f\"P-value = {pval}\\n\")\n\n    return RaoSpacingTestResult(\n        statistic=float(np.rad2deg(Uo)),\n        pval=float(pval),\n        mode=mode,\n        n_simulation=n_simulation,\n    )\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.circ_range_test","title":"<code>circ_range_test(alpha, verbose=False)</code>","text":"<p>Perform the Circular Range Test for uniformity.</p> <ul> <li>H0: The data is uniformly distributed around the circle.</li> <li>H1: The data is non-uniformly distributed (clustered).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in radians. Values must already be wrapped into <code>[-2\u03c0, 2\u03c0]</code>.</p> required <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints test details and results.</p> <code>False</code> <p>Returns:</p> Type Description <code>CircularRangeTestResult</code> <p>Dataclass containing the range statistic and corresponding p-value.</p> Reference <p>P162, Section 7.2.3 of Jammalamadaka, S. Rao and SenGupta, A. (2001)</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def circ_range_test(alpha: np.ndarray, verbose: bool = False) -&gt; CircularRangeTestResult:\n    \"\"\"\n    Perform the Circular Range Test for uniformity.\n\n    - **H0**: The data is uniformly distributed around the circle.\n    - **H1**: The data is non-uniformly distributed (clustered).\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Angles in radians. Values must already be wrapped into ``[-2\u03c0, 2\u03c0]``.\n    verbose : bool, optional\n        If ``True``, prints test details and results.\n\n    Returns\n    -------\n    CircularRangeTestResult\n        Dataclass containing the range statistic and corresponding p-value.\n\n    Reference\n    ---------\n    P162, Section 7.2.3 of Jammalamadaka, S. Rao and SenGupta, A. (2001)\n    \"\"\"\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    if np.any(np.abs(alpha) &gt; 2 * np.pi + 1e-8):\n        raise ValueError(\"`alpha` must be provided in radians within [-2\u03c0, 2\u03c0].\")\n\n    range_stat = circ_range(alpha)  # Compute test statistic\n\n    # Compute p-value using approximation formula from CircStats (if available)\n    n = alpha.size\n    stop = int(np.floor(1 / (1 - range_stat / (2 * np.pi))))\n    index = np.arange(1, stop + 1)\n\n    # Compute p-value using series expansion\n    sequence = (\n        ((-1) ** (index - 1))\n        * comb(n, index)\n        * (1 - index * (1 - range_stat / (2 * np.pi))) ** (n - 1)\n    )\n    p_value = float(np.sum(sequence))\n\n    result = CircularRangeTestResult(range_stat=float(range_stat), pval=float(p_value))\n\n    if verbose:\n        range_deg = float(np.rad2deg(result.range_stat))\n        print(\"Circular Range Test of Uniformity\")\n        print(\"---------------------------------\")\n        print(\"H0: The sample is uniformly distributed around the circle.\")\n        print(\"HA: The sample exhibits clustering (non-uniformity).\")\n        print(\"\")\n        print(f\"Sample size: {n}\")\n        print(f\"Range statistic: {result.range_stat:.5f} rad ({range_deg:.2f}\u00b0)\")\n        print(f\"P-value: {result.pval:.5g} {significance_code(result.pval)}\")\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.binomial_test","title":"<code>binomial_test(alpha, md, verbose=False)</code>","text":"<p>Perform the binomial test for the median direction of circular data.</p> <p>This test evaluates whether the population median angle is equal to a specified value.</p> <ul> <li>H0: The population has median angle <code>md</code>.</li> <li>H1: The population does not have median angle <code>md</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>md</code> <code>float</code> <p>Hypothesized median angle.</p> required <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints test details and results.</p> <code>False</code> <p>Returns:</p> Type Description <code>BinomialTestResult</code> <p>Dataclass containing the p-value and counts on each side of the hypothesized median.</p> References <p>Zar, J. H. (2010). Biostatistical Analysis. Section 27.4.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def binomial_test(\n    alpha: np.ndarray,\n    md: float,\n    verbose: bool = False,\n) -&gt; BinomialTestResult:\n    \"\"\"\n    Perform the binomial test for the median direction of circular data.\n\n    This test evaluates whether the population median angle is equal to a specified value.\n\n    - **H0**: The population has median angle `md`.\n    - **H1**: The population does not have median angle `md`.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Sample of angles in radians.\n    md : float\n        Hypothesized median angle.\n    verbose : bool, optional\n        If ``True``, prints test details and results.\n\n    Returns\n    -------\n    BinomialTestResult\n        Dataclass containing the p-value and counts on each side of the hypothesized median.\n\n    References\n    ----------\n    Zar, J. H. (2010). Biostatistical Analysis. Section 27.4.\n    \"\"\"\n    from scipy.stats import binom\n\n    alpha = np.asarray(alpha, dtype=float)\n    if alpha.size == 0:\n        raise ValueError(\"`alpha` must contain at least one angle.\")\n\n    if np.ndim(md) != 0:\n        raise ValueError(\"The median (md) must be a single scalar value.\")\n\n    # Compute circular differences from hypothesized median\n    d = circ_dist(alpha, float(md))\n\n    # Count the number of angles on each side of the hypothesized median\n    n1 = int(np.sum(d &lt; 0))\n    n2 = int(np.sum(d &gt; 0))\n    n_eff = int(n1 + n2)\n    if n_eff == 0:\n        result = BinomialTestResult(pval=1.0, n_eff=0, n1=n1, n2=n2)\n    else:\n        # Compute p-value using binomial test\n        n_min = int(min(n1, n2))\n        pval = float(2 * binom.cdf(n_min, n_eff, 0.5))\n        pval = min(pval, 1.0)\n        result = BinomialTestResult(pval=pval, n_eff=n_eff, n1=n1, n2=n2)\n\n    if verbose:\n        print(\"Circular Binomial Test for Median Direction\")\n        print(\"--------------------------------------------\")\n        print(f\"H0: Median direction equals {float(md):.5f} rad.\")\n        print(\"HA: Median direction differs from the hypothesized value.\")\n        print(\"\")\n        print(f\"Effective sample size: {result.n_eff}\")\n        print(f\"Counts below/above median: n1 = {result.n1}, n2 = {result.n2}\")\n        print(f\"P-value: {result.pval:.5f} {significance_code(result.pval)}\")\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.concentration_test","title":"<code>concentration_test(alpha1, alpha2, verbose=False)</code>","text":"<p>Parametric two-sample test for concentration equality in circular data.</p> <p>This test determines whether two von Mises-type samples have different concentration parameters (i.e., different dispersions).</p> <ul> <li>H0: The two samples have the same concentration parameter.</li> <li>H1: The two samples have different concentration parameters.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>alpha1</code> <code>ndarray</code> <p>First sample of circular data (radians).</p> required <code>alpha2</code> <code>ndarray</code> <p>Second sample of circular data (radians).</p> required <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints test details and results.</p> <code>False</code> <p>Returns:</p> Type Description <code>ConcentrationTestResult</code> <p>Dataclass with the F statistic, p-value, and associated degrees of freedom.</p> Notes <ul> <li>This test assumes that both samples follow von Mises distributions.</li> <li>The resultant vector length of the combined samples should be greater than 0.7 for validity.</li> <li>Based on Batschelet (1980), Section 6.9, p. 122-124.</li> </ul> References <p>Batschelet, E. (1980). Circular Statistics in Biology. Academic Press.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def concentration_test(\n    alpha1: np.ndarray,\n    alpha2: np.ndarray,\n    verbose: bool = False,\n) -&gt; ConcentrationTestResult:\n    \"\"\"\n    Parametric two-sample test for concentration equality in circular data.\n\n    This test determines whether two von Mises-type samples have different\n    concentration parameters (i.e., different dispersions).\n\n    - **H0**: The two samples have the same concentration parameter.\n    - **H1**: The two samples have different concentration parameters.\n\n    Parameters\n    ----------\n    alpha1 : np.ndarray\n        First sample of circular data (radians).\n    alpha2 : np.ndarray\n        Second sample of circular data (radians).\n    verbose : bool, optional\n        If ``True``, prints test details and results.\n\n    Returns\n    -------\n    ConcentrationTestResult\n        Dataclass with the F statistic, p-value, and associated degrees of freedom.\n\n    Notes\n    -----\n    - This test assumes that both samples follow von Mises distributions.\n    - The **resultant vector length** of the combined samples should be greater than 0.7 for validity.\n    - Based on Batschelet (1980), Section 6.9, p. 122-124.\n\n    References\n    ----------\n    Batschelet, E. (1980). Circular Statistics in Biology. Academic Press.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    alpha1 = np.asarray(alpha1, dtype=float)\n    alpha2 = np.asarray(alpha2, dtype=float)\n\n    # Sample sizes\n    n1, n2 = len(alpha1), len(alpha2)\n    if min(n1, n2) &lt; 2:\n        raise ValueError(\"Both samples must contain at least two observations.\")\n\n    # Compute resultant vector lengths\n    R1 = n1 * circ_r(alpha1)\n    R2 = n2 * circ_r(alpha2)\n\n    # Compute mean resultant length of combined samples\n    rbar = (R1 + R2) / (n1 + n2)\n\n    # Warn if rbar is too low\n    if rbar &lt; 0.7:\n        warnings.warn(\n            \"The resultant vector length should exceed 0.7 for the concentration test to be reliable.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # Compute F-statistic\n    df1 = n1 - 1\n    df2 = n2 - 1\n    numerator = df2 * (n1 - R1)\n    denominator = df1 * (n2 - R2)\n    if denominator &lt;= 0 or numerator &lt;= 0:\n        raise ValueError(\"Degenerate data: cannot compute concentration test statistic.\")\n    f_stat = numerator / denominator\n\n    # Compute p-value (adjusting for F-stat symmetry)\n    if f_stat &gt;= 1:\n        pval = 2 * f.sf(f_stat, df1, df2)\n    else:\n        pval = 2 * f.sf(1 / f_stat, df2, df1)\n\n    result = ConcentrationTestResult(\n        f_stat=float(f_stat),\n        pval=float(min(pval, 1.0)),\n        df1=int(df1),\n        df2=int(df2),\n    )\n\n    if verbose:\n        print(\"Concentration Equality Test\")\n        print(\"---------------------------\")\n        print(\"H0: Both samples share the same concentration parameter (\u03ba).\")\n        print(\"HA: The samples have different concentration parameters.\")\n        print(\"\")\n        print(f\"Sample sizes: n1 = {n1}, n2 = {n2}\")\n        print(\n            f\"F statistic: {result.f_stat:.5f} \"\n            f\"(df1 = {result.df1}, df2 = {result.df2})\"\n        )\n        print(f\"P-value: {result.pval:.5f} {significance_code(result.pval)}\")\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.rao_homogeneity_test","title":"<code>rao_homogeneity_test(samples, alpha=0.05, verbose=False)</code>","text":"<p>Perform Rao's test for homogeneity on multiple samples of angular data.</p> <ul> <li>Test 1: Equality of Mean Directions (Polar Vectors)</li> <li>Test 2: Equality of Dispersions</li> </ul> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of np.ndarray</code> <p>A list where each entry is a vector of angular values (in radians).</p> required <code>alpha</code> <code>float</code> <p>Significance level for the hypothesis test. Default is 0.05.</p> <code>0.05</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints test details and decisions.</p> <code>False</code> <p>Returns:</p> Type Description <code>RaoHomogeneityTestResult</code> <p>Dataclass containing test statistics, p-values, and rejection flags.</p> References <p>Jammalamadaka, S. Rao and SenGupta, A. (2001). Topics in Circular Statistics, Section 7.6.1. Rao, J.S. (1967). Large sample tests for the homogeneity of angular data, Sankhya, Ser, B., 28.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def rao_homogeneity_test(\n    samples: list,\n    alpha: float = 0.05,\n    verbose: bool = False,\n) -&gt; RaoHomogeneityTestResult:\n    \"\"\"\n    Perform Rao's test for homogeneity on multiple samples of angular data.\n\n    - **Test 1**: Equality of Mean Directions (Polar Vectors)\n    - **Test 2**: Equality of Dispersions\n\n    Parameters\n    ----------\n    samples : list of np.ndarray\n        A list where each entry is a vector of angular values (in radians).\n    alpha : float, optional\n        Significance level for the hypothesis test. Default is 0.05.\n    verbose : bool, optional\n        If ``True``, prints test details and decisions.\n\n    Returns\n    -------\n    RaoHomogeneityTestResult\n        Dataclass containing test statistics, p-values, and rejection flags.\n\n    References\n    ----------\n    Jammalamadaka, S. Rao and SenGupta, A. (2001). Topics in Circular Statistics, Section 7.6.1.\n    Rao, J.S. (1967). Large sample tests for the homogeneity of angular data, Sankhya, Ser, B., 28.\n    \"\"\"\n    if not isinstance(samples, list) or not all(\n        isinstance(s, np.ndarray) for s in samples\n    ):\n        raise ValueError(\"Input must be a list of numpy arrays.\")\n\n    k = len(samples)  # Number of samples\n    n = np.array([len(s) for s in samples])  # Sample sizes\n\n    # Compute mean cosine and sine values for each sample\n    cos_means = np.array([np.mean(np.cos(s)) for s in samples])\n    sin_means = np.array([np.mean(np.sin(s)) for s in samples])\n\n    # Compute variances\n    # Compute sample variances (use ddof=1 to match R)\n    var_cos = np.array([np.var(np.cos(s), ddof=1) for s in samples])\n    var_sin = np.array([np.var(np.sin(s), ddof=1) for s in samples])\n\n    # Compute covariance (use ddof=1 to match R's var(x, y))\n    cov_cos_sin = np.array(\n        [np.cov(np.cos(s), np.sin(s), ddof=1)[0, 1] for s in samples]\n    )\n\n    # Compute test statistics\n    s_polar = (\n        1\n        / n\n        * (\n            var_sin / cos_means**2\n            + (sin_means**2 * var_cos) / cos_means**4\n            - (2 * sin_means * cov_cos_sin) / cos_means**3\n        )\n    )\n    tan_means = sin_means / cos_means\n    H_polar = np.sum(tan_means**2 / s_polar) - (\n        np.sum(tan_means / s_polar) ** 2\n    ) / np.sum(1 / s_polar)\n\n    U = cos_means**2 + sin_means**2\n    s_disp = (\n        4\n        / n\n        * (\n            cos_means**2 * var_cos\n            + sin_means**2 * var_sin\n            + 2 * cos_means * sin_means * cov_cos_sin\n        )\n    )\n    H_disp = np.sum(U**2 / s_disp) - (np.sum(U / s_disp) ** 2) / np.sum(1 / s_disp)\n\n    # Compute p-values\n    df = k - 1  # Degrees of freedom\n    pval_polar = 1 - chi2.cdf(H_polar, df)\n    pval_disp = 1 - chi2.cdf(H_disp, df)\n\n    # Determine critical values\n    crit_polar = chi2.ppf(1 - alpha, df)\n    crit_disp = chi2.ppf(1 - alpha, df)\n\n    # Test decisions\n    reject_polar = H_polar &gt; crit_polar\n    reject_disp = H_disp &gt; crit_disp\n\n    result = RaoHomogeneityTestResult(\n        H_polar=float(H_polar),\n        pval_polar=float(pval_polar),\n        reject_polar=bool(reject_polar),\n        H_disp=float(H_disp),\n        pval_disp=float(pval_disp),\n        reject_disp=bool(reject_disp),\n    )\n\n    if verbose:\n        print(\"Rao's Homogeneity Test\")\n        print(\"----------------------\")\n        print(\"Test 1 H0: All groups share the same mean direction.\")\n        print(\"Test 2 H0: All groups share the same dispersion.\")\n        print(\"\")\n        print(\n            f\"Mean directions: H = {result.H_polar:.5f}, \"\n            f\"p = {result.pval_polar:.5f} {significance_code(result.pval_polar)}; \"\n            f\"reject @ \u03b1={alpha}: {result.reject_polar}\"\n        )\n        print(\n            f\"Dispersions:     H = {result.H_disp:.5f}, \"\n            f\"p = {result.pval_disp:.5f} {significance_code(result.pval_disp)}; \"\n            f\"reject @ \u03b1={alpha}: {result.reject_disp}\"\n        )\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.change_point_test","title":"<code>change_point_test(alpha, verbose=False)</code>","text":"<p>Perform a change point test for mean direction, concentration, or both.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Vector of angular measurements in radians.</p> required <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints test details and summary statistics.</p> <code>False</code> <p>Returns:</p> Type Description <code>ChangePointTestResult</code> <p>Dataclass containing the change point statistics.</p> References <p>Jammalamadaka, S. Rao and SenGupta, A. (2001). Topics in Circular Statistics, Chapter 11.</p> Notes <p>Ported from <code>change.pt()</code> function in the <code>CircStats</code> package for R.</p> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def change_point_test(alpha, verbose: bool = False) -&gt; ChangePointTestResult:\n    \"\"\"\n    Perform a change point test for mean direction, concentration, or both.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Vector of angular measurements in radians.\n    verbose : bool, optional\n        If ``True``, prints test details and summary statistics.\n\n    Returns\n    -------\n    ChangePointTestResult\n        Dataclass containing the change point statistics.\n\n    References\n    ----------\n    Jammalamadaka, S. Rao and SenGupta, A. (2001). Topics in Circular Statistics, Chapter 11.\n\n    Notes\n    -----\n    Ported from `change.pt()` function in the `CircStats` package for R.\n    \"\"\"\n\n    def phi(x):\n        \"\"\"Helper function for phi computation.\"\"\"\n        inv = A1inv(x)\n        bessel = i0(inv)\n        if np.isinf(bessel):\n            corr = (\n                inv\n                + np.log(\n                    1\n                    / np.sqrt(2 * np.pi * inv)\n                    * (1 + 1 / (8 * inv) + 9 / (128 * inv**2) + 225 / (1024 * inv**3))\n                )\n            )\n        else:\n            corr = np.log(bessel)\n        return x * inv - corr\n\n    def est_rho(alpha):\n        \"\"\"Estimate mean resultant length (rho).\"\"\"\n        return np.linalg.norm(np.sum(np.exp(1j * alpha))) / len(alpha)\n\n    n = len(alpha)\n    if n &lt; 4:\n        raise ValueError(\"Sample size must be at least 4 for change point test.\")\n\n    rho = est_rho(alpha)\n\n    R1, R2, V = np.zeros(n), np.zeros(n), np.zeros(n)\n\n    for k in range(1, n):\n        R1[k - 1] = est_rho(alpha[:k]) * k\n        R2[k - 1] = est_rho(alpha[k:]) * (n - k)\n\n        if 2 &lt;= k &lt;= (n - 2):\n            V[k - 1] = (k / n) * phi(R1[k - 1] / k) + ((n - k) / n) * phi(\n                R2[k - 1] / (n - k)\n            )\n\n    R1[-1] = rho * n\n    R2[-1] = 0\n\n    R_diff = R1 + R2 - rho * n\n    rmax = np.max(R_diff)\n    k_r = np.argmax(R_diff)\n    rave = np.mean(R_diff)\n\n    if n &gt; 3:\n        V = V[1 : n - 2]\n        tmax = np.max(V)\n        k_t = np.argmax(V) + 1\n        tave = np.mean(V)\n    else:\n        raise ValueError(\"Sample size must be at least 4.\")\n\n    result = ChangePointTestResult(\n        n=int(n),\n        rho=float(rho),\n        rmax=float(rmax),\n        k_r=int(k_r),\n        rave=float(rave),\n        tmax=float(tmax),\n        k_t=int(k_t),\n        tave=float(tave),\n    )\n\n    if verbose:\n        print(\"Circular Change Point Test\")\n        print(\"--------------------------\")\n        print(\"H0: No change point in mean direction or concentration.\")\n        print(\"HA: A change point is present in the sequence.\")\n        print(\"\")\n        print(f\"Sample size: {result.n}\")\n        print(f\"Overall resultant length (\u03c1): {result.rho:.5f}\")\n        print(f\"Max R statistic: {result.rmax:.5f} at k = {result.k_r}\")\n        print(f\"Average R statistic: {result.rave:.5f}\")\n        print(f\"Max T statistic: {result.tmax:.5f} at k = {result.k_t}\")\n        print(f\"Average T statistic: {result.tave:.5f}\")\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.harrison_kanji_test","title":"<code>harrison_kanji_test(alpha, idp, idq, inter=True, fn=None, verbose=False)</code>","text":"<p>Harrison-Kanji Test (Two-Way ANOVA) for Circular Data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angular measurements (radians).</p> required <code>idp</code> <code>ndarray</code> <p>Factor A identifiers for each observation.</p> required <code>idq</code> <code>ndarray</code> <p>Factor B identifiers for each observation.</p> required <code>inter</code> <code>bool</code> <p>Whether to include the interaction term. Defaults to <code>True</code>.</p> <code>True</code> <code>fn</code> <code>list</code> <p>Names for the two factors. Defaults to <code>[\"A\", \"B\"]</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints test details and results.</p> <code>False</code> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def harrison_kanji_test(\n    alpha: np.ndarray,\n    idp: np.ndarray,\n    idq: np.ndarray,\n    inter: bool = True,\n    fn: Optional[list] = None,\n    verbose: bool = False,\n) -&gt; HarrisonKanjiTestResult:\n    \"\"\"\n    Harrison-Kanji Test (Two-Way ANOVA) for Circular Data.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Angular measurements (radians).\n    idp : np.ndarray\n        Factor A identifiers for each observation.\n    idq : np.ndarray\n        Factor B identifiers for each observation.\n    inter : bool, optional\n        Whether to include the interaction term. Defaults to ``True``.\n    fn : list, optional\n        Names for the two factors. Defaults to ``[\"A\", \"B\"]``.\n    verbose : bool, optional\n        If ``True``, prints test details and results.\n    \"\"\"\n\n    if fn is None:\n        fn = [\"A\", \"B\"]\n\n    # Ensure data is in column format\n    alpha = np.asarray(alpha).flatten()\n    idp = np.asarray(idp).flatten()\n    idq = np.asarray(idq).flatten()\n\n    # Number of factor levels\n    p = len(np.unique(idp))\n    q = len(np.unique(idq))\n\n    # Data frame for aggregation\n    df = pd.DataFrame({fn[0]: idp, fn[1]: idq, \"dependent\": alpha})\n    n = len(df)\n\n    # Total resultant vector length\n    tr = n * circ_r(np.array(df[\"dependent\"].values))\n    kk = circ_kappa(tr / n)\n\n    # Compute mean resultants per group\n    gr = df.groupby(fn)\n    cn = gr.count()\n    cr = gr.agg(circ_r) * cn\n    cn = cn.unstack(fn[1])\n    cr = cr.unstack(fn[1])\n\n    # Factor A\n    gr = df.groupby(fn[0])\n    pn = gr.count()[\"dependent\"]\n    pr = gr.agg(circ_r)[\"dependent\"] * pn\n\n    # Factor B\n    gr = df.groupby(fn[1])\n    qn = gr.count()[\"dependent\"]\n    qr = gr.agg(circ_r)[\"dependent\"] * qn\n\n    if kk &gt; 2:  # Large kappa approximation\n        eff_1 = sum(pr**2 / np.sum(cn, axis=1)) - tr**2 / n\n        df_1 = p - 1\n        ms_1 = eff_1 / df_1\n\n        eff_2 = sum(qr**2 / np.sum(cn, axis=0)) - tr**2 / n\n        df_2 = q - 1\n        ms_2 = eff_2 / df_2\n\n        eff_t = n - tr**2 / n\n        df_t = n - 1\n        m = np.asarray(cn.values).mean()\n\n        if inter:\n            beta = 1 / (1 - 1 / (5 * kk) - 1 / (10 * (kk**2)))\n\n            eff_r = n - np.asarray((cr**2.0 / cn).values).sum()\n            df_r = p * q * (m - 1)\n            ms_r = eff_r / df_r\n\n            eff_i = (\n                np.asarray((cr**2.0 / cn).values).sum()\n                - sum(qr**2.0 / qn)\n                - sum(pr**2.0 / pn)\n                + tr**2 / n\n            )\n            df_i = (p - 1) * (q - 1)\n            ms_i = eff_i / df_i\n\n            FI = ms_i / ms_r\n            pI = 1 - f.cdf(FI, df_i, df_r)  # `f.cdf` is now unambiguous\n        else:\n            eff_r = n - sum(qr**2.0 / qn) - sum(pr**2.0 / pn) + tr**2 / n\n            df_r = (p - 1) * (q - 1)\n            ms_r = eff_r / df_r\n\n            eff_i, df_i, ms_i, FI, pI = None, None, None, None, np.nan\n            beta = 1\n\n        F1 = beta * ms_1 / ms_r\n        p1 = 1 - f.cdf(F1, df_1, df_r)\n\n        F2 = beta * ms_2 / ms_r\n        p2 = 1 - f.cdf(F2, df_2, df_r)\n\n    else:  # Small kappa approximation\n        rr = iv(1, kk) / iv(0, kk)\n        kappa_factor = 2 / (1 - rr**2)  # Renamed `f` to `kappa_factor`\n\n        chi1 = kappa_factor * (sum(pr**2.0 / pn) - tr**2 / n)\n        df_1 = 2 * (p - 1)\n        p1 = 1 - chi2.cdf(chi1, df=df_1)\n\n        chi2_val = kappa_factor * (sum(qr**2.0 / qn) - tr**2 / n)\n        df_2 = 2 * (q - 1)\n        p2 = 1 - chi2.cdf(chi2_val, df=df_2)\n\n        chiI = kappa_factor * (\n            np.asarray((cr**2.0 / cn).values).sum()\n            - sum(pr**2.0 / pn)\n            - sum(qr**2.0 / qn)\n            + tr**2 / n\n        )\n        df_i = (p - 1) * (q - 1)\n        pI = chi2.sf(chiI, df=df_i)\n\n    pval = float(p1.squeeze()), float(p2.squeeze()), float(np.squeeze(pI))\n\n    # Construct ANOVA Table\n    if kk &gt; 2:\n        table = pd.DataFrame(\n            {\n                \"Source\": fn + [\"Interaction\", \"Residual\", \"Total\"],\n                \"DoF\": [df_1, df_2, df_i, df_r, df_t],\n                \"SS\": [eff_1, eff_2, eff_i, eff_r, eff_t],\n                \"MS\": [ms_1, ms_2, ms_i, ms_r, np.nan],\n                \"F\": [np.squeeze(F1), np.squeeze(F2), FI, np.nan, np.nan],\n                \"p\": list(pval) + [np.nan, np.nan],\n            }\n        ).set_index(\"Source\")\n    else:\n        table = pd.DataFrame(\n            {\n                \"Source\": fn + [\"Interaction\"],\n                \"DoF\": [df_1, df_2, df_i],\n                \"chi2\": [chi1.squeeze(), chi2_val.squeeze(), chiI.squeeze()],\n                \"p\": pval,\n            }\n        ).set_index(\"Source\")\n\n    result = HarrisonKanjiTestResult(p_values=pval, anova_table=table)\n\n    if verbose:\n        p_a, p_b, p_inter = result.p_values\n\n        def _fmt(p: Optional[float]) -&gt; str:\n            if p is None or (isinstance(p, float) and math.isnan(p)):\n                return \"n/a\"\n            return f\"{p:.5f} {significance_code(p)}\"\n\n        print(\"Harrison-Kanji Two-Way Circular ANOVA\")\n        print(\"-------------------------------------\")\n        print(f\"H0 ({fn[0]}): No difference in mean direction across factor {fn[0]}.\")\n        print(f\"H0 ({fn[1]}): No difference in mean direction across factor {fn[1]}.\")\n        if inter:\n            print(\"H0 (Interaction): No interaction between the two factors.\")\n        print(\"\")\n        print(f\"{fn[0]} effect p-value: {_fmt(p_a)}\")\n        print(f\"{fn[1]} effect p-value: {_fmt(p_b)}\")\n        if inter:\n            print(f\"Interaction p-value: {_fmt(p_inter)}\")\n        print(\"\")\n        print(\"ANOVA table (first rows):\")\n        print(result.anova_table.head())\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.equal_kappa_test","title":"<code>equal_kappa_test(samples, verbose=False)</code>","text":"<p>Test for Homogeneity of Concentration Parameters (\u03ba) in Circular Data.</p> <ul> <li>H\u2080: All groups have the same concentration parameter (\u03ba).</li> <li>H\u2081: At least one group has a different \u03ba.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of np.ndarray</code> <p>List of circular data arrays (angles in radians) for different groups.</p> required <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints the test summary.</p> <code>False</code> <p>Returns:</p> Type Description <code>EqualKappaTestResult</code> <p>Dataclass containing the test statistic, p-value, and supporting metrics.</p> Notes <ul> <li>Uses different approximations based on mean resultant length (<code>r\u0304</code>):</li> <li>Small <code>r\u0304</code> (&lt; 0.45): Uses <code>arcsin</code> transformation.</li> <li>Moderate <code>r\u0304</code> (0.45 - 0.7): Uses <code>asinh</code> transformation.</li> <li>Large <code>r\u0304</code> (&gt; 0.7): Uses Bartlett-type test (log-likelihood method).</li> </ul> References <ul> <li>Jammalamadaka &amp; SenGupta (2001), Section 5.4.</li> <li>Fisher (1993), Section 4.3.</li> <li><code>equal.kappa.test</code> from R's <code>circular</code> package.</li> </ul> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def equal_kappa_test(samples: list[np.ndarray], verbose: bool = False) -&gt; EqualKappaTestResult:\n    \"\"\"\n    Test for Homogeneity of Concentration Parameters (\u03ba) in Circular Data.\n\n    - **H\u2080**: All groups have the same concentration parameter (\u03ba).\n    - **H\u2081**: At least one group has a different \u03ba.\n\n    Parameters\n    ----------\n    samples : list of np.ndarray\n        List of circular data arrays (angles in radians) for different groups.\n    verbose : bool, optional\n        If `True`, prints the test summary.\n\n    Returns\n    -------\n    EqualKappaTestResult\n        Dataclass containing the test statistic, p-value, and supporting metrics.\n\n    Notes\n    -----\n    - Uses **different approximations based on mean resultant length** (`r\u0304`):\n      - **Small `r\u0304` (&lt; 0.45)**: Uses `arcsin` transformation.\n      - **Moderate `r\u0304` (0.45 - 0.7)**: Uses `asinh` transformation.\n      - **Large `r\u0304` (&gt; 0.7)**: Uses Bartlett-type test (log-likelihood method).\n\n    References\n    ----------\n    - Jammalamadaka &amp; SenGupta (2001), Section 5.4.\n    - Fisher (1993), Section 4.3.\n    - `equal.kappa.test` from R's `circular` package.\n    \"\"\"\n\n    # Number of groups\n    k = len(samples)\n    if k &lt; 2:\n        raise ValueError(\"At least two groups are required for the test.\")\n\n    arrays = [np.asarray(group, dtype=float) for group in samples]\n    if any(arr.size == 0 for arr in arrays):\n        raise ValueError(\"Each group must contain at least one observation.\")\n\n    # Sample sizes\n    ns = np.array([arr.size for arr in arrays])\n    if np.any(ns &lt; 2):\n        raise ValueError(\"Each group must contain at least two observations.\")\n\n    # Mean resultant lengths\n    r_bars = np.array([circ_r(arr) for arr in arrays])\n    Rs = r_bars * ns  # Unnormalized resultants\n\n    # Overall resultant and mean resultant length\n    all_samples = np.hstack(arrays)\n    N = len(all_samples)\n    r_bar_all = circ_r(all_samples)\n\n    # Estimate kappa values\n    kappas = np.array([circ_kappa(r) for r in r_bars])\n    kappa_all = circ_kappa(r_bar_all)\n\n    # Choose test statistic based on `r\u0304`\n    if r_bar_all &lt; 0.45:\n        # Small `r\u0304`: arcsin transformation\n        ws = 4 * (ns - 4) / 3\n        g1s = np.arcsin(np.sqrt(3 / 8) * 2 * r_bars)\n        chi_square_stat = np.sum(ws * g1s**2) - (np.sum(ws * g1s) ** 2 / np.sum(ws))\n        regime = \"small\"\n\n    elif 0.45 &lt;= r_bar_all &lt;= 0.7:\n        # Moderate `r\u0304`: asinh transformation\n        ws = (ns - 3) / 0.798\n        g2s = np.arcsinh((r_bars - 1.089) / 0.258)\n        chi_square_stat = np.sum(ws * g2s**2) - (np.sum(ws * g2s) ** 2 / np.sum(ws))\n        regime = \"moderate\"\n\n    else:\n        # Large `r\u0304`: Bartlett-type test\n        vs = ns - 1\n        v = N - k\n        d = 1 / (3 * (k - 1)) * (np.sum(1 / vs) - 1 / v)\n        total_residual = N - np.sum(Rs)\n        residuals = ns - Rs\n        if np.any(residuals &lt;= 0):\n            raise ValueError(\"Degenerate data: within-group dispersion is zero.\")\n        if total_residual &lt;= 0:\n            raise ValueError(\"Degenerate data: between-group dispersion is zero.\")\n        chi_square_stat = (1 / (1 + d)) * (\n            v * np.log(total_residual / v) - np.sum(vs * np.log(residuals / vs))\n        )\n        regime = \"large\"\n\n    # Compute p-value\n    df = k - 1\n    p_value = 1 - chi2.cdf(chi_square_stat, df)\n\n    result = EqualKappaTestResult(\n        kappa=kappas,\n        kappa_all=float(kappa_all),\n        rho=r_bars,\n        rho_all=float(r_bar_all),\n        df=int(df),\n        statistic=float(chi_square_stat),\n        pval=float(p_value),\n        regime=regime,\n    )\n\n    # Print results if verbose is enabled\n    if verbose:\n        print(\"\\nTest for Homogeneity of Concentration Parameters (\u03ba)\")\n        print(\"------------------------------------------------------\")\n        print(f\"Mean Resultant Lengths: {result.rho}\")\n        print(f\"Overall Mean Resultant Length: {result.rho_all:.5f}\")\n        print(f\"Estimated Kappa Values: {result.kappa}\")\n        print(f\"Overall Estimated Kappa: {result.kappa_all:.5f}\")\n        print(f\"Degrees of Freedom: {result.df}\")\n        print(f\"Chi-Square Statistic: {result.statistic:.5f}\")\n        print(f\"P-value: {result.pval:.5f}\")\n        print(f\"Regime: {result.regime}\")\n        print(\"------------------------------------------------------\\n\")\n\n    return result\n</code></pre>"},{"location":"reference/hypothesis/#pycircstat2.hypothesis.common_median_test","title":"<code>common_median_test(samples, alpha=0.05, verbose=False)</code>","text":"<p>Common Median Test (Equal Median Test) for Multiple Circular Samples.</p> <ul> <li>H\u2080: All groups have the same circular median.</li> <li>H\u2081: At least one group has a different circular median.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list of np.ndarray</code> <p>List of circular data arrays (angles in radians) for different groups.</p> required <code>alpha</code> <code>float</code> <p>Significance level for deciding whether to reject the null hypothesis (default 0.05).</p> <code>0.05</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, prints the test summary.</p> <code>False</code> <p>Returns:</p> Type Description <code>CommonMedianTestResult</code> <p>Dataclass containing the common median, test statistic, p-value, and rejection flag.</p> References <ul> <li>Fisher, N. I. (1995). Statistical Analysis of Circular Data.</li> <li><code>circ_cmtest</code> from MATLAB's Circular Statistics Toolbox.</li> </ul> Source code in <code>pycircstat2/hypothesis.py</code> <pre><code>def common_median_test(\n    samples: list[np.ndarray],\n    alpha: float = 0.05,\n    verbose: bool = False,\n) -&gt; CommonMedianTestResult:\n    \"\"\"\n    Common Median Test (Equal Median Test) for Multiple Circular Samples.\n\n    - **H\u2080**: All groups have the same circular median.\n    - **H\u2081**: At least one group has a different circular median.\n\n    Parameters\n    ----------\n    samples : list of np.ndarray\n        List of circular data arrays (angles in radians) for different groups.\n    alpha : float, optional\n        Significance level for deciding whether to reject the null hypothesis (default 0.05).\n    verbose : bool, optional\n        If `True`, prints the test summary.\n\n    Returns\n    -------\n    CommonMedianTestResult\n        Dataclass containing the common median, test statistic, p-value, and rejection flag.\n\n    References\n    ----------\n    - Fisher, N. I. (1995). Statistical Analysis of Circular Data.\n    - `circ_cmtest` from MATLAB's Circular Statistics Toolbox.\n    \"\"\"\n\n    # Number of groups\n    if not (0 &lt; alpha &lt; 1):\n        raise ValueError(\"`alpha` must be between 0 and 1.\")\n\n    k = len(samples)\n    if k &lt; 2:\n        raise ValueError(\"At least two groups are required for the test.\")\n\n    arrays = [np.asarray(group, dtype=float) for group in samples]\n    if any(arr.size == 0 for arr in arrays):\n        raise ValueError(\"Each group must contain at least one observation.\")\n\n    # Sample sizes\n    ns = np.array([arr.size for arr in arrays])\n    N = int(np.sum(ns))  # Total number of observations\n\n    # Compute the common circular median\n    common_median = circ_median(np.hstack(arrays))\n\n    # Compute deviations from the common median\n    m = np.zeros(k, dtype=float)\n    for i, group in enumerate(arrays):\n        deviations = circ_dist(group, common_median)\n        m[i] = np.sum(deviations &lt; 0)\n\n    # Compute test statistic\n    M = np.sum(m)\n    if M == 0 or M == N:\n        raise ValueError(\"All observations fall on the same side of the median; test is undefined.\")\n\n    P = (N**2 / (M * (N - M))) * np.sum(m**2 / ns) - (N * M) / (N - M)\n\n    # Compute p-value\n    df = k - 1\n    p_value = 1 - chi2.cdf(P, df)\n    reject = p_value &lt; alpha\n\n    # If the null hypothesis is rejected, return NaN for the median\n    if reject:\n        common_median = np.nan\n\n    result = CommonMedianTestResult(\n        common_median=float(common_median),\n        statistic=float(P),\n        pval=float(p_value),\n        reject=bool(reject),\n    )\n\n    # Print results if verbose is enabled\n    if verbose:\n        print(\"\\nCommon Median Test (Equal Median Test)\")\n        print(\"--------------------------------------\")\n        median_display = result.common_median if not result.reject else \"NaN\"\n        print(f\"Estimated Common Median: {median_display}\")\n        print(f\"Test Statistic: {result.statistic:.5f}\")\n        print(f\"P-value: {result.pval:.5f}\")\n        decision = \"Yes\" if result.reject else \"No\"\n        print(f\"Reject H\u2080 (\u03b1={alpha:.2f}): {decision}\")\n        print(\"--------------------------------------\\n\")\n\n    return result\n</code></pre>"},{"location":"reference/regression/","title":"Regression","text":""},{"location":"reference/regression/#pycircstat2.regression.CLRegression","title":"<code>CLRegression</code>","text":"<p>Circular-Linear Regression.</p> <p>Fits a circular response to linear predictors using iterative optimization.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>A formula string like '\u03b8 ~ x1 + x2 + x3' specifying the model.</p> <code>None</code> <code>data</code> <code>DataFrame</code> <p>A pandas DataFrame containing the response and predictors.</p> <code>None</code> <code>theta</code> <code>ndarray</code> <p>A numpy array of circular response values in radians.</p> <code>None</code> <code>X</code> <code>ndarray</code> <p>A numpy array of predictor values.</p> <code>None</code> <code>model_type</code> <code>str</code> <p>Type of model to fit. Must be one of 'mean', 'kappa', or 'mixed'.</p> <ul> <li>'mean': Fit a model for the mean direction.</li> <li>'kappa': Fit a model for the concentration parameter.</li> <li>'mixed': Fit a mixed circular-linear model.</li> </ul> <code>'mixed'</code> <code>beta0</code> <code>ndarray</code> <p>Initial values for the beta coefficients.</p> <code>None</code> <code>alpha0</code> <code>float</code> <p>Initial value for the intercept.</p> <code>None</code> <code>gamma0</code> <code>ndarray</code> <p>Initial values for the gamma coefficients.</p> <code>None</code> <code>tol</code> <code>float</code> <p>Convergence tolerance for the optimization.</p> <code>1e-08</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations for the optimization.</p> <code>100</code> <code>verbose</code> <code>bool</code> <p>Whether to print optimization progress.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>result</code> <code>dict</code> <p>A dictionary containing the estimated coefficients and other statistics.</p> <ul> <li>beta : np.ndarray     Estimated beta coefficients for the mean direction.</li> <li>alpha : float     Estimated intercept for the concentration parameter..</li> <li>gamma : np.ndarray     Estimated coefficients for the concentration parameter.</li> <li>mu : float     Estimated mean direction of the circular response.</li> <li>kappa : float     Estimated concentration parameter of the circular response.</li> <li>log_likelihood : float     Log-likelihood of the model.</li> </ul> <p>Methods:</p> Name Description <code>summary</code> <p>Print a summary of the regression results.</p> Notes <p>The implementation is ported from the <code>lm.circular.cl</code> in the <code>circular</code> R package.</p> References <ul> <li>Fisher, N. I. (1993). Statistical analysis of circular data. Cambridge University Press.</li> <li>Pewsey, A., Neuh\u00e4user, M., &amp; Ruxton, G. D. (2014) Circular Statistics in R. Oxford University Press.</li> </ul> Source code in <code>pycircstat2/regression.py</code> <pre><code>class CLRegression:\n    \"\"\"\n    Circular-Linear Regression.\n\n    Fits a circular response to linear predictors using iterative optimization.\n\n    Parameters\n    ----------\n    formula : str, optional\n        A formula string like '\u03b8 ~ x1 + x2 + x3' specifying the model.\n    data : pd.DataFrame, optional\n        A pandas DataFrame containing the response and predictors.\n    theta : np.ndarray, optional\n        A numpy array of circular response values in radians.\n    X : np.ndarray, optional\n        A numpy array of predictor values.\n    model_type : str, optional\n        Type of model to fit. Must be one of 'mean', 'kappa', or 'mixed'.\n\n        - 'mean': Fit a model for the mean direction.\n        - 'kappa': Fit a model for the concentration parameter.\n        - 'mixed': Fit a mixed circular-linear model.\n\n    beta0 : np.ndarray, optional\n        Initial values for the beta coefficients.\n    alpha0 : float, optional\n        Initial value for the intercept.\n    gamma0 : np.ndarray, optional\n        Initial values for the gamma coefficients.\n    tol : float, optional\n        Convergence tolerance for the optimization.\n    max_iter : int, optional\n        Maximum number of iterations for the optimization.\n    verbose : bool, optional\n        Whether to print optimization progress.\n\n    Attributes\n    ----------\n    result : dict\n        A dictionary containing the estimated coefficients and other statistics.\n\n        - beta : np.ndarray\n            Estimated beta coefficients for the mean direction.\n        - alpha : float\n            Estimated intercept for the concentration parameter..\n        - gamma : np.ndarray\n            Estimated coefficients for the concentration parameter.\n        - mu : float\n            Estimated mean direction of the circular response.\n        - kappa : float\n            Estimated concentration parameter of the circular response.\n        - log_likelihood : float\n            Log-likelihood of the model.\n\n    Methods\n    -------\n    summary()\n        Print a summary of the regression results.\n\n\n    Notes\n    -----\n    The implementation is ported from the `lm.circular.cl` in the `circular` R package.\n\n    References\n    ----------\n    - Fisher, N. I. (1993). Statistical analysis of circular data. Cambridge University Press.\n    - Pewsey, A., Neuh\u00e4user, M., &amp; Ruxton, G. D. (2014) Circular Statistics in R. Oxford University Press.\n    \"\"\"\n\n    def __init__(\n        self,\n        formula: Optional[str] = None,\n        data: Optional[pd.DataFrame] = None,\n        theta: Optional[np.ndarray] = None,\n        X: Optional[np.ndarray] = None,\n        model_type: str = \"mixed\",\n        beta0: Union[np.ndarray, None] = None,\n        alpha0: Union[float, None] = None,\n        gamma0: Union[np.ndarray, None] = None,\n        tol: float = 1e-8,\n        max_iter: int = 100,\n        verbose: bool = False,\n    ):\n        self.verbose = verbose\n        self.tol = tol\n        self.max_iter = max_iter\n        self.model_type = model_type\n\n        # Parse inputs\n        if formula and data is not None:\n            theta_arr, X_arr, feature_names = self._parse_formula(formula, data)\n        elif theta is not None and X is not None:\n            feature_names = None\n            theta_arr, X_arr = theta, X\n        else:\n            raise ValueError(\"Provide either a formula + data or theta and X.\")\n\n        self.theta, self.X = self._prepare_design(theta_arr, X_arr)\n        if feature_names is None:\n            self.feature_names = [f\"x{i}\" for i in range(self.X.shape[1])]\n        else:\n            self.feature_names = feature_names\n\n        # Validate model type\n        if model_type not in [\"mean\", \"kappa\", \"mixed\"]:\n            raise ValueError(\"Model type must be 'mean', 'kappa', or 'mixed'.\")\n\n        # Initialize parameters\n        p = self.X.shape[1]\n        self.alpha = float(alpha0) if alpha0 is not None else 0.0\n        self.beta = self._coerce_vector(beta0, p, name=\"beta\")\n        self.gamma = self._coerce_vector(gamma0, p, name=\"gamma\")\n\n        # Fit the model\n        self.result = self._fit()\n\n    @staticmethod\n    def _coerce_vector(vec: Optional[np.ndarray], length: int, name: str) -&gt; np.ndarray:\n        if vec is None:\n            return np.zeros(length, dtype=float)\n        arr = np.asarray(vec, dtype=float).reshape(-1)\n        if arr.size != length:\n            raise ValueError(f\"Initial {name} must have length {length} (got {arr.size}).\")\n        if not np.all(np.isfinite(arr)):\n            raise ValueError(f\"Initial {name} contains non-finite values.\")\n        return arr\n\n    @staticmethod\n    def _prepare_design(theta: Iterable[float], X: Iterable[Iterable[float]]) -&gt; Tuple[np.ndarray, np.ndarray]:\n        theta_arr = np.asarray(theta, dtype=float).reshape(-1)\n        if theta_arr.size == 0:\n            raise ValueError(\"`theta` must contain at least one observation.\")\n        if not np.all(np.isfinite(theta_arr)):\n            raise ValueError(\"`theta` contains non-finite values.\")\n\n        X_arr = np.asarray(X, dtype=float)\n        if X_arr.ndim == 1:\n            X_arr = X_arr[:, None]\n        if X_arr.ndim != 2:\n            raise ValueError(\"`X` must be convertible to a 2D numeric array.\")\n        if X_arr.shape[0] != theta_arr.size:\n            raise ValueError(\"`theta` and `X` must have matching numbers of rows.\")\n        if not np.all(np.isfinite(X_arr)):\n            raise ValueError(\"`X` contains non-finite values.\")\n        return theta_arr, X_arr\n\n    def _parse_formula(\n        self, formula: str, data: pd.DataFrame\n    ) -&gt; Tuple[np.ndarray, np.ndarray, List[str]]:\n        theta_col, x_cols = formula.split(\"~\")\n        theta_series = data[theta_col.strip()]\n        if theta_series.isnull().any():\n            raise ValueError(\"Response column contains missing values.\")\n        theta = theta_series.to_numpy()\n        x_cols = [col.strip() for col in x_cols.split(\"+\")]\n        X_df = data[x_cols]\n        if X_df.isnull().any().any():\n            raise ValueError(\"Predictor columns contain missing values.\")\n        X = X_df.to_numpy()\n        return theta, X, x_cols\n\n    def _A1(self, kappa: np.ndarray) -&gt; np.ndarray:\n        return i1(kappa) / i0(kappa)\n\n    def _A1inv(self, R: float) -&gt; float:\n        if 0 &lt;= R &lt; 0.53:\n            return 2 * R + R**3 + (5 * R**5) / 6\n        elif R &lt; 0.85:\n            return -0.4 + 1.39 * R + 0.43 / (1 - R)\n        else:\n            return 1 / (R**3 - 4 * R**2 + 3 * R)\n\n    def _A1_prime(self, kappa: np.ndarray) -&gt; np.ndarray:\n        a1 = A1(kappa)\n        return 1 - a1 / kappa - a1**2\n\n    def _fit(self):\n        theta = self.theta\n        n = len(theta)\n        X = self.X\n        X1 = np.column_stack((np.ones(n), X))  # Add intercept\n        beta, alpha, gamma = self.beta, self.alpha, self.gamma\n        diff = self.tol + 1\n        log_likelihood_old = -np.inf\n\n        for iter_count in range(self.max_iter):\n            if self.model_type == \"mean\":\n                # Step 1: Compute mu and kappa\n                raw_deviation = theta - 2 * np.arctan(X @ beta)\n                S = np.mean(np.sin(raw_deviation))\n                C = np.mean(np.cos(raw_deviation))\n                R = np.hypot(S, C)\n                kappa = float(A1inv(R))\n                mu = np.arctan2(S, C)\n\n                # Step 2: Update beta\n                denom = 1 + (X @ beta) ** 2\n                G = 2 * X / denom[:, None]\n                weight = float(kappa * A1(np.array([kappa]))[0])\n                u = kappa * np.sin(raw_deviation - mu)\n                XtX = G.T @ G\n                rhs = G.T @ u + weight * XtX @ beta\n                mat = weight * XtX + 1e-8 * np.eye(X.shape[1])\n                beta_new = _safe_solve(mat, rhs)\n                alpha_new, gamma_new = alpha, gamma\n\n                # Log-likelihood\n                log_likelihood = -n * np.log(i0(kappa)) + kappa * np.sum(\n                    np.cos(raw_deviation - mu)\n                )\n\n            elif self.model_type == \"kappa\":\n                # Step 1: Compute mu and kappa\n                kappa = np.exp(alpha + X @ gamma)\n                S = float(np.sum(kappa * np.sin(theta)))\n                C = float(np.sum(kappa * np.cos(theta)))\n                mu = np.arctan2(S, C)\n\n                # Step 2: Update gamma\n                a1_kappa = self._A1(kappa)\n                a1_prime = self._A1_prime(kappa)\n                if np.any(np.isclose(a1_prime, 0.0)):\n                    raise ValueError(\"Encountered zero derivative in concentration update.\")\n                residuals_gamma = np.cos(theta - mu) - a1_kappa\n                y_gamma = residuals_gamma / (a1_prime * kappa)\n                weights = (kappa**2) * a1_prime\n                XtWX = X1.T @ (weights[:, None] * X1)\n                XtWy = X1.T @ (weights * y_gamma)\n                update = _safe_solve(XtWX + 1e-8 * np.eye(X1.shape[1]), XtWy)\n                alpha_new = alpha + update[0]\n                gamma_new = gamma + update[1:]\n                beta_new = beta\n                # Log-likelihood\n                log_likelihood = -np.sum(np.log(i0(kappa))) + np.sum(\n                    kappa * np.cos(theta - mu)\n                )\n\n            elif self.model_type == \"mixed\":\n                # Step 1: Compute mu and kappa\n                kappa = np.exp(alpha + X @ gamma)\n                raw_deviation = theta - 2 * np.arctan(X @ beta)\n                S = np.sum(kappa * np.sin(raw_deviation))\n                C = np.sum(kappa * np.cos(raw_deviation))\n                mu = np.arctan2(S, C)\n                residuals = theta - mu\n\n                # Step 2: Update beta\n                denom = 1 + (X @ beta) ** 2\n                G = 2 * X / denom[:, None]\n                weights_beta = kappa * self._A1(kappa)\n                XtWX_beta = G.T @ (weights_beta[:, None] * G)\n                rhs_beta = G.T @ (weights_beta * np.sin(residuals))\n                beta_new = _safe_solve(\n                    XtWX_beta + 1e-8 * np.eye(X.shape[1]), rhs_beta\n                )\n\n                # Step 3: Update gamma\n                a1_kappa = self._A1(kappa)\n                a1_prime = self._A1_prime(kappa)\n                if np.any(np.isclose(a1_prime, 0.0)):\n                    raise ValueError(\"Encountered zero derivative in concentration update.\")\n                residuals_gamma = np.cos(raw_deviation - mu) - a1_kappa\n                y_gamma = residuals_gamma / (a1_prime * kappa)\n                weights_gamma = (kappa**2) * a1_prime\n                XtWX = X1.T @ (weights_gamma[:, None] * X1)\n                XtWy = X1.T @ (weights_gamma * y_gamma)\n                update = _safe_solve(XtWX + 1e-8 * np.eye(X1.shape[1]), XtWy)\n                alpha_new = alpha + update[0]\n                gamma_new = gamma + update[1:]\n\n                # Log-likelihood\n                log_likelihood = -np.sum(np.log(i0(kappa))) + np.sum(\n                    kappa * np.cos(raw_deviation - mu)\n                )\n\n            # Convergence check\n            diff = np.abs(log_likelihood - log_likelihood_old)\n            if self.verbose:\n                print(\n                    f\"Iteration {iter_count + 1}: Log-Likelihood = {log_likelihood:.5f}, diff = {diff:.2e}\"\n                )\n            if diff &lt; self.tol:\n                break\n\n            beta, alpha, gamma = beta_new, alpha_new, gamma_new\n            log_likelihood_old = log_likelihood\n\n        result = {\n            \"beta\": beta,\n            \"alpha\": alpha,\n            \"gamma\": gamma,\n            \"mu\": mu,\n            \"kappa\": kappa,\n            \"log_likelihood\": log_likelihood,\n        }\n\n        se_result = self._compute_standard_errors(result)\n\n        result.update(se_result)\n\n        return result\n\n    def _compute_standard_errors(self, result):\n        \"\"\"\n        Compute standard errors for the parameters based on the fitted model.\n        \"\"\"\n        theta = self.theta\n        X = self.X\n        n = len(theta)\n        kappa = result[\"kappa\"]\n        beta = result[\"beta\"]\n        gamma = result[\"gamma\"]\n        alpha = result[\"alpha\"]\n\n        se_results = {}\n\n        if self.model_type == \"mean\":\n            # Mean Direction Model\n            denom = 1 + (X @ beta) ** 2\n            G = 2 * X / denom[:, None]\n            weight = float(kappa * self._A1(np.array([kappa]))[0])\n            XtAX = weight * (G.T @ G) + 1e-8 * np.eye(X.shape[1])\n            cov_beta = _safe_inverse(XtAX)\n            se_beta = np.sqrt(np.diag(cov_beta))\n\n            denom_mu = max((n - X.shape[1]) * kappa * self._A1(np.array([kappa]))[0], 1e-12)\n            se_mu = 1 / np.sqrt(denom_mu)\n            denom_kappa = n * (1 - self._A1(np.array([kappa]))[0] ** 2 - self._A1(np.array([kappa]))[0] / kappa)\n            se_kappa = np.sqrt(1 / max(denom_kappa, 1e-12))\n\n            se_results.update(\n                {\n                    \"se_beta\": se_beta,\n                    \"se_mu\": se_mu,\n                    \"se_kappa\": se_kappa,\n                }\n            )\n\n        elif self.model_type == \"kappa\":\n            # Concentration Parameter Model\n            X1 = np.column_stack((np.ones(n), X))  # Add intercept\n            weights = (np.exp(X1 @ np.hstack([alpha, gamma])) ** 2) * self._A1_prime(kappa)\n            XtWX = X1.T @ (weights[:, None] * X1) + 1e-8 * np.eye(X1.shape[1])\n\n            cov_gamma_alpha = _safe_inverse(XtWX)\n\n            se_alpha = np.sqrt(cov_gamma_alpha[0, 0])\n            se_gamma = np.sqrt(np.diag(cov_gamma_alpha[1:, 1:]))\n\n            denom_mu = max(np.sum(kappa * self._A1(kappa)) - 0.5, 1e-12)\n            se_mu = 1 / np.sqrt(denom_mu)\n\n            se_kappa = np.sqrt(1 / np.clip(n * self._A1_prime(kappa), 1e-12, None))\n\n            se_results.update(\n                {\n                    \"se_alpha\": se_alpha,\n                    \"se_gamma\": se_gamma,\n                    \"se_mu\": se_mu,\n                    \"se_kappa\": se_kappa,\n                }\n            )\n\n        elif self.model_type == \"mixed\":\n            # Mixed Model\n            denom = 1 + (X @ beta) ** 2\n            G = 2 * X / denom[:, None]\n            weights_beta = kappa * self._A1(kappa)\n            XtGKGX = G.T @ (weights_beta[:, None] * G) + 1e-8 * np.eye(X.shape[1])\n\n            cov_beta = _safe_inverse(XtGKGX)\n            se_beta = np.sqrt(np.diag(cov_beta))\n\n            X1 = np.column_stack((np.ones(n), X))  # Add intercept\n            weights_gamma = (np.exp(X1 @ np.hstack([alpha, gamma])) ** 2) * self._A1_prime(kappa)\n            XtWX_gamma = X1.T @ (weights_gamma[:, None] * X1) + 1e-8 * np.eye(X1.shape[1])\n\n            cov_gamma_alpha = _safe_inverse(XtWX_gamma)\n            se_alpha = np.sqrt(cov_gamma_alpha[0, 0])\n            se_gamma = np.sqrt(np.diag(cov_gamma_alpha[1:, 1:]))\n\n            denom_mu = max(np.sum(kappa * self._A1(kappa)) - 0.5, 1e-12)\n            se_mu = 1 / np.sqrt(denom_mu)\n            a1_vals = self._A1(kappa)\n            denom_kappa = n * (1 - a1_vals**2 - a1_vals / kappa)\n            se_kappa = np.sqrt(1 / np.clip(denom_kappa, 1e-12, None))\n            se_results.update(\n                {\n                    \"se_beta\": se_beta,\n                    \"se_alpha\": se_alpha,\n                    \"se_gamma\": se_gamma,\n                    \"se_mu\": se_mu,\n                    \"se_kappa\": se_kappa,\n                }\n            )\n\n        else:\n            raise ValueError(f\"Unknown model type: {self.model_type}\")\n\n        return se_results\n\n    def AIC(self):\n        \"\"\"\n        Calculate Akaike Information Criterion (AIC).\n        \"\"\"\n        if self.result is None:\n            raise ValueError(\"Model must be fitted before calculating AIC.\")\n\n        log_likelihood = self.result[\"log_likelihood\"]\n        if self.model_type == \"mean\":\n            n_params = len(self.result[\"beta\"])  # Only beta\n        elif self.model_type == \"kappa\":\n            n_params = 1 + len(self.result[\"gamma\"])  # alpha + gamma\n        elif self.model_type == \"mixed\":\n            n_params = (\n                1 + len(self.result[\"beta\"]) + len(self.result[\"gamma\"])\n            )  # alpha + beta + gamma\n        else:\n            raise ValueError(f\"Unknown model type: {self.model_type}\")\n\n        return -2 * log_likelihood + 2 * n_params\n\n    def BIC(self):\n        \"\"\"\n        Calculate Bayesian Information Criterion (BIC).\n        \"\"\"\n        if self.result is None:\n            raise ValueError(\"Model must be fitted before calculating BIC.\")\n\n        log_likelihood = self.result[\"log_likelihood\"]\n        n = len(self.theta)\n        if self.model_type == \"mean\":\n            n_params = len(self.result[\"beta\"])  # Only beta\n        elif self.model_type == \"kappa\":\n            n_params = 1 + len(self.result[\"gamma\"])  # alpha + gamma\n        elif self.model_type == \"mixed\":\n            n_params = (\n                1 + len(self.result[\"beta\"]) + len(self.result[\"gamma\"])\n            )  # alpha + beta + gamma\n        else:\n            raise ValueError(f\"Unknown model type: {self.model_type}\")\n\n        return -2 * log_likelihood + n_params * np.log(n)\n\n    def predict(self, X_new):\n        \"\"\"\n        Predict circular response values for new predictor values.\n\n        Parameters\n        ----------\n        X_new: array-like, shape (n_samples, n_features)\n            New predictor data.\n\n        Returns\n        -------\n        theta_new: array-like, shape(n_samples, )\n            New circular response values.\n        \"\"\"\n        if self.result is None:\n            raise ValueError(\"Model must be fitted before making predictions.\")\n\n        beta = self.result.get(\"beta\")\n        if beta is None or np.any(~np.isfinite(beta)):\n            raise ValueError(\"Model does not contain beta coefficients for prediction.\")\n\n        X_arr = np.asarray(X_new, dtype=float)\n        if X_arr.ndim == 1:\n            X_arr = X_arr[:, None]\n        if X_arr.shape[1] != beta.size:\n            raise ValueError(\n                f\"Expected {beta.size} predictors, received {X_arr.shape[1]}.\"\n            )\n        if not np.all(np.isfinite(X_arr)):\n            raise ValueError(\"`X_new` contains non-finite values.\")\n\n        mu = self.result[\"mu\"]\n        return mu + 2 * np.arctan(X_arr @ beta)\n\n    def summary(self):\n        if self.result is None:\n            raise ValueError(\"Model must be fitted before summarizing.\")\n\n        # Title based on model type\n        if self.model_type == \"mean\":\n            print(\"\\nCircular Regression for the Mean Direction\\n\")\n        elif self.model_type == \"kappa\":\n            print(\"\\nCircular Regression for the Concentration Parameter\\n\")\n        elif self.model_type == \"mixed\":\n            print(\"\\nMixed Circular-Linear Regression\\n\")\n\n        # Call\n        print(\"Call:\")\n        print(f\"  CLRegression(model_type='{self.model_type}')\\n\")\n\n        # Coefficients for mean direction (Beta)\n        se_beta = self.result.get(\"se_beta\")\n        if (\n            self.model_type in [\"mean\", \"mixed\"]\n            and self.result.get(\"beta\") is not None\n            and se_beta is not None\n        ):\n            print(\"Coefficients for Mean Direction (Beta):\\n\")\n            print(\n                f\"{'':&lt;5} {'Estimate':&lt;12} {'Std. Error':&lt;12} {'t value':&lt;10} {'Pr(&gt;|t|)'}\"\n            )\n            for i, coef in enumerate(self.result[\"beta\"]):\n                se_val = se_beta[i]\n                t_value = abs(coef / se_val) if se_val else np.nan\n                p_value = (\n                    2 * (1 - norm.cdf(np.abs(t_value)))\n                    if not np.isnan(t_value)\n                    else np.nan\n                )\n                print(\n                    f\"\u03b2{i:&lt;3} {coef:&lt;12.5f} {se_val:&lt;12.5f} {t_value:&lt;10.2f} {p_value:&lt;12.5f}{significance_code(p_value):&lt;3}\"\n                )\n\n        # Coefficients for concentration parameter (Gamma)\n        se_gamma = self.result.get(\"se_gamma\")\n        se_alpha = self.result.get(\"se_alpha\")\n        if (\n            self.model_type in [\"kappa\", \"mixed\"]\n            and self.result.get(\"gamma\") is not None\n            and se_gamma is not None\n            and se_alpha is not None\n        ):\n            print(\"\\nCoefficients for Concentration (Gamma):\\n\")\n            print(\n                f\"{'':&lt;5} {'Estimate':&lt;12} {'Std. Error':&lt;12} {'t value':&lt;10} {'Pr(&gt;|t|)':&lt;12}\"\n            )\n            # Report alpha as the first coefficient\n            alpha = self.result[\"alpha\"]\n            t_value_alpha = alpha / se_alpha if se_alpha else np.nan\n            p_value_alpha = (\n                2 * (1 - norm.cdf(np.abs(t_value_alpha)))\n                if not np.isnan(t_value_alpha)\n                else np.nan\n            )\n            print(\n                f\"\u03b1{'':&lt;5} {alpha:&lt;12.5f} {se_alpha:&lt;12.5f} {t_value_alpha:&lt;10.2f} {p_value_alpha:&lt;12.5f}{significance_code(p_value_alpha)}\"\n            )\n            for i, coef in enumerate(self.result[\"gamma\"]):\n                se_val = se_gamma[i]\n                t_value = coef / se_val if se_val else np.nan\n                p_value = (\n                    2 * (1 - norm.cdf(np.abs(t_value)))\n                    if not np.isnan(t_value)\n                    else np.nan\n                )\n                print(\n                    f\"\u03b3{i:&lt;5} {coef:&lt;12.5f} {se_val:&lt;12.5f} {t_value:&lt;10.2f} {p_value:&lt;12.5f}{significance_code(p_value)}\"\n                )\n\n        # Summary for mu and kappa\n        print(\"\\nSummary:\")\n        print(\"  Mean Direction (mu) in radians:\")\n        mu = self.result[\"mu\"]\n        se_mu = self.result.get(\"se_mu\")\n        if se_mu is not None:\n            print(f\"    \u03bc: {mu:.5f} (SE: {se_mu:.5f})\")\n        else:\n            print(f\"    \u03bc: {mu:.5f}\")\n\n        print(\"\\n  Concentration Parameter (kappa):\")\n        kappa = self.result[\"kappa\"]\n        se_kappa = self.result.get(\"se_kappa\")\n        if isinstance(kappa, np.ndarray):\n            print(\"    Index    kappa        Std. Error\")\n            for i, k in enumerate(kappa, start=1):\n                se_val = se_kappa[i - 1] if se_kappa is not None else float(\"nan\")\n                print(f\"    [{i}]    {k:&gt;10.5f}    {se_val:&gt;10.5f}\")\n            if se_kappa is not None:\n                print(f\"    Mean:    {np.mean(kappa):.5f} (SE: {np.mean(se_kappa):.5f})\")\n            else:\n                print(f\"    Mean:    {np.mean(kappa):.5f}\")\n        else:\n            if se_kappa is not None:\n                print(f\"    \u03ba: {kappa:.5f} (SE: {se_kappa:.5f})\")\n            else:\n                print(f\"    \u03ba: {kappa:.5f}\")\n\n        # Summary for model fit metrics\n        print(\"\\nModel Fit Metrics:\\n\")\n        print(f\"{'Metric':&lt;12} {'Value':&lt;12}\")\n        log_likelihood = self.result.get(\"log_likelihood\", float(\"nan\"))\n        nll = -log_likelihood  # Negative log-likelihood\n        print(f\"{'nLL':&lt;12} {nll:&lt;12.5f}\")\n        print(f\"{'AIC':&lt;12} {self.AIC():&lt;12.5f}\")\n        print(f\"{'BIC':&lt;12} {self.BIC():&lt;12.5f}\")\n\n        # Notes\n        print(\"\\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\")\n        print(\"p-values are approximated using the normal distribution.\\n\")\n</code></pre>"},{"location":"reference/regression/#pycircstat2.regression.CLRegression.AIC","title":"<code>AIC()</code>","text":"<p>Calculate Akaike Information Criterion (AIC).</p> Source code in <code>pycircstat2/regression.py</code> <pre><code>def AIC(self):\n    \"\"\"\n    Calculate Akaike Information Criterion (AIC).\n    \"\"\"\n    if self.result is None:\n        raise ValueError(\"Model must be fitted before calculating AIC.\")\n\n    log_likelihood = self.result[\"log_likelihood\"]\n    if self.model_type == \"mean\":\n        n_params = len(self.result[\"beta\"])  # Only beta\n    elif self.model_type == \"kappa\":\n        n_params = 1 + len(self.result[\"gamma\"])  # alpha + gamma\n    elif self.model_type == \"mixed\":\n        n_params = (\n            1 + len(self.result[\"beta\"]) + len(self.result[\"gamma\"])\n        )  # alpha + beta + gamma\n    else:\n        raise ValueError(f\"Unknown model type: {self.model_type}\")\n\n    return -2 * log_likelihood + 2 * n_params\n</code></pre>"},{"location":"reference/regression/#pycircstat2.regression.CLRegression.BIC","title":"<code>BIC()</code>","text":"<p>Calculate Bayesian Information Criterion (BIC).</p> Source code in <code>pycircstat2/regression.py</code> <pre><code>def BIC(self):\n    \"\"\"\n    Calculate Bayesian Information Criterion (BIC).\n    \"\"\"\n    if self.result is None:\n        raise ValueError(\"Model must be fitted before calculating BIC.\")\n\n    log_likelihood = self.result[\"log_likelihood\"]\n    n = len(self.theta)\n    if self.model_type == \"mean\":\n        n_params = len(self.result[\"beta\"])  # Only beta\n    elif self.model_type == \"kappa\":\n        n_params = 1 + len(self.result[\"gamma\"])  # alpha + gamma\n    elif self.model_type == \"mixed\":\n        n_params = (\n            1 + len(self.result[\"beta\"]) + len(self.result[\"gamma\"])\n        )  # alpha + beta + gamma\n    else:\n        raise ValueError(f\"Unknown model type: {self.model_type}\")\n\n    return -2 * log_likelihood + n_params * np.log(n)\n</code></pre>"},{"location":"reference/regression/#pycircstat2.regression.CLRegression.predict","title":"<code>predict(X_new)</code>","text":"<p>Predict circular response values for new predictor values.</p> <p>Parameters:</p> Name Type Description Default <code>X_new</code> <p>New predictor data.</p> required <p>Returns:</p> Name Type Description <code>theta_new</code> <code>(array - like, shape(n_samples))</code> <p>New circular response values.</p> Source code in <code>pycircstat2/regression.py</code> <pre><code>def predict(self, X_new):\n    \"\"\"\n    Predict circular response values for new predictor values.\n\n    Parameters\n    ----------\n    X_new: array-like, shape (n_samples, n_features)\n        New predictor data.\n\n    Returns\n    -------\n    theta_new: array-like, shape(n_samples, )\n        New circular response values.\n    \"\"\"\n    if self.result is None:\n        raise ValueError(\"Model must be fitted before making predictions.\")\n\n    beta = self.result.get(\"beta\")\n    if beta is None or np.any(~np.isfinite(beta)):\n        raise ValueError(\"Model does not contain beta coefficients for prediction.\")\n\n    X_arr = np.asarray(X_new, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr[:, None]\n    if X_arr.shape[1] != beta.size:\n        raise ValueError(\n            f\"Expected {beta.size} predictors, received {X_arr.shape[1]}.\"\n        )\n    if not np.all(np.isfinite(X_arr)):\n        raise ValueError(\"`X_new` contains non-finite values.\")\n\n    mu = self.result[\"mu\"]\n    return mu + 2 * np.arctan(X_arr @ beta)\n</code></pre>"},{"location":"reference/regression/#pycircstat2.regression.CCRegression","title":"<code>CCRegression</code>","text":"<p>Circular-Circular Regression.</p> <p>Fits a circular response to circular predictors using a specified order of harmonics.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>ndarray</code> <p>A numpy array of circular response values in radians.</p> <code>None</code> <code>x</code> <code>ndarray</code> <p>A numpy array of circular predictor values in radians.</p> <code>None</code> <code>order</code> <code>int</code> <p>Order of harmonics to include in the model (default is 1).</p> <code>1</code> <code>level</code> <code>float</code> <p>Significance level for testing higher-order terms (default is 0.05).</p> <code>0.05</code> <p>Attributes:</p> Name Type Description <code>rho</code> <code>float</code> <p>Circular correlation coefficient.</p> <code>fitted</code> <code>ndarray</code> <p>Fitted values of the circular response in radians.</p> <code>residuals</code> <code>ndarray</code> <p>Residuals of the circular response in radians.</p> <code>coefficients</code> <code>dict</code> <p>Coefficients of the cos and sin terms for each harmonic order.</p> <code>p_values</code> <code>ndarray</code> <p>P-values for higher-order terms.</p> <code>message</code> <code>str</code> <p>Message indicating the significance of higher-order terms.</p> <p>Methods:</p> Name Description <code>summary</code> <p>Print a summary of the regression results.</p> Notes <p>The implementation is ported from the <code>lm.circular.cc</code> in the <code>circular</code> R package.</p> References <ul> <li>Jammalamadaka, S. R., &amp; Sengupta, A. (2001) Topics in Circular Statistics. World Scientific.</li> <li>Pewsey, A., Neuh\u00e4user, M., &amp; Ruxton, G. D. (2014) Circular Statistics in R. Oxford University Press.</li> </ul> Source code in <code>pycircstat2/regression.py</code> <pre><code>class CCRegression:\n    \"\"\"\n    Circular-Circular Regression.\n\n    Fits a circular response to circular predictors using a specified order of harmonics.\n\n    Parameters\n    ----------\n    theta : np.ndarray\n        A numpy array of circular response values in radians.\n    x : np.ndarray\n        A numpy array of circular predictor values in radians.\n    order : int, optional\n        Order of harmonics to include in the model (default is 1).\n    level : float, optional\n        Significance level for testing higher-order terms (default is 0.05).\n\n    Attributes\n    ----------\n    rho : float\n        Circular correlation coefficient.\n    fitted : np.ndarray\n        Fitted values of the circular response in radians.\n    residuals : np.ndarray\n        Residuals of the circular response in radians.\n    coefficients : dict\n        Coefficients of the cos and sin terms for each harmonic order.\n    p_values : np.ndarray\n        P-values for higher-order terms.\n    message : str\n        Message indicating the significance of higher-order terms.\n\n    Methods\n    -------\n    summary()\n        Print a summary of the regression results.\n\n\n    Notes\n    -----\n    The implementation is ported from the `lm.circular.cc` in the `circular` R package.\n\n    References\n    ----------\n    - Jammalamadaka, S. R., &amp; Sengupta, A. (2001) Topics in Circular Statistics. World Scientific.\n    - Pewsey, A., Neuh\u00e4user, M., &amp; Ruxton, G. D. (2014) Circular Statistics in R. Oxford University Press.\n    \"\"\"\n\n    def __init__(\n        self,\n        formula: Optional[str] = None,\n        data: Optional[pd.DataFrame] = None,\n        theta: Optional[np.ndarray] = None,\n        x: Optional[np.ndarray] = None,\n        order: int = 1,\n        level: float = 0.05,\n    ):\n        if formula and data is not None:\n            theta_arr, x_arr, self.feature_names = self._parse_formula(formula, data)\n            self.theta = self._validate_input(theta_arr)\n            self.x = self._validate_input(x_arr)\n            if self.x.ndim == 1:\n                self.x = self.x[:, None]\n        elif theta is not None and x is not None:\n            self.theta = self._validate_input(theta)\n            self.x = self._validate_input(x)\n            if self.x.ndim == 1:\n                self.x = self.x[:, None]\n            self.feature_names = [f\"x{i}\" for i in range(self.x.shape[1])]\n        else:\n            raise ValueError(\"Provide either a formula + data or theta and x.\")\n\n        self.order = order\n        self.level = level\n\n        if self.order &lt; 1:\n            raise ValueError(\"`order` must be a positive integer.\")\n        if not (0 &lt; self.level &lt; 1):\n            raise ValueError(\"`level` must lie between 0 and 1.\")\n\n        # Fit the model\n        self.result = self._fit()\n\n    @staticmethod\n    def _validate_input(arr: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Validate input array and ensure it is in radians.\n        \"\"\"\n        arr_np = np.asarray(arr, dtype=float)\n        if arr_np.ndim == 0:\n            raise ValueError(\"Input must be at least one-dimensional.\")\n        if not np.all(np.isfinite(arr_np)):\n            raise ValueError(\"Circular input contains non-finite values.\")\n        return np.mod(arr_np, 2 * np.pi)\n\n    def _parse_formula(\n        self, formula: str, data: pd.DataFrame\n    ) -&gt; Tuple[np.ndarray, np.ndarray, List[str]]:\n        theta_col, x_cols = formula.split(\"~\")\n        theta = data[theta_col.strip()].to_numpy()\n        x_cols = [col.strip() for col in x_cols.split(\"+\")]\n        X = data[x_cols].to_numpy()\n        return theta, X, x_cols\n\n    def _fit(self):\n        n = self.x.shape[0]\n        order = self.order\n        n_features = self.x.shape[1]\n\n        # Create harmonic terms\n        cos_terms = []\n        sin_terms = []\n        cos_labels: List[Tuple[int, int]] = []\n        sin_labels: List[Tuple[int, int]] = []\n        for j in range(n_features):\n            x_col = self.x[:, j]\n            for k in range(1, order + 1):\n                cos_terms.append(np.cos(k * x_col))\n                sin_terms.append(np.sin(k * x_col))\n                cos_labels.append((j, k))\n                sin_labels.append((j, k))\n\n        # Linear models for cos(theta) and sin(theta)\n        Y_cos = np.cos(self.theta)\n        Y_sin = np.sin(self.theta)\n\n        design_matrix = [np.ones(n)] + cos_terms + sin_terms\n        X = np.column_stack(design_matrix)\n        beta_cos, _, _, _ = lstsq(X, Y_cos)\n        beta_sin, _, _, _ = lstsq(X, Y_sin)\n\n        # Fitted values\n        cos_fit = X @ beta_cos\n        sin_fit = X @ beta_sin\n        fitted = np.mod(np.arctan2(sin_fit, cos_fit), 2 * np.pi)\n\n        # Residuals\n        residuals = np.angle(np.exp(1j * (self.theta - fitted)))\n\n        # Circular correlation coefficient\n        rho = float(np.clip(np.sqrt(np.mean(cos_fit**2 + sin_fit**2)), 0.0, 1.0))\n\n        # Test higher-order terms\n        higher_order_cos = []\n        higher_order_sin = []\n        for j in range(n_features):\n            x_col = self.x[:, j]\n            higher_order_cos.append(np.cos((order + 1) * x_col))\n            higher_order_sin.append(np.sin((order + 1) * x_col))\n        if higher_order_cos:\n            W = np.column_stack(higher_order_cos + higher_order_sin)\n        else:\n            W = np.empty((n, 0))\n\n        # Projection matrix for the current model\n        if W.size:\n            XtX = X.T @ X\n            M = X @ _safe_inverse(XtX) @ X.T\n            H = W.T @ (np.eye(n) - M) @ W\n            H_inv = _safe_inverse(H)\n            N = W @ H_inv @ W.T\n\n            residual_cos = Y_cos - X @ beta_cos\n            residual_sin = Y_sin - X @ beta_sin\n\n            denom_cos = float(residual_cos.T @ residual_cos)\n            denom_sin = float(residual_sin.T @ residual_sin)\n            adj = max(n - (2 * order + 1), 1)\n            T1 = (\n                adj\n                * float(residual_cos.T @ N @ residual_cos)\n                / max(denom_cos, 1e-12)\n            )\n            T2 = (\n                adj\n                * float(residual_sin.T @ N @ residual_sin)\n                / max(denom_sin, 1e-12)\n            )\n\n            p1 = 1 - chi2.cdf(T1, W.shape[1])\n            p2 = 1 - chi2.cdf(T2, W.shape[1])\n            p_values = np.array([p1, p2], dtype=float)\n        else:\n            p_values = np.array([np.nan, np.nan], dtype=float)\n\n        # Message about higher-order terms\n        if np.all(np.isnan(p_values)):\n            message = \"No additional harmonics available for testing.\"\n        elif np.all(p_values &gt; self.level):\n            message = (\n                f\"Higher-order terms are not significant at the {self.level} level.\"\n            )\n        else:\n            message = f\"Higher-order terms are significant at the {self.level} level.\"\n\n        return {\n            \"rho\": rho,\n            \"fitted\": fitted,\n            \"residuals\": residuals,\n            \"coefficients\": {\n                \"cos\": beta_cos,\n                \"sin\": beta_sin,\n            },\n            \"cos_labels\": cos_labels,\n            \"sin_labels\": sin_labels,\n            \"p_values\": p_values,\n            \"message\": message,\n        }\n\n    def summary(self):\n        \"\"\"\n        Print a summary of the regression results.\n        \"\"\"\n        print(\"\\nCircular-Circular Regression\\n\")\n        print(f\"Circular Correlation Coefficient (rho): {self.result['rho']:.5f}\\n\")\n\n        print(\"Coefficients:\")\n        cos_coeffs = self.result[\"coefficients\"][\"cos\"]\n        sin_coeffs = self.result[\"coefficients\"][\"sin\"]\n        cos_labels = self.result.get(\"cos_labels\", [])\n        sin_labels = self.result.get(\"sin_labels\", [])\n\n        # Headers\n        print(f\"{'Harmonic':&lt;12} {'Cosine Coeff':&lt;14} {'Sine Coeff':&lt;14}\")\n\n        # Intercept\n        print(f\"{'(Intercept)':&lt;12} {cos_coeffs[0]:&lt;14.5f} {sin_coeffs[0]:&lt;14.5f}\")\n\n        # Cosine harmonics\n        offset = 1\n        for idx, (feature_idx, harmonic) in enumerate(cos_labels):\n            label = f\"cos(x{feature_idx + 1},k={harmonic})\"\n            print(\n                f\"{label:&lt;12} {cos_coeffs[offset + idx]:&lt;14.5f} {sin_coeffs[offset + idx]:&lt;14.5f}\"\n            )\n\n        # Sine harmonics\n        sine_offset = offset + len(cos_labels)\n        for idx, (feature_idx, harmonic) in enumerate(sin_labels):\n            label = f\"sin(x{feature_idx + 1},k={harmonic})\"\n            print(\n                f\"{label:&lt;12} {cos_coeffs[sine_offset + idx]:&lt;14.5f} {sin_coeffs[sine_offset + idx]:&lt;14.5f}\"\n            )\n\n        print(\"\\nP-values for Higher-Order Terms:\")\n        print(\n            f\"p1: {self.result['p_values'][0]:.5f}, p2: {self.result['p_values'][1]:.5f}\"\n        )\n\n        print(f\"\\n{self.result['message']}\\n\")\n</code></pre>"},{"location":"reference/regression/#pycircstat2.regression.CCRegression.summary","title":"<code>summary()</code>","text":"<p>Print a summary of the regression results.</p> Source code in <code>pycircstat2/regression.py</code> <pre><code>def summary(self):\n    \"\"\"\n    Print a summary of the regression results.\n    \"\"\"\n    print(\"\\nCircular-Circular Regression\\n\")\n    print(f\"Circular Correlation Coefficient (rho): {self.result['rho']:.5f}\\n\")\n\n    print(\"Coefficients:\")\n    cos_coeffs = self.result[\"coefficients\"][\"cos\"]\n    sin_coeffs = self.result[\"coefficients\"][\"sin\"]\n    cos_labels = self.result.get(\"cos_labels\", [])\n    sin_labels = self.result.get(\"sin_labels\", [])\n\n    # Headers\n    print(f\"{'Harmonic':&lt;12} {'Cosine Coeff':&lt;14} {'Sine Coeff':&lt;14}\")\n\n    # Intercept\n    print(f\"{'(Intercept)':&lt;12} {cos_coeffs[0]:&lt;14.5f} {sin_coeffs[0]:&lt;14.5f}\")\n\n    # Cosine harmonics\n    offset = 1\n    for idx, (feature_idx, harmonic) in enumerate(cos_labels):\n        label = f\"cos(x{feature_idx + 1},k={harmonic})\"\n        print(\n            f\"{label:&lt;12} {cos_coeffs[offset + idx]:&lt;14.5f} {sin_coeffs[offset + idx]:&lt;14.5f}\"\n        )\n\n    # Sine harmonics\n    sine_offset = offset + len(cos_labels)\n    for idx, (feature_idx, harmonic) in enumerate(sin_labels):\n        label = f\"sin(x{feature_idx + 1},k={harmonic})\"\n        print(\n            f\"{label:&lt;12} {cos_coeffs[sine_offset + idx]:&lt;14.5f} {sin_coeffs[sine_offset + idx]:&lt;14.5f}\"\n        )\n\n    print(\"\\nP-values for Higher-Order Terms:\")\n    print(\n        f\"p1: {self.result['p_values'][0]:.5f}, p2: {self.result['p_values'][1]:.5f}\"\n    )\n\n    print(f\"\\n{self.result['message']}\\n\")\n</code></pre>"},{"location":"reference/utils/","title":"Utilities","text":""},{"location":"reference/utils/#pycircstat2.utils.data2rad","title":"<code>data2rad(data, k=360)</code>","text":"<p>Convert data measured on a circular scale to corresponding angular directions.</p> \\[ \\alpha = \\frac{2\\pi \\times \\mathrm{data}}{k} \\] <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray or float</code> <p>Data measured on a circular scale.</p> required <code>k</code> <code>float or int</code> <p>Number of intervals in the full cycle. Default is 360.</p> <code>360</code> <p>Returns:</p> Name Type Description <code>angle</code> <code>ndarray or float</code> <p>Angular directions in radian.</p> Source code in <code>pycircstat2/utils.py</code> <pre><code>def data2rad(\n    data: Union[np.ndarray, float, int],\n    k: Union[float, int] = 360,  # number of intervals in the full cycle\n) -&gt; Union[np.ndarray, float]:  # eq(26.1), zar 2010\n    r\"\"\"Convert data measured on a circular scale to\n    corresponding angular directions.\n\n    $$ \\alpha = \\frac{2\\pi \\times \\mathrm{data}}{k} $$\n\n    Parameters\n    ----------\n    data : np.ndarray or float\n        Data measured on a circular scale.\n    k : float or int\n        Number of intervals in the full cycle. Default is 360.\n\n    Returns\n    -------\n    angle: np.ndarray or float\n        Angular directions in radian.\n    \"\"\"\n    return 2 * np.pi * data / k\n</code></pre>"},{"location":"reference/utils/#pycircstat2.utils.time2float","title":"<code>time2float(x, sep=':')</code>","text":"<p>Convert an array of strings in time (hh:mm) to an array of floats.</p> Source code in <code>pycircstat2/utils.py</code> <pre><code>def time2float(x: Union[np.ndarray, list, str], sep: str = \":\") -&gt; np.ndarray:\n    \"\"\"Convert an array of strings in time (hh:mm) to an array of floats.\"\"\"\n\n    def _t2f(x: str, sep: str):\n        \"\"\"Convert string of time to float. E.g. 12:45 -&gt; 12.75\"\"\"\n        hr, min = x.split(sep)\n        return float(hr) + float(min) / 60\n\n    t2f = np.vectorize(_t2f)\n    return t2f(x, sep)\n</code></pre>"},{"location":"reference/utils/#pycircstat2.utils.angmod","title":"<code>angmod(rad, bounds=[0, 2 * np.pi])</code>","text":"<p>Normalize angles to a specified range.</p> Parameters: <p>rad : Union[np.ndarray, float, int]     An angle or array of angles in radians. bounds : list, optional     A list or tuple of two values [min, max] defining the target range. Default is [0, 2\u03c0).</p> Returns: <p>Union[np.ndarray, float]     The normalized angle(s), constrained to the specified range.</p> Source code in <code>pycircstat2/utils.py</code> <pre><code>def angmod(\n    rad: Union[np.ndarray, float, np.float64, int], bounds: list = [0, 2 * np.pi]\n) -&gt; Union[np.ndarray, float, np.float64]:\n    \"\"\"\n    Normalize angles to a specified range.\n\n    Parameters:\n    -----------\n    rad : Union[np.ndarray, float, int]\n        An angle or array of angles in radians.\n    bounds : list, optional\n        A list or tuple of two values [min, max] defining the target range. Default is [0, 2\u03c0).\n\n    Returns:\n    --------\n    Union[np.ndarray, float]\n        The normalized angle(s), constrained to the specified range.\n    \"\"\"\n    if len(bounds) != 2 or bounds[0] &gt;= bounds[1]:\n        raise ValueError(\n            \"bounds must be a list or tuple with two values [min, max] where min &lt; max.\"\n        )\n\n    bound_min, bound_max = bounds\n    bound_span = bound_max - bound_min\n    result = ((rad - bound_min) % bound_span + bound_span) % bound_span + bound_min\n\n    # Adjust values equal to bound_max to bound_min for consistency\n    if isinstance(result, np.ndarray):\n        result[result == bound_max] = bound_min\n    elif result == bound_max:\n        result = bound_min\n\n    return result\n</code></pre>"},{"location":"reference/utils/#pycircstat2.utils.angular_distance","title":"<code>angular_distance(a, b)</code>","text":"<p>Angular distance between two angles.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[ndarray, list, float]</code> <p>angle(s).</p> required <code>b</code> <code>float</code> <p>target angle.</p> required <p>Returns:</p> Name Type Description <code>e</code> <code>ndarray</code> <p>angular distance</p> Reference <p>P642, Section 27.2, Zar, 2010</p> Source code in <code>pycircstat2/utils.py</code> <pre><code>def angular_distance(a: Union[np.ndarray, list, float], b: float) -&gt; np.ndarray:\n    \"\"\"Angular distance between two angles.\n\n    Parameters\n    ----------\n    a: np.ndarray or float\n        angle(s).\n\n    b: float\n        target angle.\n\n    Returns\n    -------\n    e: np.ndarray\n        angular distance\n\n    Reference\n    ---------\n    P642, Section 27.2, Zar, 2010\n    \"\"\"\n\n    if isinstance(a, list):\n        a = np.asarray(a)\n\n    c = angmod(a - b)\n    d = 2 * np.pi - c\n    e = np.min(np.array([c, d]), axis=0)\n\n    return e\n</code></pre>"},{"location":"reference/utils/#pycircstat2.utils.is_within_circular_range","title":"<code>is_within_circular_range(value, lb, ub)</code>","text":"<p>Check if a value lies within the circular range [lb, ub].</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The value to check.</p> required <code>lb</code> <code>float</code> <p>The lower bound of the range.</p> required <code>ub</code> <code>float</code> <p>The upper bound of the range.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the value is within the circular range, False otherwise.</p> Source code in <code>pycircstat2/utils.py</code> <pre><code>def is_within_circular_range(value: float, lb: float, ub: float) -&gt; bool:\n    \"\"\"\n    Check if a value lies within the circular range [lb, ub].\n\n    Parameters\n    ----------\n    value : float\n        The value to check.\n    lb : float\n        The lower bound of the range.\n    ub : float\n        The upper bound of the range.\n\n    Returns\n    -------\n    bool\n        True if the value is within the circular range, False otherwise.\n    \"\"\"\n    value = np.mod(value, 2 * np.pi)\n    lb = np.mod(lb, 2 * np.pi)\n    ub = np.mod(ub, 2 * np.pi)\n\n    if lb &lt;= ub:\n        # Standard range\n        return lb &lt;= value &lt;= ub\n    else:\n        # Wrapping range\n        return value &gt;= lb or value &lt;= ub\n</code></pre>"},{"location":"reference/utils/#pycircstat2.utils.rotate_data","title":"<code>rotate_data(alpha, angle, unit='radian')</code>","text":"<p>Rotate a circular dataset by a given angle, supporting degrees, radians, and hours.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Angles in the specified unit.</p> required <code>angle</code> <code>float</code> <p>Rotation angle in the specified unit.</p> required <code>unit</code> <code>str</code> <p>Unit of measurement (\"degree\", \"radian\", or \"hour\"). Default is \"radian\".</p> <code>'radian'</code> <p>Returns:</p> Name Type Description <code>rotated_alpha</code> <code>ndarray</code> <p>Rotated angles, normalized within the unit's full cycle.</p> Source code in <code>pycircstat2/utils.py</code> <pre><code>def rotate_data(alpha: np.ndarray, angle: float, unit: str = \"radian\") -&gt; np.ndarray:\n    \"\"\"\n    Rotate a circular dataset by a given angle, supporting degrees, radians, and hours.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Angles in the specified unit.\n    angle : float\n        Rotation angle in the specified unit.\n    unit : str, optional\n        Unit of measurement (\"degree\", \"radian\", or \"hour\"). Default is \"radian\".\n\n    Returns\n    -------\n    rotated_alpha : np.ndarray\n        Rotated angles, normalized within the unit's full cycle.\n    \"\"\"\n    if unit == \"degree\":\n        n_intervals = 360\n    elif unit == \"radian\":\n        n_intervals = 2 * np.pi\n    elif unit == \"hour\":\n        n_intervals = 24\n    else:\n        raise ValueError(\"Unit must be 'degree', 'radian', or 'hour'.\")\n\n    # Convert to radians for consistent computation\n    alpha_rad = data2rad(alpha, k=n_intervals)\n    angle_rad = data2rad(angle, k=n_intervals)\n\n    # Perform rotation and normalize in radians\n    rotated_alpha_rad = angmod(alpha_rad + angle_rad, bounds=[0, 2 * np.pi])\n\n    # Convert back to the original unit\n    rotated_alpha = rad2data(rotated_alpha_rad, k=n_intervals)\n\n    return np.asarray(rotated_alpha)\n</code></pre>"},{"location":"reference/visualization/","title":"Visualization","text":""},{"location":"reference/visualization/#pycircstat2.visualization.circ_plot","title":"<code>circ_plot(circ_data, ax=None, config=None)</code>","text":"<p>Plots circular data with various visualization options.</p> <p>Parameters:</p> Name Type Description Default <code>circ_data</code> <code>Circular</code> <p>A Circular object containing the data to plot.</p> required <code>ax</code> <code>Axes</code> <p>The axis to plot on. If None, a new figure is created.</p> <code>None</code> <code>config</code> <code>dict</code> <p>Configuration dictionary that overrides defaults.</p> <ul> <li> <p>\"figsize\" : tuple, default=(5, 5)     Size of the figure in inches.</p> </li> <li> <p>\"projection\" : str, default=\"polar\"     Type of projection used for the plot.</p> </li> <li> <p>\"grid\" : bool, default=True     Whether to display grid lines.</p> </li> <li> <p>\"spine\" : bool, default=False     Whether to show the polar spine.</p> </li> <li> <p>\"axis\" : bool, default=True     Whether to display the axis.</p> </li> <li> <p>\"outward\" : bool, default=True     Determines whether scatter points are plotted outward or inward.</p> </li> <li> <p>\"zero_location\" : str, default=\"N\"     The reference direction for 0 degrees (e.g., \"N\", \"E\", \"S\", \"W\").</p> </li> <li> <p>\"clockwise\" : int, default=-1     Direction of angle increase: -1 for clockwise, 1 for counterclockwise.</p> </li> <li> <p>\"radius\" : dict     Controls radial axis settings:</p> <ul> <li>\"ticks\" : list, default=[0, 1]     Radial tick values.</li> <li>\"lim_max\" : float or None, default=None     Maximum radial axis limit.</li> </ul> </li> <li> <p>\"scatter\" : dict     Controls scatter plot settings:</p> <ul> <li>\"marker\" : str, default=\"o\"     Marker style for scatter points.</li> <li>\"color\" : str, default=\"black\"     Color of scatter points.</li> <li>\"size\" : int, default=10     Size of scatter markers.</li> <li>\"r_start\" : float, default=1     Starting radius for scatter points.</li> </ul> </li> <li> <p>\"rose\" : dict     Controls rose diagram settings:</p> <ul> <li>\"bins\" : int, default=12     Number of bins for histogram.</li> <li>\"counts\" : bool, default=False     Whether to display counts on bars.</li> </ul> </li> <li> <p>\"density\" : dict or bool     Controls density estimation settings:</p> <ul> <li>If False, disables density plotting.</li> <li>If True, uses default settings.</li> <li>If dict, allows customization:<ul> <li>\"method\" : str, default=\"nonparametric\"     Method for density estimation (\"nonparametric\" or \"MovM\").</li> <li>\"color\" : str, default=\"black\"     Color of the density line.</li> <li>\"linestyle\" : str, default=\"-\"     Line style of the density plot.</li> <li>\"f\" : array-like, optional     Precomputed radial density offsets. When provided, <code>method</code> is ignored.</li> <li>\"x\" : array-like, optional     Angles in radians corresponding to <code>f</code>. Defaults to an even grid on <code>[0, 2\u03c0]</code>.</li> </ul> </li> </ul> </li> <li> <p>\"mean\" : dict or bool     Controls mean direction plotting:</p> <ul> <li>If False, disables mean plot.</li> <li>If True, uses default settings.</li> <li>If dict, allows customization:<ul> <li>\"color\" : str, default=\"black\"     Color of the mean line.</li> <li>\"linestyle\" : str, default=\"-\"     Line style of the mean plot.</li> <li>\"kind\" : str, default=\"arrow\"     Type of mean representation.</li> <li>\"ci\" : bool, default=True     Whether to display mean confidence intervals.</li> </ul> </li> </ul> </li> <li> <p>\"median\" : dict or bool     Controls median direction plotting:</p> <ul> <li>If False, disables median plot.</li> <li>If True, uses default settings.</li> <li>If dict, allows customization:  <ul> <li>\"color\" : str, default=\"black\"     Color of the median line.</li> <li>\"linestyle\" : str, default=\"dotted\"     Line style of the median plot.</li> <li>\"ci\" : bool, default=True     Whether to display median confidence intervals.</li> </ul> </li> </ul> </li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The matplotlib Axes object containing the plot.</p> Source code in <code>pycircstat2/visualization.py</code> <pre><code>def circ_plot(\n    circ_data,\n    ax=None,\n    config=None,\n):\n\n    \"\"\"Plots circular data with various visualization options.\n\n    Parameters\n    ----------\n    circ_data : Circular\n        A Circular object containing the data to plot.\n    ax : matplotlib.axes._axes.Axes, optional\n        The axis to plot on. If None, a new figure is created.\n    config : dict, optional\n        Configuration dictionary that overrides defaults.\n\n        - **\"figsize\"** : tuple, default=(5, 5)  \n            Size of the figure in inches.\n\n        - **\"projection\"** : str, default=\"polar\"  \n            Type of projection used for the plot.\n\n        - **\"grid\"** : bool, default=True  \n            Whether to display grid lines.\n\n        - **\"spine\"** : bool, default=False  \n            Whether to show the polar spine.\n\n        - **\"axis\"** : bool, default=True  \n            Whether to display the axis.\n\n        - **\"outward\"** : bool, default=True  \n            Determines whether scatter points are plotted outward or inward.\n\n        - **\"zero_location\"** : str, default=\"N\"  \n            The reference direction for 0 degrees (e.g., \"N\", \"E\", \"S\", \"W\").\n\n        - **\"clockwise\"** : int, default=-1  \n            Direction of angle increase: -1 for clockwise, 1 for counterclockwise.\n\n        - **\"radius\"** : dict  \n            Controls radial axis settings:\n            - **\"ticks\"** : list, default=[0, 1]  \n                Radial tick values.\n            - **\"lim_max\"** : float or None, default=None  \n                Maximum radial axis limit.\n\n        - **\"scatter\"** : dict  \n            Controls scatter plot settings:\n            - **\"marker\"** : str, default=\"o\"  \n                Marker style for scatter points.\n            - **\"color\"** : str, default=\"black\"  \n                Color of scatter points.\n            - **\"size\"** : int, default=10  \n                Size of scatter markers.\n            - **\"r_start\"** : float, default=1  \n                Starting radius for scatter points.\n\n        - **\"rose\"** : dict  \n            Controls rose diagram settings:\n            - **\"bins\"** : int, default=12  \n                Number of bins for histogram.\n            - **\"counts\"** : bool, default=False  \n                Whether to display counts on bars.\n\n        - **\"density\"** : dict or bool  \n            Controls density estimation settings:\n            - **If False**, disables density plotting.\n            - **If True**, uses default settings.\n            - **If dict**, allows customization:\n                - **\"method\"** : str, default=\"nonparametric\"  \n                    Method for density estimation (\"nonparametric\" or \"MovM\").\n                - **\"color\"** : str, default=\"black\"  \n                    Color of the density line.\n                - **\"linestyle\"** : str, default=\"-\"  \n                    Line style of the density plot.\n                - **\"f\"** : array-like, optional  \n                    Precomputed radial density offsets. When provided, `method` is ignored.\n                - **\"x\"** : array-like, optional  \n                    Angles in radians corresponding to `f`. Defaults to an even grid on `[0, 2\u03c0]`.\n\n        - **\"mean\"** : dict or bool  \n            Controls mean direction plotting:\n            - **If False**, disables mean plot.\n            - **If True**, uses default settings.\n            - **If dict**, allows customization:\n                - **\"color\"** : str, default=\"black\"  \n                    Color of the mean line.\n                - **\"linestyle\"** : str, default=\"-\"  \n                    Line style of the mean plot.\n                - **\"kind\"** : str, default=\"arrow\"  \n                    Type of mean representation.\n                - **\"ci\"** : bool, default=True  \n                    Whether to display mean confidence intervals.\n\n        - **\"median\"** : dict or bool  \n            Controls median direction plotting:\n            - **If False**, disables median plot.\n            - **If True**, uses default settings.\n            - **If dict**, allows customization:  \n                - **\"color\"** : str, default=\"black\"  \n                    Color of the median line.\n                - **\"linestyle\"** : str, default=\"dotted\"  \n                    Line style of the median plot.\n                - **\"ci\"** : bool, default=True  \n                    Whether to display median confidence intervals.\n\n    Returns\n    -------\n    ax : matplotlib.axes._axes.Axes\n        The matplotlib Axes object containing the plot.\n    \"\"\"\n\n    # Merge user config with defaults recursively\n    config = _merge_dicts(DEFAULT_CIRC_PLOT_CONFIG, config or {})\n\n    # check axes\n    if ax is None:\n        fig, ax = plt.subplots(\n            figsize=config[\"figsize\"],\n            subplot_kw={\"projection\": config[\"projection\"]},\n            layout=\"constrained\",\n        )\n\n    # plot\n    if not circ_data.grouped:\n\n        # plot scatter\n        alpha, counts = np.unique(circ_data.alpha.round(3), return_counts=True)\n        alpha = np.repeat(alpha, counts)\n\n        if config[\"outward\"]:\n            radii = np.hstack(\n                [\n                    config[\"scatter\"][\"r_start\"]\n                    + 0.05\n                    + np.arange(0, 0.05 * int(f), 0.05)[: int(f)]\n                    for f in counts\n                ]\n            )\n        else:\n            radii = np.hstack(\n                [\n                    config[\"scatter\"][\"r_start\"]\n                    - 0.05\n                    - np.arange(0, 0.05 * int(f), 0.05)[: int(f)]\n                    for f in counts\n                ]\n            )\n        ax.scatter(\n            alpha, radii, \n            marker=config[\"scatter\"][\"marker\"], \n            color=config[\"scatter\"][\"color\"], \n            s=config[\"scatter\"][\"size\"]\n        )\n\n        # plot density\n        if config[\"density\"]:  # and not np.isclose(circ_data.r, 0):\n\n            density_config = config[\"density\"]\n            density_method = density_config.get(\"method\", \"nonparametric\")\n            density_color = density_config.get(\"color\", \"black\")\n            density_linestyle = density_config.get(\"linestyle\", \"-\")\n\n            custom_f = density_config.get(\"f\", None)\n            custom_x = density_config.get(\"x\", None)\n\n            if custom_f is not None:\n                f = np.asarray(custom_f, dtype=float)\n                if f.ndim != 1:\n                    raise ValueError(\"`density['f']` must be a one-dimensional array.\")\n\n                if custom_x is None:\n                    x = np.linspace(0, 2 * np.pi, f.size)\n                else:\n                    x = np.asarray(custom_x, dtype=float)\n                    if x.shape != f.shape:\n                        raise ValueError(\n                            \"`density['x']` must have the same shape as `density['f']`.\"\n                        )\n            else:\n\n                if density_method == \"nonparametric\":\n                    h0 = density_config.get(\n                        \"h0\", compute_smooth_params(circ_data.r, circ_data.n)\n                    )\n                    x, f = nonparametric_density_estimation(circ_data.alpha, h0)\n\n                elif density_method == \"MovM\":\n\n                    x = np.linspace(0, 2 * np.pi, 100)\n                    f = circ_data.mixture_opt.predict_density(x=x, unit=\"radian\")\n\n                else:\n                    raise ValueError(\n                        f\"`{density_config['method']}` in `density` is not supported.\"\n                    )\n\n            # save density to circ_data\n            circ_data.density_x = x\n            circ_data.density_f = f\n            f_ = f + 1.05  # add the radius of the plotted circle\n            ax.plot(\n                x,\n                f_,\n                color=density_color,\n                linestyle=density_linestyle,\n            )\n            if config[\"radius\"][\"lim_max\"] is None:\n                ax.set_ylim(0, f_.max())\n            else:\n                ax.set_ylim(0, config[\"radius\"][\"lim_max\"])\n        else:\n            if config[\"radius\"][\"lim_max\"] is None:\n                ax.set_ylim(0, radii.max() + 0.025)\n            else:\n                ax.set_ylim(0, config[\"radius\"][\"lim_max\"])\n\n    # plot rose diagram\n    if config[\"rose\"]:\n\n        if not circ_data.grouped:\n            alpha = circ_data.alpha\n            w, beta = np.histogram(\n                alpha, bins=config[\"rose\"][\"bins\"], range=(0, 2 * np.pi)\n            )  # np.histogram return bin edges\n            beta = 0.5 * (beta[:-1] + beta[1:])\n        else:\n            w = circ_data.w\n            beta = circ_data.alpha\n\n        w_sqrt = np.sqrt(w)\n        w_norm = w_sqrt / w_sqrt.max()\n\n        width = config.get(\"width\", 2 * np.pi / len(beta))\n\n        bars = ax.bar(\n            beta,\n            w_norm,\n            width=width,\n            color=config[\"rose\"][\"color\"],\n            ec=config[\"rose\"][\"edgecolor\"],\n            alpha=config[\"rose\"][\"alpha\"],\n            bottom=0,\n            zorder=2,\n        )\n        if config[\"rose\"][\"counts\"]:\n\n            for i, v in enumerate(w):\n\n                angle = rotation = beta[i].round(3)\n                if angle &gt;= np.pi / 2 and angle &lt; 3 * np.pi / 2:\n                    # alignment = \"right\"\n                    rotation = rotation + np.pi\n                # else:\n                #     alignment = \"left\"\n\n                if v != 0:\n                    ax.text(\n                        x=angle,\n                        y=bars[i].get_height() - 0.075,\n                        s=str(v),\n                        ha=\"center\",\n                        va=\"center\",\n                        rotation=rotation,\n                        rotation_mode=\"anchor\",\n                        color=\"black\",\n                    )\n\n        if circ_data.grouped and config[\"density\"]:\n            x = np.linspace(0, 2 * np.pi, 100)\n            f = circ_data.mixture_opt.predict_density(x=x, unit=\"radian\") + 1\n            ax.plot(x, f, color=\"black\", linestyle=\"-\")\n            if config[\"rlim_max\"] is None:\n                ax.set_ylim(0, f.max())\n            else:\n                ax.set_ylim(0, config[\"rlim_max\"])\n    else:\n        w = circ_data.w\n        config[\"radius\"][\"ticks\"] = [1]  # overwrite\n\n    if config[\"mean\"]:\n\n        radius = circ_data.r\n\n        ax.plot(\n            [0, circ_data.mean],\n            [0, radius],\n            color=config[\"mean\"].get(\"color\", \"black\"),\n            ls=config[\"mean\"].get(\"linestyle\", \"-\"),\n            label=\"mean\",\n            zorder=5,\n        )\n\n        if config[\"mean\"][\"ci\"]:\n\n            if circ_data.mean_lb &lt; circ_data.mean_ub:\n                x1 = np.linspace(circ_data.mean_lb, circ_data.mean_ub, num=50)\n            else:\n                x1 = np.linspace(\n                    circ_data.mean_lb, circ_data.mean_ub + 2 * np.pi, num=50\n                )\n\n            # plot arc\n            ax.plot(\n                x1,\n                np.ones_like(x1) * radius,\n                ls=\"-\",\n                color=config[\"mean\"][\"color\"],\n                zorder=5,\n                lw=2,\n            )\n            # plot arc cap\n            ax.errorbar(x1[0], radius, yerr=0.03, capsize=0, color=config[\"mean\"][\"color\"], lw=2)\n            ax.errorbar(x1[-1], radius, yerr=0.03, capsize=0, color=config[\"mean\"][\"color\"], lw=2)\n\n    if config[\"median\"]:\n        ax.plot(\n            [0, circ_data.median],\n            [0, 0.95],\n            color=config[\"median\"][\"color\"],\n            ls=config[\"median\"].get(\"linestyle\", \"dotted\"),\n            label=\"median\",\n            zorder=5,\n        )\n\n        if config[\"median\"][\"ci\"]:\n            if circ_data.median_lb &lt; circ_data.median_ub:\n                x1 = np.linspace(circ_data.median_lb, circ_data.median_ub, num=50)\n            else:\n                x1 = np.linspace(\n                    circ_data.median_lb, circ_data.median_ub + 2 * np.pi, num=50\n                )\n            # plot arc\n            ax.plot(\n                x1,\n                np.ones_like(x1) - 0.05,\n                ls=\"dotted\",\n                color=config[\"median\"][\"color\"],\n                zorder=5,\n                lw=2,\n            )\n            # plot arc cap\n            ax.errorbar(x1[0], 0.95, yerr=0.03, capsize=0, color=config[\"median\"][\"color\"], lw=2)\n            ax.errorbar(x1[-1], 0.95, yerr=0.03, capsize=0, color=config[\"median\"][\"color\"], lw=2)\n\n    ax.set_theta_zero_location(config[\"zero_location\"])\n    ax.set_theta_direction(config[\"clockwise\"])\n    ax.grid(config[\"grid\"])\n    ax.axis(config[\"axis\"])\n    ax.spines[\"polar\"].set_visible(config[\"spine\"])\n    ax.set_rgrids(config[\"radius\"][\"ticks\"], [\"\" for _ in range(len(config[\"radius\"][\"ticks\"]))], fontsize=16)\n\n    if circ_data.unit == \"hour\":\n        position_major = np.arange(0, 2 * np.pi, 2 * np.pi / 8)\n        position_minor = np.arange(0, 2 * np.pi, 2 * np.pi / 24)\n        labels = [f\"{i}:00\" for i in np.arange(0, circ_data.full_cycle, 3)]\n        ax.xaxis.set_major_locator(ticker.FixedLocator(position_major))\n        ax.xaxis.set_minor_locator(ticker.FixedLocator(position_minor))\n        ax.xaxis.set_major_formatter(ticker.FixedFormatter(labels))\n\n    gridlines = ax.yaxis.get_gridlines()\n    gridlines[-1].set_color(\"k\")\n    gridlines[-1].set_linewidth(1)\n\n    if config[\"legend\"] and (config[\"median\"] or config[\"mean\"]):\n        ax.legend(frameon=False)\n\n    return ax\n</code></pre>"}]}